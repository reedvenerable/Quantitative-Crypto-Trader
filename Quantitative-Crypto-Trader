{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reedvenerable/Quantitative-Crypto-Trader/blob/main/Quantitative-Crypto-Trader\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c597d03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c597d03",
        "outputId": "b475de4f-305c-4f3a-fd0a-ab98f221a6ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: ruff in /usr/local/lib/python3.12/dist-packages (0.12.11)\n",
            "Collecting mypy\n",
            "  Downloading mypy-1.17.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly) (25.0)\n",
            "Collecting mypy_extensions>=1.0.0 (from mypy)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from mypy)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading mypy-1.17.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: pathspec, mypy_extensions, mypy\n",
            "Successfully installed mypy-1.17.1 mypy_extensions-1.1.0 pathspec-0.12.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch numpy pandas plotly tqdm ruff mypy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18de2b08",
      "metadata": {
        "id": "18de2b08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac45ab8-8dab-47c9-9c13-da5162d0b0c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install streamlit\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0439f7e1",
      "metadata": {
        "id": "0439f7e1"
      },
      "outputs": [],
      "source": [
        "!pkill -f \"streamlit run /content/exec_dashboard.py\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc25a421",
      "metadata": {
        "id": "bc25a421"
      },
      "outputs": [],
      "source": [
        "#Core types\n",
        "\n",
        "\n",
        "from collections import deque\n",
        "import itertools\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4301c483",
      "metadata": {
        "id": "4301c483"
      },
      "outputs": [],
      "source": [
        "#Order book\n",
        "\n",
        "class Order:\n",
        "    _ids = itertools.count(0)\n",
        "    def __init__(self, side, price, qty):\n",
        "        self.id = next(Order._ids)\n",
        "        self.side = side                # \"BUY\" or \"SELL\"\n",
        "        self.price = int(price)\n",
        "        self.qty = int(qty)\n",
        "\n",
        "class OrderBook:\n",
        "    def __init__(self):\n",
        "        self.bids = deque()             # highest price first\n",
        "        self.asks = deque()             # lowest price first\n",
        "        self.trades = []                # list of tuples: (side, price, qty)\n",
        "\n",
        "    def submit(self, side, price, qty):\n",
        "        order = Order(side, int(price), int(qty))\n",
        "        if side == \"BUY\":\n",
        "            # match against asks\n",
        "            while self.asks and order.qty > 0 and order.price >= self.asks[0].price:\n",
        "                best = self.asks[0]\n",
        "                take = min(order.qty, best.qty)\n",
        "                self.trades.append((side, best.price, take))\n",
        "                order.qty -= take\n",
        "                best.qty  -= take\n",
        "                if best.qty == 0:\n",
        "                    self.asks.popleft()\n",
        "            if order.qty > 0:\n",
        "                self.bids.append(order)\n",
        "                # keep bids sorted descending by price\n",
        "                self.bids = deque(sorted(self.bids, key=lambda o: -o.price))\n",
        "        else:\n",
        "            # match against bids\n",
        "            while self.bids and order.qty > 0 and order.price <= self.bids[0].price:\n",
        "                best = self.bids[0]\n",
        "                take = min(order.qty, best.qty)\n",
        "                self.trades.append((side, best.price, take))\n",
        "                order.qty -= take\n",
        "                best.qty  -= take\n",
        "                if best.qty == 0:\n",
        "                    self.bids.popleft()\n",
        "            if order.qty > 0:\n",
        "                self.asks.append(order)\n",
        "                # keep asks sorted ascending by price\n",
        "                self.asks = deque(sorted(self.asks, key=lambda o: o.price))\n",
        "        return order.id\n",
        "\n",
        "    def cancel(self, order_id):\n",
        "        self.bids = deque([o for o in self.bids if o.id != order_id])\n",
        "        self.asks = deque([o for o in self.asks if o.id != order_id])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3788e4aa",
      "metadata": {
        "id": "3788e4aa"
      },
      "outputs": [],
      "source": [
        "#Tiny helpers\n",
        "\n",
        "def best_bid(book): return book.bids[0].price if book.bids else None\n",
        "def best_ask(book): return book.asks[0].price if book.asks else None\n",
        "def mid(book):\n",
        "    bb, ba = best_bid(book), best_ask(book)\n",
        "    if bb is not None and ba is not None:\n",
        "        return 0.5 * (bb + ba)\n",
        "    return bb if bb is not None else ba\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d7f6ed1",
      "metadata": {
        "id": "0d7f6ed1"
      },
      "outputs": [],
      "source": [
        "#Real L2 DataFeed\n",
        "\n",
        "import csv\n",
        "from collections import defaultdict\n",
        "\n",
        "class DataFeed:\n",
        "    \"\"\"\n",
        "    Minimal order-book replay feed.\n",
        "    Input CSV must have headers: ts_ms,type,side,price,qty[,venue]\n",
        "      - type  in {\"add\",\"trade\",\"cancel\"}\n",
        "      - side  in {\"B\",\"S\"}\n",
        "      - price numeric (float or int ticks)\n",
        "      - qty   integer\n",
        "      - venue optional; defaults to \"FAST\"\n",
        "    \"\"\"\n",
        "    def __init__(self, path, step_ms=500, tick_size=1.0, default_venue=\"FAST\"):\n",
        "        self.path = path\n",
        "        self.step_ms = int(step_ms)\n",
        "        self.tick_size = float(tick_size)\n",
        "        self.default_venue = default_venue\n",
        "        self._buckets = None\n",
        "        self._ordered_keys = None\n",
        "        self._idx = 0\n",
        "\n",
        "    def _load(self):\n",
        "        buckets = defaultdict(list)\n",
        "        with open(self.path, \"r\", newline=\"\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for row in reader:\n",
        "                try:\n",
        "                    ts = int(row[\"ts_ms\"])\n",
        "                    typ = row[\"type\"].lower()\n",
        "                    side = row.get(\"side\", \"\").upper() or \"B\"\n",
        "                    px   = float(row[\"price\"])\n",
        "                    qty  = int(float(row[\"qty\"]))\n",
        "                    ven  = row.get(\"venue\", self.default_venue)\n",
        "                except Exception:\n",
        "                    continue  # skip bad rows\n",
        "\n",
        "                # bucket by step window\n",
        "                key = (ts // self.step_ms) * self.step_ms\n",
        "                buckets[key].append({\n",
        "                    \"venue\": ven,\n",
        "                    \"type\": typ,           # \"add\", \"trade\", \"cancel\"\n",
        "                    \"side\": side,          # \"B\" or \"S\"\n",
        "                    \"price\": px,\n",
        "                    \"qty\": qty,\n",
        "                })\n",
        "        self._buckets = buckets\n",
        "        self._ordered_keys = sorted(buckets.keys())\n",
        "        self._idx = 0\n",
        "\n",
        "    def reset(self, start_ms=None):\n",
        "        if self._buckets is None:\n",
        "            self._load()\n",
        "        if start_ms is not None:\n",
        "            # jump to the first bucket >= start_ms\n",
        "            key = (int(start_ms) // self.step_ms) * self.step_ms\n",
        "            # find index\n",
        "            ks = self._ordered_keys\n",
        "            lo, hi = 0, len(ks)\n",
        "            while lo < hi:\n",
        "                mid = (lo+hi)//2\n",
        "                if ks[mid] < key: lo = mid+1\n",
        "                else: hi = mid\n",
        "            self._idx = lo\n",
        "        else:\n",
        "            self._idx = 0\n",
        "\n",
        "    def next_events(self):\n",
        "        \"\"\"Return list of events for the next step; [] if out of data.\"\"\"\n",
        "        if self._buckets is None or self._idx >= len(self._ordered_keys):\n",
        "            return []\n",
        "        k = self._ordered_keys[self._idx]\n",
        "        self._idx += 1\n",
        "        return self._buckets[k]\n",
        "\n",
        "def price_to_int(price, tick_size=1.0):\n",
        "    \"\"\"Convert float price to integer ticks for your OrderBook.\"\"\"\n",
        "    return int(round(float(price) / float(tick_size)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57d79400",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57d79400",
        "outputId": "72cb89b9-9dc2-4821-ff61-11f508dbb980"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FAST] connecting: wss://stream.binance.us:9443/ws/btcusdt@depth@500ms\n",
            "[SLOW] connecting: wss://stream.binance.vision/ws/btcusdt@depth@500ms\n",
            "[SLOW] error: gaierror: [Errno -2] Name or service not known\n"
          ]
        }
      ],
      "source": [
        "!pip -q install websockets nest_asyncio\n",
        "\n",
        "import asyncio, json, time, csv, os, websockets, nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Recorder configuration\n",
        "REC_SECONDS   = 30         # default shorter duration (~1 minute) for quicker runs; adjust as needed\n",
        "STEP_MS       = 60000        # 500 ms buckets (must match @depth@500ms)\n",
        "SYMBOL        = \"BTCUSDT\"\n",
        "OUT_DIR       = \"/content/data\"; os.makedirs(OUT_DIR, exist_ok=True)\n",
        "OUT_PATH      = f\"{OUT_DIR}/l2_feed.csv\"\n",
        "LEVELS        = 8          # number of top levels per side to track\n",
        "RECV_TIMEOUTS = (3.0, 6.0) # initial and fallback timeouts for websocket recv\n",
        "IDLE_WARN_S   = 10         # warn if no data for this many seconds\n",
        "\n",
        "def _emit_level_deltas(prev_map, now_map, side_flag):\n",
        "    \"\"\"Compute add/cancel events between previous and current top-of-book maps.\"\"\"\n",
        "    out = []\n",
        "    for px, sz_now in now_map.items():\n",
        "        d = int(round(sz_now - prev_map.get(px, 0.0)))\n",
        "        if d > 0:\n",
        "            out.append((\"add\", side_flag, float(px), d))\n",
        "        elif d < 0:\n",
        "            out.append((\"cancel\", side_flag, float(px), -d))\n",
        "    for px, sz_prev in prev_map.items():\n",
        "        if px not in now_map and sz_prev > 0:\n",
        "            out.append((\"cancel\", side_flag, float(px), int(round(sz_prev))))\n",
        "    return out\n",
        "\n",
        "async def _record_from(url, venue, out_path, step_ms=STEP_MS, seconds=REC_SECONDS,\n",
        "                       levels=LEVELS, timeouts=RECV_TIMEOUTS):\n",
        "    \"\"\"Record top-of-book deltas from a single WebSocket endpoint with timeout handling.\"\"\"\n",
        "    start = time.monotonic()\n",
        "    last_msg = start\n",
        "    last_bucket = None\n",
        "    prev_bids, prev_asks = {}, {}\n",
        "    wrote_any = False\n",
        "    with open(out_path, \"a\", newline=\"\") as f:\n",
        "        w = csv.writer(f)\n",
        "        async with websockets.connect(url, ping_interval=20) as ws:\n",
        "            while (t := time.monotonic()) - start < seconds:\n",
        "                # Periodic heartbeat\n",
        "                if int(t - start) % 10 == 0 and int(t - start) != int(last_msg - start):\n",
        "                    print(f\"[{venue}] ~{int(t - start)}s elapsed, wrote={{wrote_any}}\")\n",
        "                    last_msg = t\n",
        "                msg = None\n",
        "                try:\n",
        "                    msg = await asyncio.wait_for(ws.recv(), timeout=timeouts[0])\n",
        "                except asyncio.TimeoutError:\n",
        "                    if time.monotonic() - last_msg >= IDLE_WARN_S:\n",
        "                        print(f\"[{venue}] idle >{IDLE_WARN_S}s, continuing...\")\n",
        "                    try:\n",
        "                        msg = await asyncio.wait_for(ws.recv(), timeout=timeouts[1])\n",
        "                    except asyncio.TimeoutError:\n",
        "                        continue\n",
        "                last_msg = time.monotonic()\n",
        "                try:\n",
        "                    d = json.loads(msg)\n",
        "                except Exception:\n",
        "                    continue\n",
        "                ts = int(d.get(\"E\") or time.time()*1000)\n",
        "                bucket = (ts // step_ms) * step_ms\n",
        "                if last_bucket is None:\n",
        "                    last_bucket = bucket\n",
        "                now_bids = {float(px): float(sz) for px, sz in d.get(\"b\", [])[:levels]}\n",
        "                now_asks = {float(px): float(sz) for px, sz in d.get(\"a\", [])[:levels]}\n",
        "                if bucket != last_bucket:\n",
        "                    evsB = _emit_level_deltas(prev_bids, now_bids, \"B\")\n",
        "                    evsS = _emit_level_deltas(prev_asks, now_asks, \"S\")\n",
        "                    if evsB or evsS:\n",
        "                        for ev in evsB:\n",
        "                            w.writerow([last_bucket, ev[0], ev[1], f\"{ev[2]:.2f}\", ev[3], venue])\n",
        "                        for ev in evsS:\n",
        "                            w.writerow([last_bucket, ev[0], ev[1], f\"{ev[2]:.2f}\", ev[3], venue])\n",
        "                        wrote_any = True\n",
        "                    prev_bids, prev_asks = now_bids, now_asks\n",
        "                    last_bucket = bucket\n",
        "    return wrote_any\n",
        "\n",
        "async def record_depth_500ms_fast(symbol=SYMBOL, out_path=OUT_PATH, step_ms=STEP_MS, seconds=REC_SECONDS):\n",
        "    \"\"\"Record deltas using a race between Binance US and the vision mirror endpoints.\"\"\"\n",
        "    # header row\n",
        "    with open(out_path, \"w\", newline=\"\") as f:\n",
        "        csv.writer(f).writerow([\"ts_ms\",\"type\",\"side\",\"price\",\"qty\",\"venue\"])\n",
        "    url_us   = f\"wss://stream.binance.us:9443/ws/{symbol.lower()}@depth@500ms\"\n",
        "    url_mirr = f\"wss://stream.binance.vision/ws/{symbol.lower()}@depth@500ms\"\n",
        "\n",
        "    async def _try(url, venue):\n",
        "        try:\n",
        "            print(f\"[{venue}] connecting: {url}\")\n",
        "            wrote = await _record_from(url, venue, out_path, step_ms, seconds)\n",
        "            print(f\"[{venue}] done. wrote_any={{wrote}}\")\n",
        "            return venue, wrote, None\n",
        "        except Exception as e:\n",
        "            return venue, False, f\"{type(e).__name__}: {e}\"\n",
        "\n",
        "    done, pending = await asyncio.wait(\n",
        "        {asyncio.create_task(_try(url_us, \"FAST\")), asyncio.create_task(_try(url_mirr, \"SLOW\"))},\n",
        "        return_when=asyncio.FIRST_COMPLETED,\n",
        "    )\n",
        "    for p in pending:\n",
        "        p.cancel()\n",
        "    for task in done:\n",
        "        venue, wrote, err = task.result()\n",
        "        if err:\n",
        "            print(f\"[{venue}] error: {err}\")\n",
        "        else:\n",
        "            print(f\"Depth feed written to: {out_path} | venue: {venue} | wrote_any={{wrote}}\")\n",
        "\n",
        "# Execute recorder once when this cell runs\n",
        "await record_depth_500ms_fast()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65ad805a",
      "metadata": {
        "id": "65ad805a"
      },
      "outputs": [],
      "source": [
        "#Runner v3\n",
        "\n",
        "import os, numpy as np, pandas as pd\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "EXPORT_DIR = \"/content/data/exports\"; os.makedirs(EXPORT_DIR, exist_ok=True)\n",
        "\n",
        "def run_adaptive_kept_logged_v3(env, title=\"Adaptive v0 + keeper + log\", run_stamp=None, parent_side=\"BUY\"):\n",
        "    \"\"\"\n",
        "    Runs your policy-driven execution loop and exports:\n",
        "      *_metrics.csv, *_fills.csv (counts placeholder), *_route_log.csv (if present),\n",
        "      *_fills_true.csv (t, ts_ms, side, qty, price, venue).\n",
        "    Returns: (df_metrics, df_fills_counts, route_log_df, df_fills_true, export_prefix)\n",
        "    \"\"\"\n",
        "    run_stamp = run_stamp or datetime.now(timezone.utc).strftime(\"%Y%m%d-%H%M%S\")\n",
        "    export_prefix = os.path.join(EXPORT_DIR, f\"run_{run_stamp}\")\n",
        "    parent_sign = 1 if str(parent_side).upper().startswith(\"B\") else -1\n",
        "\n",
        "    if \"pick_policy\" in globals():\n",
        "        try:\n",
        "            pol = pick_policy(\"adaptive\")\n",
        "        except Exception:\n",
        "            try:\n",
        "                pol = pick_policy(\"pov\")\n",
        "            except Exception:\n",
        "                pol = (lambda _s: np.array([0.40, 1], dtype=float))\n",
        "    else:\n",
        "        pol = (lambda _s: np.array([0.40, 1], dtype=float))\n",
        "\n",
        "    state = env.reset()\n",
        "    if \"recompute_arrival0\" in globals():\n",
        "        try: recompute_arrival0(env)\n",
        "        except Exception: pass\n",
        "    if \"seed_books_from_feed\" in globals() and \"feed\" in globals():\n",
        "        try: seed_books_from_feed(env, feed)\n",
        "        except Exception: pass\n",
        "\n",
        "    metrics, fills_true_rows, route_log_rows = [], [], []\n",
        "    last_len = {sv.venue.name: 0 for sv in env.vl}\n",
        "    fills_by_venue_rows = []\n",
        "\n",
        "    def _ts_ms_or_none():\n",
        "        try:\n",
        "            return int(getattr(env, \"ts_ms\", None) or env.t * getattr(env, \"step_ms\", 1000))\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = pol(state)\n",
        "        result = env.step(action)\n",
        "\n",
        "\n",
        "        bb_list, ba_list = [], []\n",
        "        for sv in env.vl:\n",
        "            bb = sv.venue.book.bids[0].price if sv.venue.book.bids else None\n",
        "            ba = sv.venue.book.asks[0].price if sv.venue.book.asks else None\n",
        "            if bb is not None: bb_list.append(float(bb))\n",
        "            if ba is not None: ba_list.append(float(ba))\n",
        "        if bb_list and ba_list:\n",
        "            mid_snap = 0.5 * (max(bb_list) + min(ba_list))\n",
        "            spr_snap = max(0.0, min(ba_list) - max(bb_list))\n",
        "        else:\n",
        "            mid_snap = float(getattr(env, \"mid\", np.nan))\n",
        "            spr_snap = float(getattr(env, \"spread\", np.nan))\n",
        "\n",
        "        metrics.append({\n",
        "            \"t\": env.t,\n",
        "            \"venue\": getattr(env, \"best_venue\", \"\"),\n",
        "            \"mid\": mid_snap,\n",
        "            \"spread\": spr_snap,\n",
        "            \"depth_bid_top3\": getattr(env, \"depth_bid_top3\", np.nan),\n",
        "            \"depth_ask_top3\": getattr(env, \"depth_ask_top3\", np.nan),\n",
        "            \"cum_filled\": int(getattr(env, \"cum_filled\", 0)),\n",
        "            \"remaining\": int(getattr(env, \"remaining\", 0)),\n",
        "        })\n",
        "\n",
        "\n",
        "        ts_ms = _ts_ms_or_none()\n",
        "        for sv in env.vl:\n",
        "            vname = sv.venue.name\n",
        "            new_trades = sv.venue.book.trades[last_len[vname]:]\n",
        "            for tr in new_trades:\n",
        "                px = np.nan; qty = 0; side = (\"BUY\" if parent_sign == 1 else \"SELL\")\n",
        "                if not isinstance(tr, (tuple, list)):\n",
        "                    continue\n",
        "                L = len(tr)\n",
        "                try:\n",
        "                    if L == 3:\n",
        "                        a, b, c = tr[0], tr[1], tr[2]\n",
        "                        # ('BUY', px, qty)\n",
        "                        if isinstance(a, str) and not isinstance(b, str):\n",
        "                            side = a.upper(); px = float(b); qty = int(c)\n",
        "                        # (px, 'BUY', qty)\n",
        "                        elif isinstance(b, str) and not isinstance(a, str):\n",
        "                            px = float(a); side = b.upper(); qty = int(c)\n",
        "                        # (px, qty, ?)\n",
        "                        elif not isinstance(a, str) and isinstance(b, (int, float, np.integer, np.floating)):\n",
        "                            px = float(a); qty = int(b)\n",
        "                        else:\n",
        "                            px = float(a); qty = int(b)\n",
        "                    elif L == 2:\n",
        "                        # (px, qty)\n",
        "                        px = float(tr[0]); qty = int(tr[1])\n",
        "                    else:\n",
        "                        continue\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "                fills_true_rows.append({\n",
        "                    \"t\": int(env.t),\n",
        "                    \"ts_ms\": ts_ms if ts_ms is not None else int(env.t),\n",
        "                    \"side\": side,\n",
        "                    \"qty\": int(qty),\n",
        "                    \"price\": float(px),\n",
        "                    \"venue\": vname\n",
        "                })\n",
        "            last_len[vname] = len(sv.venue.book.trades)\n",
        "\n",
        "\n",
        "        counts_row = {\"t\": env.t}\n",
        "        for sv in env.vl:\n",
        "            counts_row[f\"fill_{sv.venue.name}\"] = 0\n",
        "        fills_by_venue_rows.append(counts_row)\n",
        "\n",
        "\n",
        "        if hasattr(env, \"_route_log\") and isinstance(env._route_log, list):\n",
        "            route_log_rows = env._route_log\n",
        "\n",
        "\n",
        "        if ('USE_KEEPER' in globals() and USE_KEEPER) or ('_feed_stalled' in globals() and _feed_stalled(env)):\n",
        "            if \"keep_books_healthy\" in globals():\n",
        "                try: keep_books_healthy(env)\n",
        "                except Exception: pass\n",
        "\n",
        "        state = getattr(result, \"state\", None)\n",
        "        done  = bool(getattr(result, \"done\", False))\n",
        "\n",
        "\n",
        "    df_metrics       = pd.DataFrame(metrics)\n",
        "    df_fills_counts  = pd.DataFrame(fills_by_venue_rows).fillna(0)\n",
        "    df_fills_true    = pd.DataFrame(fills_true_rows, columns=[\"t\",\"ts_ms\",\"side\",\"qty\",\"price\",\"venue\"])\n",
        "    route_log_df     = pd.DataFrame(route_log_rows)\n",
        "\n",
        "    df_metrics.to_csv(f\"{export_prefix}_metrics.csv\", index=False)\n",
        "    df_fills_counts.to_csv(f\"{export_prefix}_fills.csv\", index=False)\n",
        "    if not route_log_df.empty:\n",
        "        route_log_df.to_csv(f\"{export_prefix}_route_log.csv\", index=False)\n",
        "    if not df_fills_true.empty:\n",
        "        df_fills_true.to_csv(f\"{export_prefix}_fills_true.csv\", index=False)\n",
        "\n",
        "    print(\"Export prefix:\", export_prefix)\n",
        "    return df_metrics, df_fills_counts, route_log_df, df_fills_true, export_prefix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f49fb07",
      "metadata": {
        "id": "4f49fb07"
      },
      "outputs": [],
      "source": [
        "!pip -q install websockets nest_asyncio\n",
        "\n",
        "import asyncio, json, time, csv, os, websockets, nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db527ec6",
      "metadata": {
        "id": "db527ec6"
      },
      "outputs": [],
      "source": [
        "#CONFIG\n",
        "\n",
        "SYMBOLS     = [\"BTCUSDT\", \"ETHUSDT\", \"SOLUSDT\", \"BNBUSDT\", \"ADAUSDT\"]\n",
        "REC_SECONDS = 30\n",
        "STEP_MS     = 60000\n",
        "OUT_DIR     = \"/content/data/l2_multi\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#L2 Depth Recorder\n",
        "!pip -q install websockets nest_asyncio\n",
        "\n",
        "import asyncio, json, time, csv, os, sys, subprocess, glob, math, zipfile\n",
        "import websockets, nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from datetime import date, timedelta\n",
        "\n",
        "try:    REC_SECONDS\n",
        "except: REC_SECONDS = 30\n",
        "\n",
        "try:    STEP_MS\n",
        "except: STEP_MS = 60000\n",
        "\n",
        "try:    SYMBOLS\n",
        "except: SYMBOLS = [\"BTCUSDT\", \"ETHUSDT\", \"SOLUSDT\", \"BNBUSDT\", \"ADAUSDT\"]\n",
        "\n",
        "try:    OUT_DIR\n",
        "except: OUT_DIR = \"/content/data/l2_multi\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "RAW_DIR = \"/content/data/raw\"; os.makedirs(RAW_DIR, exist_ok=True)\n",
        "DAYS    = 15                         # how many calendar days of aggTrades to fetch\n",
        "START   = date.today() - timedelta(days=DAYS)\n",
        "MAX_DL_WORKERS = 8                  # parallel download threads\n",
        "RECV_TIMEOUT_S = 5.0                # websocket recv timeout (prevents indefinite hangs)\n",
        "# --------------------------\n",
        "\n",
        "\n",
        "def _emit_level_deltas(prev_map, now_map, side_flag):\n",
        "    \"\"\"Return [('add'|'cancel', 'B'|'S', price, qty)] from best bid/ask deltas.\"\"\"\n",
        "    out = []\n",
        "    for px, sz_now in now_map.items():\n",
        "        d = int(round(sz_now - prev_map.get(px, 0.0)))\n",
        "        if   d > 0: out.append((\"add\",    side_flag, float(px), d))\n",
        "        elif d < 0: out.append((\"cancel\", side_flag, float(px), -d))\n",
        "    for px, sz_prev in prev_map.items():\n",
        "        if px not in now_map and sz_prev > 0:\n",
        "            out.append((\"cancel\", side_flag, float(px), int(round(sz_prev))))\n",
        "    return out\n",
        "\n",
        "\n",
        "async def record_depth_500ms(symbol, out_path, step_ms=STEP_MS, seconds=REC_SECONDS):\n",
        "    \"\"\"Record top-of-book deltas for one symbol to CSV. Stops after first working venue.\"\"\"\n",
        "    url_us   = f\"wss://stream.binance.us:9443/ws/{symbol.lower()}@depth@500ms\"\n",
        "    url_mirr = f\"wss://stream.binance.vision/ws/{symbol.lower()}@depth@500ms\"\n",
        "\n",
        "    # header\n",
        "    with open(out_path, \"w\", newline=\"\") as f:\n",
        "        csv.writer(f).writerow([\"ts_ms\",\"type\",\"side\",\"price\",\"qty\",\"venue\"])\n",
        "\n",
        "    async def _run(url, venue):\n",
        "        last_bucket = None\n",
        "        prev_bids, prev_asks = {}, {}\n",
        "        start = time.time()\n",
        "        with open(out_path, \"a\", newline=\"\") as f:\n",
        "            w = csv.writer(f)\n",
        "            async with websockets.connect(\n",
        "                url,\n",
        "                ping_interval=20,\n",
        "                ping_timeout=20,\n",
        "                close_timeout=10,\n",
        "                open_timeout=10,\n",
        "                max_size=2**22\n",
        "            ) as ws:\n",
        "                while True:\n",
        "                    # stop if time exceeded\n",
        "                    if time.time() - start >= seconds:\n",
        "                        break\n",
        "                    # bounded recv to avoid indefinite block\n",
        "                    try:\n",
        "                        msg = await asyncio.wait_for(ws.recv(), timeout=RECV_TIMEOUT_S)\n",
        "                    except asyncio.TimeoutError:\n",
        "                        # no message; loop back to reevaluate time limit\n",
        "                        continue\n",
        "                    except Exception:\n",
        "                        # transient network/parse error; keep going until time expires\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "                        d = json.loads(msg)\n",
        "                    except Exception:\n",
        "                        continue\n",
        "\n",
        "                    ts = int(d.get(\"E\") or time.time()*1000)\n",
        "                    bucket = (ts // step_ms) * step_ms\n",
        "                    if last_bucket is None:\n",
        "                        last_bucket = bucket\n",
        "\n",
        "                    now_bids = {float(px): float(sz) for px, sz in d.get(\"b\", [])[:5]}\n",
        "                    now_asks = {float(px): float(sz) for px, sz in d.get(\"a\", [])[:5]}\n",
        "\n",
        "                    if bucket != last_bucket:\n",
        "                        for ev in _emit_level_deltas(prev_bids, now_bids, \"B\"):\n",
        "                            w.writerow([last_bucket, ev[0], ev[1], f\"{ev[2]:.2f}\", ev[3], venue])\n",
        "                        for ev in _emit_level_deltas(prev_asks, now_asks, \"S\"):\n",
        "                            w.writerow([last_bucket, ev[0], ev[1], f\"{ev[2]:.2f}\", ev[3], venue])\n",
        "                        prev_bids, prev_asks = now_bids, now_asks\n",
        "                        last_bucket = bucket\n",
        "\n",
        "        return time.time() - start\n",
        "\n",
        "    # try FAST then SLOW, STOP after first success\n",
        "    for url, ven in [(url_us, \"FAST\"), (url_mirr, \"SLOW\")]:\n",
        "        try:\n",
        "            elapsed = await _run(url, ven)\n",
        "            print(f\"{symbol}: recorded {elapsed:.1f}s via {ven}\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"{symbol} reconnect on {ven}: {type(e).__name__} {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"{symbol}: depth feed written → {out_path}\")\n",
        "\n",
        "\n",
        "t0 = time.time()\n",
        "print(\"Start recording:\", time.strftime(\"%X\"))\n",
        "tasks = [record_depth_500ms(sym, os.path.join(OUT_DIR, f\"{sym}_l2.csv\")) for sym in SYMBOLS]\n",
        "await asyncio.gather(*tasks)\n",
        "print(\"Recording complete:\", time.strftime(\"%X\"), f\"(elapsed {time.time()-t0:.1f}s)\")\n",
        "\n",
        "print(\"Files:\")\n",
        "for sym in SYMBOLS:\n",
        "    path = os.path.join(OUT_DIR, f\"{sym}_l2.csv\")\n",
        "    print(\" -\", path, \"exists\" if os.path.exists(path) else \"(missing)\")\n",
        "\n",
        "\n",
        "\n",
        "def dl_day(sym: str, d: date):\n",
        "    ymd = d.strftime(\"%Y-%m-%d\")\n",
        "    url = f\"https://data.binance.vision/data/spot/daily/aggTrades/{sym}/{sym}-aggTrades-{ymd}.zip\"\n",
        "    fn  = os.path.join(RAW_DIR, f\"{sym}-aggTrades-{ymd}.zip\")\n",
        "    subprocess.run([\"wget\",\"-q\",\"--tries=2\",\"-T\",\"10\",\"-O\",fn,\"-c\",url], check=False)\n",
        "    return fn\n",
        "\n",
        "print(\"Start downloads:\", time.strftime(\"%X\"))\n",
        "t1 = time.time()\n",
        "futs = []\n",
        "with ThreadPoolExecutor(max_workers=MAX_DL_WORKERS) as ex:\n",
        "    for sym in SYMBOLS:\n",
        "        for i in range(DAYS):\n",
        "            futs.append(ex.submit(dl_day, sym, START + timedelta(days=i)))\n",
        "    for _ in as_completed(futs):\n",
        "        pass\n",
        "print(\"Downloads complete:\", time.strftime(\"%X\"), f\"(elapsed {time.time()-t1:.1f}s)\")\n",
        "\n",
        "\n",
        "def safe_unzip(path_zip: str, out_dir: str):\n",
        "    try:\n",
        "        if os.path.getsize(path_zip) <= 22:\n",
        "            return False\n",
        "        with zipfile.ZipFile(path_zip, 'r') as zf:\n",
        "            zf.extractall(out_dir)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "unzipped_ok = 0\n",
        "for zp in glob.glob(os.path.join(RAW_DIR, \"*.zip\")):\n",
        "    if safe_unzip(zp, RAW_DIR):\n",
        "        unzipped_ok += 1\n",
        "\n",
        "print(f\"Unzip complete ({unzipped_ok} archives extracted):\", time.strftime(\"%X\"))\n",
        "print(\"All done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNM9t5F99pig",
        "outputId": "a154b33e-6ae7-4c57-dc44-ea276d8a9fdc"
      },
      "id": "yNM9t5F99pig",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start recording: 22:12:57\n",
            "BNBUSDT: recorded 36.3s via FAST\n",
            "BNBUSDT: depth feed written → /content/data/l2_multi/BNBUSDT_l2.csv\n",
            "BTCUSDT: recorded 36.5s via FAST\n",
            "BTCUSDT: depth feed written → /content/data/l2_multi/BTCUSDT_l2.csv\n",
            "ADAUSDT: recorded 36.9s via FAST\n",
            "ADAUSDT: depth feed written → /content/data/l2_multi/ADAUSDT_l2.csv\n",
            "SOLUSDT: recorded 36.9s via FAST\n",
            "SOLUSDT: depth feed written → /content/data/l2_multi/SOLUSDT_l2.csv\n",
            "ETHUSDT: recorded 39.8s via FAST\n",
            "ETHUSDT: depth feed written → /content/data/l2_multi/ETHUSDT_l2.csv\n",
            "Recording complete: 22:13:37 (elapsed 39.8s)\n",
            "Files:\n",
            " - /content/data/l2_multi/BTCUSDT_l2.csv exists\n",
            " - /content/data/l2_multi/ETHUSDT_l2.csv exists\n",
            " - /content/data/l2_multi/SOLUSDT_l2.csv exists\n",
            " - /content/data/l2_multi/BNBUSDT_l2.csv exists\n",
            " - /content/data/l2_multi/ADAUSDT_l2.csv exists\n",
            "Start downloads: 22:13:37\n",
            "Downloads complete: 22:13:49 (elapsed 11.5s)\n",
            "Unzip complete (75 archives extracted): 22:14:21\n",
            "All done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Guards\n",
        "\n",
        "import os, glob, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "RAW_DIR   = globals().get(\"RAW_DIR\", \"/content/data/raw\")\n",
        "FEEDS_DIR = \"/content/data/feeds\"; os.makedirs(FEEDS_DIR, exist_ok=True)\n",
        "EVENT_DIR = \"/content/data/events\"; os.makedirs(EVENT_DIR, exist_ok=True)\n",
        "\n",
        "DAYS_TO_USE = int(globals().get(\"DAYS\", 3))\n",
        "\n",
        "# Symbols to process\n",
        "if \"SYMBOLS\" in globals():\n",
        "    SYMBOLS = list(globals()[\"SYMBOLS\"])\n",
        "else:\n",
        "    detected = sorted({os.path.basename(p).split(\"-\")[0]\n",
        "                       for p in glob.glob(os.path.join(RAW_DIR, \"*-aggTrades-*.csv\"))})\n",
        "    if not detected:\n",
        "        raise RuntimeError(f\"No raw aggTrades CSVs found in {RAW_DIR}.\")\n",
        "    SYMBOLS = detected\n",
        "print(\"Symbols to process:\", SYMBOLS)\n",
        "\n",
        "# Pick CSV engine safely\n",
        "_HAS_PYARROW = False\n",
        "try:\n",
        "    import pyarrow  # noqa\n",
        "    _HAS_PYARROW = True\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def _parse_ymd_from_name(path):\n",
        "    bn = os.path.basename(path)\n",
        "    try:\n",
        "        y, m, d_csv = bn.split(\"-\")[-3:]\n",
        "        d = d_csv.replace(\".csv\",\"\")\n",
        "        return datetime.strptime(f\"{y}-{m}-{d}\", \"%Y-%m-%d\").date()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _coerce_cols(df, mapping):\n",
        "    \"\"\"Return DataFrame with required columns using a mapping dict:\n",
        "       mapping keys = required ['tradeId','price','qty','time','isBuyerMaker'] → values = source column names.\"\"\"\n",
        "    need = [\"tradeId\",\"price\",\"qty\",\"time\",\"isBuyerMaker\"]\n",
        "    for k in need:\n",
        "        if mapping.get(k) not in df.columns:\n",
        "            raise KeyError(k)\n",
        "    out = pd.DataFrame({\n",
        "    \"ts_ms\": raw[\"time\"],\n",
        "    \"type\":  \"trade\",              # aggTrades are trade prints only\n",
        "    \"side\":  side,\n",
        "    \"price\": raw[\"price\"].astype(float),\n",
        "    # ✅ corrected qty handling\n",
        "    \"qty\":   pd.to_numeric(raw[\"qty\"], errors=\"coerce\").astype(float).clip(lower=0.0).fillna(0.0),\n",
        "    \"venue\": venue_series,\n",
        "    \"symbol\": symbol_name,\n",
        "})\n",
        "    out = out[out[\"time\"].notna()].copy()\n",
        "    return out\n",
        "\n",
        "def _guess_mapping(df):\n",
        "    \"\"\"Handle multiple header styles: lettered (a,p,q,f,l,T,m,M) and long names.\n",
        "       If headers are weird, guess by dtype/patterns.\"\"\"\n",
        "    cols = list(df.columns)\n",
        "    lc   = [str(c).lower() for c in cols]\n",
        "    col_by_lc = dict(zip(lc, cols))\n",
        "\n",
        "    if set([\"a\",\"p\",\"q\",\"f\",\"l\",\"t\",\"m\"]).issubset(set(lc)) or set([\"a\",\"p\",\"q\",\"f\",\"l\",\"t\"]).issubset(set(lc)):\n",
        "        return {\n",
        "            \"tradeId\":      col_by_lc.get(\"a\"),\n",
        "            \"price\":        col_by_lc.get(\"p\"),\n",
        "            \"qty\":          col_by_lc.get(\"q\"),\n",
        "            \"time\":         col_by_lc.get(\"t\"),\n",
        "            \"isBuyerMaker\": col_by_lc.get(\"m\", col_by_lc.get(\"isbuyermaker\")),\n",
        "        }\n",
        "\n",
        "    def pick(cands):\n",
        "        for k in cands:\n",
        "            if k in col_by_lc: return col_by_lc[k]\n",
        "        return None\n",
        "    mapping = {\n",
        "        \"tradeId\":      pick([\"tradeid\",\"tid\",\"id\",\"a\"]),\n",
        "        \"price\":        pick([\"price\",\"p\"]),\n",
        "        \"qty\":          pick([\"qty\",\"quantity\",\"q\"]),\n",
        "        \"time\":         pick([\"time\",\"timestamp\",\"ts\",\"t\",\"T\".lower()]),\n",
        "        \"isBuyerMaker\": pick([\"isbuyermaker\",\"buyer_maker\",\"bm\",\"m\"]),\n",
        "    }\n",
        "    if all(mapping.values()):\n",
        "        return mapping\n",
        "\n",
        "    cand_time = None\n",
        "    max_len = -1\n",
        "    for c in cols:\n",
        "        s = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "        nan_frac = s.isna().mean()\n",
        "        if nan_frac < 0.2:\n",
        "            if (s.max(skipna=True) or 0) > 1e11 and (s.min(skipna=True) or 0) > 1e9:\n",
        "                if s.notna().sum() > max_len:\n",
        "                    max_len = s.notna().sum()\n",
        "                    cand_time = c\n",
        "    cand_price = None\n",
        "    for c in cols:\n",
        "        s = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "        if c == cand_time: continue\n",
        "        if s.notna().sum() and (s.astype(float) % 1 != 0).mean() > 0.1:\n",
        "            cand_price = c; break\n",
        "    cand_qty = None\n",
        "    for c in cols:\n",
        "        if c in (cand_time, cand_price): continue\n",
        "        s = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "        if s.notna().sum() and s.ge(0).mean() > 0.95:\n",
        "            cand_qty = c; break\n",
        "    cand_maker = pick([\"isbuyermaker\",\"buyer_maker\",\"bm\",\"m\"]) or \"m\" if \"m\" in lc else cols[-1]\n",
        "\n",
        "    mapping = {\n",
        "        \"tradeId\": pick([\"tradeid\",\"tid\",\"id\",\"a\"]) or cols[0],\n",
        "        \"price\":   cand_price or pick([\"price\",\"p\"]) or cols[1],\n",
        "        \"qty\":     cand_qty   or pick([\"qty\",\"quantity\",\"q\"]) or cols[2],\n",
        "        \"time\":    cand_time  or pick([\"time\",\"timestamp\",\"ts\",\"t\"]) or cols[3],\n",
        "        \"isBuyerMaker\": cand_maker,\n",
        "    }\n",
        "    return mapping\n",
        "\n",
        "def _read_daily_csv(path_csv):\n",
        "    \"\"\"Read one daily file robustly (headerless or headered).\"\"\"\n",
        "    try:\n",
        "        kw = dict(header=None)\n",
        "        if _HAS_PYARROW: kw[\"engine\"] = \"pyarrow\"\n",
        "        raw = pd.read_csv(path_csv, **kw)\n",
        "        if raw.shape[1] >= 7:\n",
        "            cols = [\"a\",\"p\",\"q\",\"f\",\"l\",\"t\",\"m\",\"M\"][:raw.shape[1]]\n",
        "            raw.columns = cols\n",
        "            mapping = _guess_mapping(raw)\n",
        "            df = _coerce_cols(raw, mapping)\n",
        "        else:\n",
        "            raise ValueError(\"Too few columns in headerless parse\")\n",
        "    except Exception:\n",
        "        kw = dict(header=0)\n",
        "        if _HAS_PYARROW: kw[\"engine\"] = \"pyarrow\"\n",
        "        else: kw[\"low_memory\"] = False\n",
        "        raw = pd.read_csv(path_csv, **kw)\n",
        "        mapping = _guess_mapping(raw)\n",
        "        df = _coerce_cols(raw, mapping)\n",
        "\n",
        "    side_bool = df[\"isBuyerMaker\"].astype(\"string\").str.lower().isin([\"true\",\"1\",\"t\",\"yes\",\"y\"])\n",
        "    side = np.where(side_bool.to_numpy(), \"S\", \"B\")\n",
        "    tradeId = pd.to_numeric(df[\"tradeId\"], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
        "    venues = np.where((tradeId % 2)==0, \"FAST\", \"SLOW\")\n",
        "\n",
        "    ev = pd.DataFrame({\n",
        "    \"ts_ms\": raw[\"time\"].astype(\"int64\"),\n",
        "    \"type\":  \"trade\",\n",
        "    \"side\":  side,\n",
        "    \"price\": raw[\"price\"].astype(float),\n",
        "    \"qty\":   pd.to_numeric(raw[\"qty\"], errors=\"coerce\").astype(float).clip(lower=0.0).fillna(0.0),\n",
        "    \"venue\": venues,\n",
        "    \"symbol\": symbol,\n",
        "})\n",
        "\n",
        "    ev.sort_values(\"ts_ms\", inplace=True)\n",
        "    return ev\n",
        "\n",
        "feed_paths = {}\n",
        "\n",
        "for sym in SYMBOLS:\n",
        "    daily_all = sorted(glob.glob(os.path.join(RAW_DIR, f\"{sym}-aggTrades-*.csv\")))\n",
        "    if not daily_all:\n",
        "        print(f\"[warn] no raw CSVs for {sym}\");\n",
        "        continue\n",
        "\n",
        "    dated = [(p, _parse_ymd_from_name(p)) for p in daily_all]\n",
        "    dated = [(p, d) for p, d in dated if d is not None]\n",
        "    if dated:\n",
        "        dated.sort(key=lambda x: x[1])\n",
        "        use = dated[-DAYS_TO_USE:] if DAYS_TO_USE>0 else dated\n",
        "        daily = [p for p, _ in use]\n",
        "        first_d, last_d = use[0][1], use[-1][1]\n",
        "        print(f\"{sym}: using last {len(daily)} day(s): {first_d} → {last_d} (of {len(daily_all)})\")\n",
        "        suffix = f\"{len(daily)}d\"\n",
        "    else:\n",
        "        daily = daily_all[-DAYS_TO_USE:]\n",
        "        print(f\"{sym}: using last {len(daily)} files (couldn’t parse dates)\")\n",
        "        suffix = f\"{len(daily)}d\"\n",
        "\n",
        "    out_merged = os.path.join(FEEDS_DIR, f\"{sym}_trades_{suffix}.csv\")\n",
        "    if os.path.exists(out_merged): os.remove(out_merged)\n",
        "\n",
        "    first = True; rows_written = 0\n",
        "    for path in daily:\n",
        "        try:\n",
        "            ev = _read_daily_csv(path)\n",
        "            if not ev.empty:\n",
        "                ev.to_csv(out_merged, index=False, header=first, mode=\"a\")\n",
        "                first = False\n",
        "                rows_written += len(ev)\n",
        "            del ev; gc.collect()\n",
        "        except Exception as e:\n",
        "            print(\"skip:\", os.path.basename(path), type(e).__name__, e)\n",
        "            continue\n",
        "\n",
        "    feed_paths[sym] = out_merged if rows_written>0 else None\n",
        "    print(f\"{sym}: wrote {out_merged} | rows={rows_written}\")\n",
        "\n",
        "print(\"Per-symbol stitched feeds:\", feed_paths)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDVLGsWHQ0BT",
        "outputId": "a250bbc7-479f-4d37-a289-dcdac6bf5484"
      },
      "id": "CDVLGsWHQ0BT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbols to process: ['BTCUSDT', 'ETHUSDT', 'SOLUSDT', 'BNBUSDT', 'ADAUSDT']\n",
            "BTCUSDT: using last 15 day(s): 2025-08-25 → 2025-09-08 (of 15)\n",
            "skip: BTCUSDT-aggTrades-2025-08-25.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BTCUSDT-aggTrades-2025-08-26.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BTCUSDT-aggTrades-2025-08-27.csv NameError name 'raw' is not defined\n",
            "skip: BTCUSDT-aggTrades-2025-08-28.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BTCUSDT-aggTrades-2025-08-29.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BTCUSDT-aggTrades-2025-08-30.csv NameError name 'raw' is not defined\n",
            "skip: BTCUSDT-aggTrades-2025-08-31.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BTCUSDT-aggTrades-2025-09-01.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BTCUSDT-aggTrades-2025-09-02.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BTCUSDT-aggTrades-2025-09-03.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BTCUSDT-aggTrades-2025-09-04.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BTCUSDT-aggTrades-2025-09-05.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BTCUSDT-aggTrades-2025-09-06.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BTCUSDT-aggTrades-2025-09-07.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BTCUSDT-aggTrades-2025-09-08.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "BTCUSDT: wrote /content/data/feeds/BTCUSDT_trades_15d.csv | rows=0\n",
            "ETHUSDT: using last 15 day(s): 2025-08-25 → 2025-09-08 (of 15)\n",
            "skip: ETHUSDT-aggTrades-2025-08-25.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: ETHUSDT-aggTrades-2025-08-26.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: ETHUSDT-aggTrades-2025-08-27.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: ETHUSDT-aggTrades-2025-08-28.csv NameError name 'raw' is not defined\n",
            "skip: ETHUSDT-aggTrades-2025-08-29.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: ETHUSDT-aggTrades-2025-08-30.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: ETHUSDT-aggTrades-2025-08-31.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: ETHUSDT-aggTrades-2025-09-01.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: ETHUSDT-aggTrades-2025-09-02.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: ETHUSDT-aggTrades-2025-09-03.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: ETHUSDT-aggTrades-2025-09-04.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: ETHUSDT-aggTrades-2025-09-05.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: ETHUSDT-aggTrades-2025-09-06.csv NameError name 'raw' is not defined\n",
            "skip: ETHUSDT-aggTrades-2025-09-07.csv NameError name 'raw' is not defined\n",
            "skip: ETHUSDT-aggTrades-2025-09-08.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "ETHUSDT: wrote /content/data/feeds/ETHUSDT_trades_15d.csv | rows=0\n",
            "SOLUSDT: using last 15 day(s): 2025-08-25 → 2025-09-08 (of 15)\n",
            "skip: SOLUSDT-aggTrades-2025-08-25.csv NameError name 'raw' is not defined\n",
            "skip: SOLUSDT-aggTrades-2025-08-26.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: SOLUSDT-aggTrades-2025-08-27.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: SOLUSDT-aggTrades-2025-08-28.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: SOLUSDT-aggTrades-2025-08-29.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: SOLUSDT-aggTrades-2025-08-30.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: SOLUSDT-aggTrades-2025-08-31.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: SOLUSDT-aggTrades-2025-09-01.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: SOLUSDT-aggTrades-2025-09-02.csv NameError name 'raw' is not defined\n",
            "skip: SOLUSDT-aggTrades-2025-09-03.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: SOLUSDT-aggTrades-2025-09-04.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: SOLUSDT-aggTrades-2025-09-05.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: SOLUSDT-aggTrades-2025-09-06.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: SOLUSDT-aggTrades-2025-09-07.csv NameError name 'raw' is not defined\n",
            "skip: SOLUSDT-aggTrades-2025-09-08.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "SOLUSDT: wrote /content/data/feeds/SOLUSDT_trades_15d.csv | rows=0\n",
            "BNBUSDT: using last 15 day(s): 2025-08-25 → 2025-09-08 (of 15)\n",
            "skip: BNBUSDT-aggTrades-2025-08-25.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BNBUSDT-aggTrades-2025-08-26.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BNBUSDT-aggTrades-2025-08-27.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BNBUSDT-aggTrades-2025-08-28.csv NameError name 'raw' is not defined\n",
            "skip: BNBUSDT-aggTrades-2025-08-29.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BNBUSDT-aggTrades-2025-08-30.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BNBUSDT-aggTrades-2025-08-31.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BNBUSDT-aggTrades-2025-09-01.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BNBUSDT-aggTrades-2025-09-02.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BNBUSDT-aggTrades-2025-09-03.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BNBUSDT-aggTrades-2025-09-04.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BNBUSDT-aggTrades-2025-09-05.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BNBUSDT-aggTrades-2025-09-06.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: BNBUSDT-aggTrades-2025-09-07.csv NameError name 'raw' is not defined\n",
            "skip: BNBUSDT-aggTrades-2025-09-08.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "BNBUSDT: wrote /content/data/feeds/BNBUSDT_trades_15d.csv | rows=0\n",
            "ADAUSDT: using last 15 day(s): 2025-08-25 → 2025-09-08 (of 15)\n",
            "skip: ADAUSDT-aggTrades-2025-08-25.csv NameError name 'raw' is not defined\n",
            "skip: ADAUSDT-aggTrades-2025-08-26.csv NameError name 'raw' is not defined\n",
            "skip: ADAUSDT-aggTrades-2025-08-27.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: ADAUSDT-aggTrades-2025-08-28.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: ADAUSDT-aggTrades-2025-08-29.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: ADAUSDT-aggTrades-2025-08-30.csv NameError name 'raw' is not defined\n",
            "skip: ADAUSDT-aggTrades-2025-08-31.csv NameError name 'raw' is not defined\n",
            "skip: ADAUSDT-aggTrades-2025-09-01.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: ADAUSDT-aggTrades-2025-09-02.csv NameError name 'raw' is not defined\n",
            "skip: ADAUSDT-aggTrades-2025-09-03.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: ADAUSDT-aggTrades-2025-09-04.csv NameError name 'raw' is not defined\n",
            "skip: ADAUSDT-aggTrades-2025-09-05.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: ADAUSDT-aggTrades-2025-09-06.csv TypeError arg must be a list, tuple, 1-d array, or Series\n",
            "skip: ADAUSDT-aggTrades-2025-09-07.csv NameError name 'raw' is not defined\n",
            "skip: ADAUSDT-aggTrades-2025-09-08.csv NameError name 'raw' is not defined\n",
            "ADAUSDT: wrote /content/data/feeds/ADAUSDT_trades_15d.csv | rows=0\n",
            "Per-symbol stitched feeds: {'BTCUSDT': None, 'ETHUSDT': None, 'SOLUSDT': None, 'BNBUSDT': None, 'ADAUSDT': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Scanner\n",
        "import os, glob, pandas as pd\n",
        "FEEDS_DIR = \"/content/data/feeds\"\n",
        "EVENT_DIR = \"/content/data/events\"\n",
        "SYMBOLS   = [\"BTCUSDT\",\"ETHUSDT\",\"SOLUSDT\",\"BNBUSDT\",\"ADAUSDT\"]\n",
        "\n",
        "rows = []\n",
        "for s in SYMBOLS:\n",
        "    stitched = f\"{FEEDS_DIR}/{s}_trades_3d.csv\"\n",
        "    shards   = sorted(glob.glob(f\"{EVENT_DIR}/{s}_trades_*.csv\"))\n",
        "    if os.path.exists(stitched):\n",
        "        try:\n",
        "            ts = pd.read_csv(stitched, usecols=[\"ts_ms\"])\n",
        "            span_h = round((ts[\"ts_ms\"].iloc[-1]-ts[\"ts_ms\"].iloc[0])/1000/3600,2); n = len(ts)\n",
        "        except Exception:\n",
        "            span_h, n = None, None\n",
        "        rows.append((s,\"stitched\",stitched,span_h,n))\n",
        "    else:\n",
        "        rows.append((s,\"missing\",\"-\",None,None))\n",
        "    rows.append((s,\"shards\",len(shards),None,None))\n",
        "pd.DataFrame(rows, columns=[\"Symbol\",\"Type\",\"Path/Count\",\"Span (h)\",\"Rows\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "uWLvHd_dl4F1",
        "outputId": "81dd8f88-1e4e-483e-e604-5d53e4b64630"
      },
      "id": "uWLvHd_dl4F1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Symbol     Type Path/Count Span (h)  Rows\n",
              "0  BTCUSDT  missing          -     None  None\n",
              "1  BTCUSDT   shards          0     None  None\n",
              "2  ETHUSDT  missing          -     None  None\n",
              "3  ETHUSDT   shards          0     None  None\n",
              "4  SOLUSDT  missing          -     None  None\n",
              "5  SOLUSDT   shards          0     None  None\n",
              "6  BNBUSDT  missing          -     None  None\n",
              "7  BNBUSDT   shards          0     None  None\n",
              "8  ADAUSDT  missing          -     None  None\n",
              "9  ADAUSDT   shards          0     None  None"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a310c6b3-3449-4918-bc73-0530045f501e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Symbol</th>\n",
              "      <th>Type</th>\n",
              "      <th>Path/Count</th>\n",
              "      <th>Span (h)</th>\n",
              "      <th>Rows</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BTCUSDT</td>\n",
              "      <td>missing</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BTCUSDT</td>\n",
              "      <td>shards</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ETHUSDT</td>\n",
              "      <td>missing</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ETHUSDT</td>\n",
              "      <td>shards</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SOLUSDT</td>\n",
              "      <td>missing</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SOLUSDT</td>\n",
              "      <td>shards</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>BNBUSDT</td>\n",
              "      <td>missing</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>BNBUSDT</td>\n",
              "      <td>shards</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>ADAUSDT</td>\n",
              "      <td>missing</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ADAUSDT</td>\n",
              "      <td>shards</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a310c6b3-3449-4918-bc73-0530045f501e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a310c6b3-3449-4918-bc73-0530045f501e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a310c6b3-3449-4918-bc73-0530045f501e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-908a77ab-4c35-4cd6-8c01-cba8061b56de\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-908a77ab-4c35-4cd6-8c01-cba8061b56de')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-908a77ab-4c35-4cd6-8c01-cba8061b56de button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download last N days\n",
        "import os, subprocess, datetime as dt\n",
        "\n",
        "SYMBOLS  = [\"BTCUSDT\",\"ETHUSDT\",\"SOLUSDT\",\"BNBUSDT\",\"ADAUSDT\"]\n",
        "N_DAYS   = 3\n",
        "RAW_DIR  = \"/content/data/raw\"; os.makedirs(RAW_DIR, exist_ok=True)\n",
        "\n",
        "start_day = dt.date.today() - dt.timedelta(days=N_DAYS)\n",
        "def dl_day(sym, d):\n",
        "    ymd = d.strftime(\"%Y-%m-%d\")\n",
        "    url = f\"https://data.binance.vision/data/spot/daily/aggTrades/{sym}/{sym}-aggTrades-{ymd}.zip\"\n",
        "    fn  = os.path.join(RAW_DIR, f\"{sym}-aggTrades-{ymd}.zip\")\n",
        "    r = subprocess.run([\"wget\",\"-q\",\"-O\",fn,\"-c\",url], check=False)\n",
        "    return fn if os.path.exists(fn) else None\n",
        "\n",
        "got = []\n",
        "for s in SYMBOLS:\n",
        "    print(f\"Downloading {s} ...\")\n",
        "    for i in range(N_DAYS):\n",
        "        fn = dl_day(s, start_day + dt.timedelta(days=i))\n",
        "        if fn: got.append(fn)\n",
        "print(\"downloaded:\", len(got), \"files\")\n",
        "\n",
        "# Unzip\n",
        "subprocess.run([\"bash\",\"-lc\", f'unzip -o \"{RAW_DIR}/*.zip\" -d \"{RAW_DIR}\" >/dev/null 2>&1 || true'], check=False)\n",
        "print(\"unzipped into\", RAW_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5e2h81Fl7u-",
        "outputId": "d8a8d695-bc5f-46e4-bcd8-880b16946126"
      },
      "id": "p5e2h81Fl7u-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading BTCUSDT ...\n",
            "Downloading ETHUSDT ...\n",
            "Downloading SOLUSDT ...\n",
            "Downloading BNBUSDT ...\n",
            "Downloading ADAUSDT ...\n",
            "downloaded: 15 files\n",
            "unzipped into /content/data/raw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert aggTrades\n",
        "import os, glob, numpy as np, pandas as pd\n",
        "\n",
        "RAW_DIR   = \"/content/data/raw\"\n",
        "EVENT_DIR = \"/content/data/events\"; os.makedirs(EVENT_DIR, exist_ok=True)\n",
        "\n",
        "def convert_one_csv(path_csv, symbol=\"BTCUSDT\"):\n",
        "    raw_try = pd.read_csv(path_csv, header=None, low_memory=False)\n",
        "    if raw_try.shape[1] >= 7:\n",
        "        if raw_try.shape[1] >= 8:\n",
        "            raw_try.columns = [\"tradeId\",\"price\",\"qty\",\"firstId\",\"lastId\",\"time\",\"isBuyerMaker\",\"isBestMatch\"]\n",
        "        else:\n",
        "            raw_try.columns = [\"tradeId\",\"price\",\"qty\",\"firstId\",\"lastId\",\"time\",\"isBuyerMaker\"]\n",
        "        raw = raw_try\n",
        "    else:\n",
        "        raw = pd.read_csv(path_csv, header=0, low_memory=False)\n",
        "        cols = [c.lower() for c in raw.columns]\n",
        "        def mapcol(names):\n",
        "            for k in names:\n",
        "                if k in cols: return raw.columns[cols.index(k)]\n",
        "            return None\n",
        "        rename = {}\n",
        "        idc   = mapcol([\"tradeid\",\"tid\",\"id\"])\n",
        "        price = mapcol([\"price\",\"p\"])\n",
        "        qty   = mapcol([\"qty\",\"quantity\",\"q\"])\n",
        "        timec = mapcol([\"time\",\"t\",\"timestamp\",\"ts\"])\n",
        "        isbm  = mapcol([\"isbuyermaker\",\"bm\",\"buyer_maker\"])\n",
        "        if idc:   rename[idc]   = \"tradeId\"\n",
        "        if price: rename[price] = \"price\"\n",
        "        if qty:   rename[qty]   = \"qty\"\n",
        "        if timec: rename[timec] = \"time\"\n",
        "        if isbm:  rename[isbm]  = \"isBuyerMaker\"\n",
        "        raw = raw.rename(columns=rename)\n",
        "\n",
        "    raw[\"time\"]  = pd.to_numeric(raw[\"time\"],  errors=\"coerce\")\n",
        "    raw[\"price\"] = pd.to_numeric(raw[\"price\"], errors=\"coerce\")\n",
        "    raw[\"qty\"]   = pd.to_numeric(raw[\"qty\"],   errors=\"coerce\")\n",
        "    raw = raw[raw[\"time\"].notna()].copy()\n",
        "\n",
        "    side = raw[\"isBuyerMaker\"].astype(str).str.lower().isin([\"true\",\"1\",\"t\",\"yes\"])\n",
        "    side = np.where(side, \"S\", \"B\")\n",
        "    venues = np.where(pd.to_numeric(raw[\"tradeId\"], errors=\"coerce\").fillna(0).astype(int) % 2 == 0, \"FAST\", \"SLOW\")\n",
        "\n",
        "    ev = pd.DataFrame({\n",
        "        \"ts_ms\": raw[\"time\"].astype(\"int64\"),\n",
        "        \"type\":  \"trade\",\n",
        "        \"side\":  side,\n",
        "        \"price\": raw[\"price\"].astype(float),\n",
        "        \"qty\":   pd.to_numeric(raw[\"qty\"], errors=\"coerce\").astype(float).clip(lower=0.0).fillna(0.0),\n",
        "        \"venue\": venues,\n",
        "        \"symbol\": symbol,\n",
        "    })\n",
        "    return ev\n",
        "\n",
        "daily_out = 0\n",
        "for s in SYMBOLS:\n",
        "    csvs = sorted(glob.glob(os.path.join(RAW_DIR, f\"{s}-aggTrades-*.csv\")))\n",
        "    for c in csvs:\n",
        "        try:\n",
        "            ev = convert_one_csv(c, symbol=s)\n",
        "            out_daily = os.path.join(EVENT_DIR, f\"{s}_trades_{os.path.basename(c)}\")\n",
        "            ev.to_csv(out_daily, index=False)\n",
        "            daily_out += 1\n",
        "        except Exception as e:\n",
        "            print(\"skip malformed day:\", os.path.basename(c), type(e).__name__, str(e))\n",
        "print(\"wrote daily events:\", daily_out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhiFJkrEl_ZW",
        "outputId": "94993433-d28f-4e44-ee9a-ba7b406c8c5e"
      },
      "id": "BhiFJkrEl_ZW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wrote daily events: 75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stitch last DAYS\n",
        "import os, glob, pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "EVENT_DIR = \"/content/data/events\"\n",
        "FEEDS_DIR = \"/content/data/feeds\"; os.makedirs(FEEDS_DIR, exist_ok=True)\n",
        "DAYS_USE  = 3\n",
        "\n",
        "def _parse_date_from_daily(path):\n",
        "    bn = os.path.basename(path)\n",
        "    try:\n",
        "        y, m, d_csv = bn.split(\"-\")[-3:]\n",
        "        d = d_csv.replace(\".csv\",\"\")\n",
        "        return datetime.strptime(f\"{y}-{m}-{d}\", \"%Y-%m-%d\").date()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "for s in SYMBOLS:\n",
        "    daily = sorted(glob.glob(os.path.join(EVENT_DIR, f\"{s}_trades_{s}-aggTrades-*.csv\")))\n",
        "    if not daily:\n",
        "        print(f\"[{s}] no daily event files; run the converter first.\"); continue\n",
        "    dated = [(p, _parse_date_from_daily(p)) for p in daily]\n",
        "    dated = [(p,d) for p,d in dated if d is not None]\n",
        "    if not dated:\n",
        "        print(f\"[{s}] no parsable dates in filenames.\"); continue\n",
        "    dated.sort(key=lambda x: x[1])\n",
        "    use = [p for p,_ in dated[-DAYS_USE:]]   # last N days\n",
        "    parts = [pd.read_csv(p) for p in use]\n",
        "    big = pd.concat(parts, ignore_index=True).sort_values(\"ts_ms\").reset_index(drop=True)\n",
        "    out = os.path.join(FEEDS_DIR, f\"{s}_trades_{DAYS_USE}d.csv\")\n",
        "    big.to_csv(out, index=False)\n",
        "    print(f\"[{s}] stitched -> {out} | rows={len(big):,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0Pke_D8mDAk",
        "outputId": "6cb8c27a-60ce-4ee1-bfb2-77d906d74e3a"
      },
      "id": "Q0Pke_D8mDAk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BTCUSDT] stitched -> /content/data/feeds/BTCUSDT_trades_3d.csv | rows=1,272,032\n",
            "[ETHUSDT] stitched -> /content/data/feeds/ETHUSDT_trades_3d.csv | rows=1,750,428\n",
            "[SOLUSDT] stitched -> /content/data/feeds/SOLUSDT_trades_3d.csv | rows=975,406\n",
            "[BNBUSDT] stitched -> /content/data/feeds/BNBUSDT_trades_3d.csv | rows=656,743\n",
            "[ADAUSDT] stitched -> /content/data/feeds/ADAUSDT_trades_3d.csv | rows=189,755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dacd835a",
      "metadata": {
        "id": "dacd835a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "fc6fe68f-514c-4815-cc42-9f3bc63b0731"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Event feed availability:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    Symbol    Source                                 Path/Count  Span (h)  \\\n",
              "0  BTCUSDT  stitched  /content/data/feeds/BTCUSDT_trades_3d.csv  71999.76   \n",
              "1  ETHUSDT  stitched  /content/data/feeds/ETHUSDT_trades_3d.csv  71999.26   \n",
              "2  SOLUSDT  stitched  /content/data/feeds/SOLUSDT_trades_3d.csv  71999.58   \n",
              "3  BNBUSDT  stitched  /content/data/feeds/BNBUSDT_trades_3d.csv  71999.52   \n",
              "4  ADAUSDT  stitched  /content/data/feeds/ADAUSDT_trades_3d.csv  71999.36   \n",
              "\n",
              "      Rows  \n",
              "0  1272032  \n",
              "1  1750428  \n",
              "2   975406  \n",
              "3   656743  \n",
              "4   189755  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-844538d6-bca0-4048-bb55-e5fdf1fc9383\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Symbol</th>\n",
              "      <th>Source</th>\n",
              "      <th>Path/Count</th>\n",
              "      <th>Span (h)</th>\n",
              "      <th>Rows</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BTCUSDT</td>\n",
              "      <td>stitched</td>\n",
              "      <td>/content/data/feeds/BTCUSDT_trades_3d.csv</td>\n",
              "      <td>71999.76</td>\n",
              "      <td>1272032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ETHUSDT</td>\n",
              "      <td>stitched</td>\n",
              "      <td>/content/data/feeds/ETHUSDT_trades_3d.csv</td>\n",
              "      <td>71999.26</td>\n",
              "      <td>1750428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SOLUSDT</td>\n",
              "      <td>stitched</td>\n",
              "      <td>/content/data/feeds/SOLUSDT_trades_3d.csv</td>\n",
              "      <td>71999.58</td>\n",
              "      <td>975406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BNBUSDT</td>\n",
              "      <td>stitched</td>\n",
              "      <td>/content/data/feeds/BNBUSDT_trades_3d.csv</td>\n",
              "      <td>71999.52</td>\n",
              "      <td>656743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ADAUSDT</td>\n",
              "      <td>stitched</td>\n",
              "      <td>/content/data/feeds/ADAUSDT_trades_3d.csv</td>\n",
              "      <td>71999.36</td>\n",
              "      <td>189755</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-844538d6-bca0-4048-bb55-e5fdf1fc9383')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-844538d6-bca0-4048-bb55-e5fdf1fc9383 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-844538d6-bca0-4048-bb55-e5fdf1fc9383');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c6e87023-fcd6-4d05-847e-2c4c9a95b65e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c6e87023-fcd6-4d05-847e-2c4c9a95b65e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c6e87023-fcd6-4d05-847e-2c4c9a95b65e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_30b1f8de-6d11-42e9-87a0-6f14ad9b62cd\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('report')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_30b1f8de-6d11-42e9-87a0-6f14ad9b62cd button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('report');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "report",
              "summary": "{\n  \"name\": \"report\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Symbol\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"ETHUSDT\",\n          \"ADAUSDT\",\n          \"SOLUSDT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"stitched\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Path/Count\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"/content/data/feeds/ETHUSDT_trades_3d.csv\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Span (h)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.19462785001126973,\n        \"min\": 71999.26,\n        \"max\": 71999.76,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          71999.26\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rows\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 593132,\n        \"min\": 189755,\n        \"max\": 1750428,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1750428\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default feed set to: BTCUSDT [stitched] -> /content/data/feeds/BTCUSDT_trades_3d.csv\n"
          ]
        }
      ],
      "source": [
        "#Event feed chooser\n",
        "import os, glob, pandas as pd\n",
        "\n",
        "SYMBOLS   = [\"BTCUSDT\",\"ETHUSDT\",\"SOLUSDT\",\"BNBUSDT\",\"ADAUSDT\"]\n",
        "DAYS_USE  = 3\n",
        "STEP_MS   = 500\n",
        "TICK_SIZE = 0.01\n",
        "\n",
        "FEEDS_DIR = \"/content/data/feeds\"\n",
        "EVENT_DIR = \"/content/data/events\"\n",
        "\n",
        "def _stitched_path(sym): return f\"{FEEDS_DIR}/{sym}_trades_{DAYS_USE}d.csv\"\n",
        "def _shard_paths(sym):   return sorted(glob.glob(f\"{EVENT_DIR}/{sym}_trades_*.csv\"))\n",
        "\n",
        "def _span_hours_csv(csv_path):\n",
        "    try:\n",
        "        ts = pd.read_csv(csv_path, usecols=[\"ts_ms\"])\n",
        "        return round((ts[\"ts_ms\"].iloc[-1] - ts[\"ts_ms\"].iloc[0]) / 1000 / 3600, 2), len(ts)\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "feeds       = {}\n",
        "feed_paths  = {}\n",
        "sources     = {}\n",
        "missing     = []\n",
        "shards_used = {}\n",
        "\n",
        "for sym in SYMBOLS:\n",
        "    stitched = _stitched_path(sym)\n",
        "    shard_list = _shard_paths(sym)\n",
        "    if os.path.exists(stitched):\n",
        "        feeds[sym] = DataFeed(stitched, step_ms=STEP_MS, tick_size=TICK_SIZE, default_venue=\"FAST\")\n",
        "        feed_paths[sym] = stitched\n",
        "        sources[sym] = \"stitched\"\n",
        "    elif \"ShardedDataFeed\" in globals() and len(shard_list) >= DAYS_USE:\n",
        "        last_n = shard_list[-DAYS_USE:]\n",
        "        feeds[sym] = ShardedDataFeed(last_n, step_ms=STEP_MS, tick_size=TICK_SIZE, default_venue=\"FAST\")\n",
        "        feed_paths[sym] = f\"{len(last_n)} shards\"\n",
        "        shards_used[sym] = last_n\n",
        "        sources[sym] = \"shards\"\n",
        "    else:\n",
        "        missing.append(sym)\n",
        "\n",
        "rows = []\n",
        "for sym in SYMBOLS:\n",
        "    if sym in feeds:\n",
        "        if sources[sym] == \"stitched\":\n",
        "            span_h, nrows = _span_hours_csv(feed_paths[sym])\n",
        "            rows.append((sym, sources[sym], feed_paths[sym], span_h, nrows))\n",
        "        else:\n",
        "            last_n = shards_used.get(sym, [])\n",
        "            rows.append((sym, sources[sym], f\"{len(last_n)} shards\", None, None))\n",
        "    else:\n",
        "        rows.append((sym, \"missing\", \"-\", None, None))\n",
        "\n",
        "report = pd.DataFrame(rows, columns=[\"Symbol\",\"Source\",\"Path/Count\",\"Span (h)\",\"Rows\"])\n",
        "print(\"Event feed availability:\")\n",
        "display(report)\n",
        "\n",
        "if feeds:\n",
        "    DEFAULT_SYMBOL = next(sym for sym in SYMBOLS if sym in feeds)\n",
        "    feed = feeds[DEFAULT_SYMBOL]\n",
        "    print(f\"Default feed set to: {DEFAULT_SYMBOL} [{sources[DEFAULT_SYMBOL]}] -> {feed_paths[DEFAULT_SYMBOL]}\")\n",
        "else:\n",
        "    raise FileNotFoundError(\n",
        "        \"No suitable multi-day feed found for any symbol. \"\n",
        "        \"Create stitched 3-day files in /content/data/feeds (e.g. BTCUSDT_trades_3d.csv), \"\n",
        "        \"or ensure ShardedDataFeed is available with daily event shards in /content/data/events.\"\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3dc0cc3",
      "metadata": {
        "id": "e3dc0cc3"
      },
      "outputs": [],
      "source": [
        "#Core dataclasses\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class StepResult:\n",
        "    state: object\n",
        "    reward: float\n",
        "    done: bool\n",
        "    info: dict\n",
        "\n",
        "@dataclass\n",
        "class LatencyModel:\n",
        "    base_steps: int = 0\n",
        "    jitter_steps: int = 0\n",
        "\n",
        "@dataclass\n",
        "class Venue:\n",
        "    name: str\n",
        "    book: OrderBook\n",
        "    maker_bps: float\n",
        "    taker_bps: float\n",
        "    tick: int = 1\n",
        "    def snapshot(self):\n",
        "        bb = self.book.bids[0].price if self.book.bids else None\n",
        "        ba = self.book.asks[0].price if self.book.asks else None\n",
        "        if bb is not None and ba is not None: m = 0.5*(bb+ba)\n",
        "        elif bb is not None:                 m = bb\n",
        "        elif ba is not None:                 m = ba\n",
        "        else:                                m = None\n",
        "        return bb, ba, m\n",
        "\n",
        "@dataclass\n",
        "class SORVenue:\n",
        "    venue: Venue\n",
        "    latency: LatencyModel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "588321fb",
      "metadata": {
        "id": "588321fb"
      },
      "outputs": [],
      "source": [
        "#Minimal base env\n",
        "\n",
        "class ExecEnvLatencyV2:\n",
        "    \"\"\"\n",
        "    Holds venues, running totals, simple helpers. Child class (Real) adds feed + step().\n",
        "    \"\"\"\n",
        "    def __init__(self, vl, total_qty=120, max_steps=5000, target_participation=0.12,\n",
        "                 queue_loss_penalty_bps=3.0, time_penalty_bps_per_step=0.0,\n",
        "                 fill_bonus_bps_per_share=0.0, reward_clip_bps=500.0, **kwargs):\n",
        "        import numpy as _np\n",
        "        self.vl = vl\n",
        "        self.total_qty = int(total_qty)\n",
        "        self.remaining = int(total_qty)\n",
        "        self.max_steps = int(max_steps)\n",
        "        self.t = 0\n",
        "        self.target_participation   = float(target_participation)\n",
        "        self.queue_loss_penalty_bps = float(queue_loss_penalty_bps)\n",
        "        self.time_penalty_bps_per_step = float(time_penalty_bps_per_step)\n",
        "        self.fill_bonus_bps_per_share  = float(fill_bonus_bps_per_share)\n",
        "        self.reward_clip_bps = float(reward_clip_bps)\n",
        "        self.cum_spent = 0.0\n",
        "        self.cum_fee   = 0.0\n",
        "        self.cum_filled= 0\n",
        "        self.last_step_fills = 0\n",
        "        self.last_step_queue_loss_qty = 0\n",
        "        self.recent_bg_vol = [0]*6\n",
        "        self.arrival0 = self._arrival_mid_global()\n",
        "\n",
        "    def _arrival_mid_global(self):\n",
        "        mids = []\n",
        "        for sv in self.vl:\n",
        "            _, _, m = sv.venue.snapshot()\n",
        "            if m is not None: mids.append(m)\n",
        "        return float(sum(mids)/len(mids)) if mids else None\n",
        "\n",
        "    def _state(self):\n",
        "        return {\"t\": self.t, \"remaining\": self.remaining, \"cum_filled\": self.cum_filled}\n",
        "\n",
        "    def _route_and_queue(self, taker_qty, passive_qty, passive_offset): return {\"maker\": int(passive_qty), \"taker\": int(taker_qty)}\n",
        "    def _process_arrivals(self): pass\n",
        "\n",
        "    def reset(self):\n",
        "        self.remaining = int(self.total_qty)\n",
        "        self.t = 0\n",
        "        self.cum_spent = 0.0\n",
        "        self.cum_fee   = 0.0\n",
        "        self.cum_filled= 0\n",
        "        self.last_step_fills = 0\n",
        "        self.last_step_queue_loss_qty = 0\n",
        "        self.recent_bg_vol = [0]*6\n",
        "        self.arrival0 = self._arrival_mid_global()\n",
        "        return self._state()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66eeba4d",
      "metadata": {
        "id": "66eeba4d"
      },
      "outputs": [],
      "source": [
        "#ExecEnvLatencyV2Real\n",
        "\n",
        "\n",
        "class ExecEnvLatencyV2Real(ExecEnvLatencyV2):\n",
        "    def __init__(self, vl, feed: DataFeed, tick_size=1.0, **kwargs):\n",
        "        self.feed = feed\n",
        "        self.tick_size = float(tick_size)\n",
        "        super().__init__(vl=vl, **kwargs)\n",
        "\n",
        "    def reset(self):\n",
        "        self.feed.reset()\n",
        "        return super().reset()\n",
        "\n",
        "    def _apply_feed_step(self):\n",
        "        \"\"\"\n",
        "        Apply one bucket of real L2 events to the proper venue books.\n",
        "        Uses explicit add/cancel deltas at price levels; does NOT fake crosses.\n",
        "        \"\"\"\n",
        "        evs = self.feed.next_events()\n",
        "        if not evs:\n",
        "            return 0\n",
        "        applied = 0\n",
        "        name_map = {sv.venue.name: sv for sv in self.vl}\n",
        "        default_sv = self.vl[0] if self.vl else None\n",
        "\n",
        "        for ev in evs:\n",
        "            sv = name_map.get(ev.get(\"venue\"), default_sv)\n",
        "            if sv is None:\n",
        "                continue\n",
        "            side = \"BUY\" if ev.get(\"side\") == \"B\" else \"SELL\"\n",
        "            px   = price_to_int(ev.get(\"price\"), self.tick_size)\n",
        "            qty  = int(ev.get(\"qty\", 0))\n",
        "            typ  = ev.get(\"type\")\n",
        "\n",
        "            if typ == \"add\" and qty > 0:\n",
        "                sv.venue.book.submit(side, px, qty)\n",
        "                applied += 1\n",
        "            elif typ == \"cancel\" and qty > 0:\n",
        "                book = sv.venue.book\n",
        "                if side == \"BUY\" and book.bids:\n",
        "                    qleft = qty\n",
        "                    for i in range(len(book.bids) - 1, -1, -1):\n",
        "                        o = book.bids[i]\n",
        "                        if o.price == px and qleft > 0:\n",
        "                            take = min(o.qty, qleft)\n",
        "                            o.qty -= take\n",
        "                            qleft -= take\n",
        "                            if o.qty <= 0:\n",
        "                                book.bids.remove(o)\n",
        "                    applied += 1\n",
        "                elif side == \"SELL\" and book.asks:\n",
        "                    qleft = qty\n",
        "                    for i in range(len(book.asks) - 1, -1, -1):\n",
        "                        o = book.asks[i]\n",
        "                        if o.price == px and qleft > 0:\n",
        "                            take = min(o.qty, qleft)\n",
        "                            o.qty -= take\n",
        "                            qleft -= take\n",
        "                            if o.qty <= 0:\n",
        "                                book.asks.remove(o)\n",
        "                    applied += 1\n",
        "            else:\n",
        "                pass\n",
        "        return applied\n",
        "\n",
        "    def step(self, action):\n",
        "        taker_share    = float(np.clip(action[0], 0.0, 1.0))\n",
        "        passive_offset = int(np.clip(round(action[1]), 0, 2))\n",
        "\n",
        "\n",
        "        step_ref_mid = self._arrival_mid_global()\n",
        "\n",
        "        _ = self._apply_feed_step()\n",
        "\n",
        "        bg_before = [len(sv.venue.book.trades) for sv in self.vl]\n",
        "\n",
        "        before_fills = {sv.venue.name: len(sv.venue.book.trades) for sv in self.vl}\n",
        "\n",
        "\n",
        "        self._arrived_this_step = {}\n",
        "        self._process_arrivals()\n",
        "\n",
        "        step_spent  = 0.0\n",
        "        step_fee    = 0.0\n",
        "        step_filled = 0\n",
        "\n",
        "        arrived = getattr(self, \"_arrived_this_step\", {})\n",
        "\n",
        "        for sv in self.vl:\n",
        "            name = sv.venue.name\n",
        "            new_trades = sv.venue.book.trades[before_fills[name]:]\n",
        "\n",
        "            ledger = [dict(px=e[\"px\"], qty=int(e[\"qty\"]), role=e[\"role\"])\n",
        "                      for e in arrived.get(name, [])]\n",
        "\n",
        "            for _, price, q in new_trades:\n",
        "                use_role = \"taker\"\n",
        "                for entry in ledger:\n",
        "                    if entry[\"qty\"] <= 0:\n",
        "                        continue\n",
        "                    if entry[\"px\"] == price:\n",
        "                        use_role = entry[\"role\"]\n",
        "                        consume  = min(q, entry[\"qty\"])\n",
        "                        entry[\"qty\"] -= consume\n",
        "                        break\n",
        "\n",
        "                role_fee_bps = sv.venue.taker_bps if use_role == \"taker\" else sv.venue.maker_bps\n",
        "                step_spent  += price * q\n",
        "                step_fee    += (role_fee_bps / 1e4) * price * q\n",
        "                step_filled += q\n",
        "\n",
        "        bg_vol = 0\n",
        "        for i, sv in enumerate(self.vl):\n",
        "            new = sv.venue.book.trades[bg_before[i]:]\n",
        "            bg_vol += int(sum(q for _, _, q in new))\n",
        "        self.recent_bg_vol.pop(0); self.recent_bg_vol.append(bg_vol)\n",
        "        avg_bg = max(1, int(np.mean(self.recent_bg_vol)))\n",
        "\n",
        "        if self.remaining > 0 and self.t < self.max_steps:\n",
        "            pov_target  = max(1, int(np.ceil(self.target_participation * avg_bg)))\n",
        "            taker_qty   = min(int(np.floor(taker_share * pov_target)), self.remaining)\n",
        "            passive_qty = max(0, min(pov_target - taker_qty, self.remaining - taker_qty))\n",
        "            intent = self._route_and_queue(taker_qty, passive_qty, passive_offset)\n",
        "        else:\n",
        "            intent = {\"maker\": 0, \"taker\": 0}\n",
        "\n",
        "        queue_loss_qty = 0\n",
        "        if intent[\"maker\"] > 0 and step_filled == 0 and step_ref_mid is not None:\n",
        "            now_mid = self._arrival_mid_global()\n",
        "            if now_mid is not None and now_mid > step_ref_mid:\n",
        "                queue_loss_qty = intent[\"maker\"]\n",
        "\n",
        "        self.cum_spent   += step_spent\n",
        "        self.cum_fee     += step_fee\n",
        "        self.cum_filled  += step_filled\n",
        "        self.remaining   -= step_filled\n",
        "        self.last_step_fills          = step_filled\n",
        "        self.last_step_queue_loss_qty = queue_loss_qty\n",
        "\n",
        "        reward_is = 0.0\n",
        "        if step_filled > 0 and step_ref_mid:\n",
        "            vwap_step_net = (step_spent + step_fee) / step_filled\n",
        "            reward_is = -(((vwap_step_net - step_ref_mid) / step_ref_mid) * 1e4)\n",
        "\n",
        "        remaining_frac = self.remaining / max(1, self.total_qty)\n",
        "        reward_queue   = - self.queue_loss_penalty_bps * (queue_loss_qty / max(1, intent[\"maker\"] if intent[\"maker\"]>0 else 1))\n",
        "        reward_time    = - self.time_penalty_bps_per_step * remaining_frac\n",
        "        reward_fillb   =   self.fill_bonus_bps_per_share * step_filled\n",
        "        reward_step    = reward_is + reward_queue + reward_time + reward_fillb\n",
        "        reward_step    = float(np.clip(reward_step, -self.reward_clip_bps, self.reward_clip_bps))\n",
        "\n",
        "        self.t += 1\n",
        "        done = (self.remaining <= 0) or (self.t >= self.max_steps)\n",
        "\n",
        "        info = {\n",
        "            \"filled\": step_filled,\n",
        "            \"queue_loss_qty\": queue_loss_qty,\n",
        "            \"remaining\": self.remaining,\n",
        "            \"vwap_net_cum\": (self.cum_spent + self.cum_fee) / self.cum_filled if self.cum_filled else np.nan,\n",
        "            \"is_bps_cum_vs_start\": (((self.cum_spent + self.cum_fee)/max(1, self.cum_filled) - self.arrival0) / self.arrival0) * 1e4\n",
        "                                   if self.cum_filled and self.arrival0 else np.nan,\n",
        "        }\n",
        "        return StepResult(self._state(), reward_step, done, info)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e5a3d59",
      "metadata": {
        "id": "4e5a3d59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6971b318-cea5-4b5e-d72a-a9cb03ab547a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env_real ready | venues: ['FAST', 'SLOW']\n",
            "Params: {'FAST': {'maker_bps': -1.0, 'taker_bps': 1.5, 'lat_base': 0, 'lat_jitter': 0, 'tick': 1}, 'SLOW': {'maker_bps': -3.0, 'taker_bps': 2.8, 'lat_base': 1, 'lat_jitter': 2, 'tick': 1}}\n"
          ]
        }
      ],
      "source": [
        "#Build venues & real replay env\n",
        "\n",
        "\n",
        "import os, numpy as np\n",
        "\n",
        "VENUE_PARAMS = {\n",
        "    \"FAST\": {\"maker_bps\": -1.0, \"taker_bps\": 1.5, \"lat_base\": 0, \"lat_jitter\": 0, \"tick\": 1},\n",
        "    \"SLOW\": {\"maker_bps\": -3.0, \"taker_bps\": 2.8, \"lat_base\": 1, \"lat_jitter\": 2, \"tick\": 1},\n",
        "}\n",
        "\n",
        "fast = SORVenue(\n",
        "    venue=Venue(\n",
        "        \"FAST\",\n",
        "        OrderBook(),\n",
        "        maker_bps=VENUE_PARAMS[\"FAST\"][\"maker_bps\"],\n",
        "        taker_bps=VENUE_PARAMS[\"FAST\"][\"taker_bps\"],\n",
        "        tick=VENUE_PARAMS[\"FAST\"][\"tick\"],\n",
        "    ),\n",
        "    latency=LatencyModel(\n",
        "        base_steps=VENUE_PARAMS[\"FAST\"][\"lat_base\"],\n",
        "        jitter_steps=VENUE_PARAMS[\"FAST\"][\"lat_jitter\"],\n",
        "    ),\n",
        ")\n",
        "\n",
        "slow = SORVenue(\n",
        "    venue=Venue(\n",
        "        \"SLOW\",\n",
        "        OrderBook(),\n",
        "        maker_bps=VENUE_PARAMS[\"SLOW\"][\"maker_bps\"],\n",
        "        taker_bps=VENUE_PARAMS[\"SLOW\"][\"taker_bps\"],\n",
        "        tick=VENUE_PARAMS[\"SLOW\"][\"tick\"],\n",
        "    ),\n",
        "    latency=LatencyModel(\n",
        "        base_steps=VENUE_PARAMS[\"SLOW\"][\"lat_base\"],\n",
        "        jitter_steps=VENUE_PARAMS[\"SLOW\"][\"lat_jitter\"],\n",
        "    ),\n",
        ")\n",
        "\n",
        "if \"feed\" not in globals():\n",
        "    L2_PATH     = \"/content/data/l2_feed.csv\"\n",
        "    TRADES_PATH = \"/content/data/binance_trades_feed.csv\"\n",
        "    STEP_MS     = 100\n",
        "\n",
        "    if os.path.exists(L2_PATH):\n",
        "        feed_path = L2_PATH\n",
        "        source_label = \"L2 depth (add/cancel)\"\n",
        "    elif os.path.exists(TRADES_PATH):\n",
        "        feed_path = TRADES_PATH\n",
        "        source_label = \"trades-only (aggTrades converter)\"\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            \"No event feed found. Record depth to /content/data/l2_feed.csv or create \"\n",
        "            \"/content/data/binance_trades_feed.csv with the converter.\"\n",
        "        )\n",
        "\n",
        "    feed = DataFeed(feed_path, step_ms=STEP_MS, tick_size=1.0, default_venue=\"FAST\")\n",
        "    print(f\"Using event feed: {feed_path} | source={source_label} | step_ms={STEP_MS}\")\n",
        "\n",
        "env_real = ExecEnvLatencyV2Real(\n",
        "    [fast, slow],\n",
        "    feed=feed,\n",
        "    tick_size=0.01,\n",
        "    total_qty=3000,\n",
        "    max_steps=5000,\n",
        "    target_participation=4.0,\n",
        "    queue_loss_penalty_bps=3.0\n",
        ")\n",
        "\n",
        "print(\"env_real ready | venues:\", [sv.venue.name for sv in env_real.vl])\n",
        "print(\"Params:\", {k: {kk: vv for kk, vv in v.items()} for k, v in VENUE_PARAMS.items()})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "566a7636",
      "metadata": {
        "id": "566a7636",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "302f135e-6334-4897-da64-084f344e5020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Books seeded. FAST bids/asks sizes: 1 1\n"
          ]
        }
      ],
      "source": [
        "#Seed order books\n",
        "\n",
        "def _first_trade_price(feed):\n",
        "    feed.reset()\n",
        "    first_px = None\n",
        "    for _ in range(10):\n",
        "        evs = feed.next_events()\n",
        "        if not evs:\n",
        "            break\n",
        "        for ev in evs:\n",
        "            if ev.get(\"type\") == \"trade\":\n",
        "                first_px = float(ev[\"price\"])\n",
        "                break\n",
        "        if first_px is not None:\n",
        "            break\n",
        "    feed.reset()\n",
        "    return first_px\n",
        "\n",
        "def seed_books_from_feed(env, feed, qty=12):\n",
        "    to_ticks = lambda p: int(round(float(p) / float(env.tick_size)))\n",
        "    px = _first_trade_price(feed) or 100.0\n",
        "    mid_ticks = max(2, to_ticks(px))\n",
        "    for sv in env.vl:\n",
        "        if not sv.venue.book.bids and not sv.venue.book.asks:\n",
        "            sv.venue.book.submit(\"BUY\",  mid_ticks - 1, qty)\n",
        "            sv.venue.book.submit(\"SELL\", mid_ticks + 1, qty)\n",
        "\n",
        "seed_books_from_feed(env_real, feed)\n",
        "print(\"Books seeded. FAST bids/asks sizes:\",\n",
        "      len(env_real.vl[0].venue.book.bids), len(env_real.vl[0].venue.book.asks))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3865145",
      "metadata": {
        "id": "b3865145"
      },
      "outputs": [],
      "source": [
        "#Tactics toggle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def _snapshot_metrics(env):\n",
        "    rows = []\n",
        "    for sv in env.vl:\n",
        "        bb = sv.venue.book.bids[0].price if sv.venue.book.bids else None\n",
        "        ba = sv.venue.book.asks[0].price if sv.venue.book.asks else None\n",
        "        if bb is not None and ba is not None:\n",
        "            m = 0.5 * (bb + ba)\n",
        "            spread = ba - bb\n",
        "        else:\n",
        "            m = bb if bb is not None else ba\n",
        "            spread = None\n",
        "        depth_bid = sum(o.qty for o in list(sv.venue.book.bids)[:3]) if sv.venue.book.bids else 0\n",
        "        depth_ask = sum(o.qty for o in list(sv.venue.book.asks)[:3]) if sv.venue.book.asks else 0\n",
        "        rows.append({\n",
        "            \"t\": env.t,\n",
        "            \"venue\": sv.venue.name,\n",
        "            \"mid\": m,\n",
        "            \"spread\": spread,\n",
        "            \"depth_bid_top3\": depth_bid,\n",
        "            \"depth_ask_top3\": depth_ask,\n",
        "            \"cum_filled\": env.cum_filled,\n",
        "            \"remaining\": env.remaining,\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "def pick_policy(name: str):\n",
        "    \"\"\"Map a friendly name to an action vector [taker_share, passive_offset].\"\"\"\n",
        "    n = (name or \"\").lower()\n",
        "    if n in (\"twap\", \"twap_like\"):\n",
        "        return lambda s: np.array([0.20, 1], dtype=float)\n",
        "    if n in (\"pov\", \"pov_like\"):\n",
        "        return lambda s: np.array([0.40, 1], dtype=float)\n",
        "    if n in (\"aggr\", \"aggressive\"):\n",
        "        return lambda s: np.array([0.80, 0], dtype=float)\n",
        "    return lambda s: np.array([0.40, 1], dtype=float)\n",
        "\n",
        "def run_policy(env, policy_name=\"pov\"):\n",
        "    \"\"\"Reset, reseed books, run until done, collect a metrics DataFrame.\"\"\"\n",
        "    pol = pick_policy(policy_name)\n",
        "    state = env.reset()\n",
        "    seed_books_from_feed(env, feed)\n",
        "\n",
        "    done = False\n",
        "    metrics = []\n",
        "    while not done:\n",
        "        action = pol(state)\n",
        "        result = env.step(action)\n",
        "        metrics += _snapshot_metrics(env)\n",
        "        state, done = result.state, result.done\n",
        "\n",
        "    df = pd.DataFrame(metrics)\n",
        "    cum_is_bps = (((env.cum_spent + env.cum_fee)/max(1, env.cum_filled) - env.arrival0) / env.arrival0) * 1e4\n",
        "    print(f\"[{policy_name}] Filled {env.cum_filled}/{env.total_qty} | IS vs start (bps): {cum_is_bps:.2f}\")\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40be49eb",
      "metadata": {
        "id": "40be49eb"
      },
      "outputs": [],
      "source": [
        "#Quick plots for a run\n",
        "\n",
        "\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "def quick_plots(df: pd.DataFrame, title: str = \"Run\"):\n",
        "    df_ms = df.dropna(subset=[\"mid\"])\n",
        "    if not df_ms.empty:\n",
        "        fig1 = px.line(df_ms, x=\"t\", y=\"mid\", color=\"venue\",\n",
        "                       title=f\"{title} — Mid by venue\")\n",
        "        fig1.show()\n",
        "\n",
        "        fig2 = px.line(df_ms, x=\"t\", y=\"spread\", color=\"venue\",\n",
        "                       title=f\"{title} — Spread by venue\")\n",
        "        fig2.show()\n",
        "\n",
        "    prog = df.groupby(\"t\", as_index=False)[[\"cum_filled\", \"remaining\"]].max()\n",
        "    fig3 = px.line(prog, x=\"t\", y=[\"cum_filled\", \"remaining\"],\n",
        "                   title=f\"{title} — Fill progress\")\n",
        "    fig3.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b20edf1",
      "metadata": {
        "id": "4b20edf1"
      },
      "outputs": [],
      "source": [
        "#Adaptive Router\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def _best_effective_prices_for_buy(env):\n",
        "    \"\"\"\n",
        "    For each venue, estimate effective buy prices:\n",
        "      maker: post at (best_bid + 1*tick) and apply maker_bps\n",
        "      taker: cross at (best_ask + 3*tick) and apply taker_bps\n",
        "    Returns (best_maker_price, best_maker_venue, best_taker_price, best_taker_venue)\n",
        "    \"\"\"\n",
        "    best_maker = (np.inf, None)\n",
        "    best_taker = (np.inf, None)\n",
        "    for sv in env.vl:\n",
        "        bb, ba, _ = sv.venue.snapshot()\n",
        "        tick = sv.venue.tick\n",
        "\n",
        "        if bb is not None:\n",
        "            place_px_maker = max(1, bb + 1 * tick)\n",
        "            eff_maker = place_px_maker * (1 + sv.venue.maker_bps / 1e4)\n",
        "            if eff_maker < best_maker[0]:\n",
        "                best_maker = (eff_maker, sv.venue.name)\n",
        "\n",
        "        if ba is not None:\n",
        "            place_px_taker = ba + 3 * tick\n",
        "            eff_taker = place_px_taker * (1 + sv.venue.taker_bps / 1e4)\n",
        "            if eff_taker < best_taker[0]:\n",
        "                best_taker = (eff_taker, sv.venue.name)\n",
        "\n",
        "    return best_maker[0], best_maker[1], best_taker[0], best_taker[1]\n",
        "\n",
        "def make_adaptive_policy(env):\n",
        "    \"\"\"\n",
        "    Returns a policy(state)->action function.\n",
        "    Heuristics:\n",
        "      - If we're behind schedule, increase taker_share.\n",
        "      - If taker effective price beats maker, nudge more to taker.\n",
        "      - Use passive_offset=1 when spreads are decent; 0 when very tight.\n",
        "    \"\"\"\n",
        "    def policy(_state):\n",
        "        expected = env.total_qty * (env.t + 1) / max(1, env.max_steps)\n",
        "        lag = expected - (env.total_qty - env.remaining)\n",
        "        lag_ratio = float(np.clip(lag / max(1, 0.15 * env.total_qty), 0.0, 1.0))\n",
        "\n",
        "        best_maker_px, best_maker_ven, best_taker_px, best_taker_ven = _best_effective_prices_for_buy(env)\n",
        "        taker_adv = 0.0\n",
        "        if np.isfinite(best_maker_px) and np.isfinite(best_taker_px) and best_taker_px < best_maker_px:\n",
        "            taker_adv = 0.25\n",
        "\n",
        "        spreads = []\n",
        "        for sv in env.vl:\n",
        "            bb, ba, _ = sv.venue.snapshot()\n",
        "            if bb is not None and ba is not None:\n",
        "                spreads.append(ba - bb)\n",
        "        avg_spread = np.mean(spreads) if spreads else None\n",
        "\n",
        "        base = 0.25\n",
        "        taker_share = np.clip(base + 0.6 * lag_ratio + taker_adv, 0.0, 1.0)\n",
        "\n",
        "        passive_offset = 1\n",
        "        if avg_spread is not None and avg_spread <= 1:\n",
        "            passive_offset = 0\n",
        "\n",
        "        if env.t % 50 == 0:\n",
        "            print(f\"[t={env.t}] best_maker@{best_maker_ven}, best_taker@{best_taker_ven}, \"\n",
        "                  f\"lag_ratio={lag_ratio:.2f} -> action=[taker={taker_share:.2f}, offset={passive_offset}]\")\n",
        "\n",
        "        return np.array([taker_share, passive_offset], dtype=float)\n",
        "\n",
        "    return policy\n",
        "\n",
        "def run_adaptive(env, title=\"adaptive\"):\n",
        "    \"\"\"Runs the environment using the adaptive router policy and returns a metrics DataFrame.\"\"\"\n",
        "    state = env.reset()\n",
        "    seed_books_from_feed(env, feed)\n",
        "    pol = make_adaptive_policy(env)\n",
        "\n",
        "    metrics = []\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = pol(state)\n",
        "        result = env.step(action)\n",
        "        metrics += _snapshot_metrics(env)\n",
        "        state, done = result.state, result.done\n",
        "\n",
        "    df = pd.DataFrame(metrics)\n",
        "    cum_is_bps = (((env.cum_spent + env.cum_fee)/max(1, env.cum_filled) - env.arrival0) / env.arrival0) * 1e4\n",
        "    print(f\"[{title}] Filled {env.cum_filled}/{env.total_qty} | IS vs start (bps): {cum_is_bps:.2f}\")\n",
        "    return df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9979c542",
      "metadata": {
        "id": "9979c542"
      },
      "outputs": [],
      "source": [
        "#Latency queue + true per-venue routing\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def _route_and_queue(self, taker_qty: int, passive_qty: int, passive_offset: int):\n",
        "    \"\"\"\n",
        "    Split maker/taker flow and enqueue child orders to the best venue by\n",
        "    fee-adjusted effective price. Orders arrive after venue latency.\n",
        "    \"\"\"\n",
        "    if not hasattr(self, \"_pending\"):\n",
        "        self._pending = []\n",
        "\n",
        "    if passive_qty > 0:\n",
        "        best_eff, best_sv, best_px = np.inf, None, None\n",
        "        for sv in self.vl:\n",
        "            bb = sv.venue.book.bids[0].price if sv.venue.book.bids else None\n",
        "            if bb is None: continue\n",
        "            px = max(1, bb + int(passive_offset) * sv.venue.tick)\n",
        "            eff = px * (1 + sv.venue.maker_bps / 1e4)\n",
        "            if eff < best_eff:\n",
        "                best_eff, best_sv, best_px = eff, sv, px\n",
        "        if best_sv is not None:\n",
        "            lat = best_sv.latency.base_steps + (np.random.randint(0, best_sv.latency.jitter_steps + 1)\n",
        "                                                if best_sv.latency.jitter_steps > 0 else 0)\n",
        "            arrive_t = self.t + lat\n",
        "            self._pending.append((\"BUY\", int(best_px), int(passive_qty), best_sv, \"maker\", arrive_t))\n",
        "\n",
        "    if taker_qty > 0:\n",
        "        best_eff, best_sv, best_px = np.inf, None, None\n",
        "        for sv in self.vl:\n",
        "            ba = sv.venue.book.asks[0].price if sv.venue.book.asks else None\n",
        "            if ba is None: continue\n",
        "            px = ba + 3 * sv.venue.tick\n",
        "            eff = px * (1 + sv.venue.taker_bps / 1e4)\n",
        "            if eff < best_eff:\n",
        "                best_eff, best_sv, best_px = eff, sv, px\n",
        "        if best_sv is not None:\n",
        "            lat = best_sv.latency.base_steps + (np.random.randint(0, best_sv.latency.jitter_steps + 1)\n",
        "                                                if best_sv.latency.jitter_steps > 0 else 0)\n",
        "            arrive_t = self.t + lat\n",
        "            self._pending.append((\"BUY\", int(best_px), int(taker_qty), best_sv, \"taker\", arrive_t))\n",
        "\n",
        "    return {\"maker\": int(passive_qty), \"taker\": int(taker_qty)}\n",
        "\n",
        "def _process_arrivals(self):\n",
        "    \"\"\"\n",
        "    Deliver enqueued child orders that have 'arrived' this step.\n",
        "    Submitting to the books here ensures fills are counted this step\n",
        "    (we took the before_fills snapshot just above in .step()).\n",
        "    \"\"\"\n",
        "    if not hasattr(self, \"_pending\"):\n",
        "        self._pending = []\n",
        "    if not self._pending:\n",
        "        return\n",
        "    carry = []\n",
        "    for side, px, qty, sv, role, arrive_t in self._pending:\n",
        "        if arrive_t <= self.t:\n",
        "            sv.venue.book.submit(side, px, qty)\n",
        "        else:\n",
        "            carry.append((side, px, qty, sv, role, arrive_t))\n",
        "    self._pending = carry\n",
        "\n",
        "ExecEnvLatencyV2._route_and_queue = _route_and_queue\n",
        "ExecEnvLatencyV2._process_arrivals = _process_arrivals\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf58d542",
      "metadata": {
        "id": "cf58d542"
      },
      "outputs": [],
      "source": [
        "#Temporary Liquidity Keeper\n",
        "\n",
        "USE_KEEPER = False\n",
        "\n",
        "def keep_books_healthy(env, target_spread_ticks=2, min_top_qty=30, level_size=10):\n",
        "    \"\"\"\n",
        "    For each venue:\n",
        "      - ensure both sides exist (seed 1-tick inside ref mid if empty)\n",
        "      - if spread > target, add inside quotes to compress it\n",
        "      - ensure top-3 depth has at least min_top_qty shares\n",
        "    \"\"\"\n",
        "    for sv in env.vl:\n",
        "        book = sv.venue.book\n",
        "        tick = sv.venue.tick\n",
        "\n",
        "        bb = book.bids[0].price if book.bids else None\n",
        "        ba = book.asks[0].price if book.asks else None\n",
        "\n",
        "        if bb is not None and ba is not None:\n",
        "            ref_mid = (bb + ba) // 2\n",
        "        else:\n",
        "            ref_mid = bb or ba\n",
        "            if ref_mid is None:\n",
        "                continue\n",
        "\n",
        "        if bb is None:\n",
        "            book.submit(\"BUY\",  max(1, int(ref_mid - 1 * tick)), level_size)\n",
        "        if ba is None:\n",
        "            book.submit(\"SELL\", int(ref_mid + 1 * tick),       level_size)\n",
        "\n",
        "        bb = book.bids[0].price if book.bids else None\n",
        "        ba = book.asks[0].price if book.asks else None\n",
        "\n",
        "        if bb is not None and ba is not None and (ba - bb) > target_spread_ticks * tick:\n",
        "            book.submit(\"BUY\",  ba - 1 * tick, level_size)\n",
        "            book.submit(\"SELL\", bb + 1 * tick, level_size)\n",
        "\n",
        "        topQb = sum(o.qty for o in list(book.bids)[:3]) if book.bids else 0\n",
        "        topQa = sum(o.qty for o in list(book.asks)[:3]) if book.asks else 0\n",
        "        if topQb < min_top_qty:\n",
        "            book.submit(\"BUY\",  book.bids[0].price if book.bids else max(1, int(ref_mid - 1 * tick)),\n",
        "                        min_top_qty - topQb)\n",
        "        if topQa < min_top_qty:\n",
        "            book.submit(\"SELL\", book.asks[0].price if book.asks else int(ref_mid + 1 * tick),\n",
        "                        min_top_qty - topQa)\n",
        "\n",
        "def _feed_stalled(env):\n",
        "    for sv in env.vl:\n",
        "        if not sv.venue.book.bids or not sv.venue.book.asks:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def run_policy_kept(env, policy_name=\"pov\"):\n",
        "    df = run_policy(env, policy_name)\n",
        "    if USE_KEEPER or _feed_stalled(env):\n",
        "        keep_books_healthy(env)\n",
        "    return df\n",
        "\n",
        "def run_adaptive_kept(env, title=\"Adaptive v0 + keeper\"):\n",
        "    state = env.reset()\n",
        "    seed_books_from_feed(env, feed)\n",
        "    pol = make_adaptive_policy(env)\n",
        "\n",
        "    import pandas as pd\n",
        "    metrics, done = [], False\n",
        "    while not done:\n",
        "        action = pol(state)\n",
        "        result = env.step(action)\n",
        "        metrics += _snapshot_metrics(env)\n",
        "        if USE_KEEPER or _feed_stalled(env):\n",
        "            keep_books_healthy(env)\n",
        "        state, done = result.state, result.done\n",
        "\n",
        "    df = pd.DataFrame(metrics)\n",
        "    cum_is_bps = (((env.cum_spent + env.cum_fee)/max(1, env.cum_filled) - env.arrival0) / env.arrival0) * 1e4\n",
        "    print(f\"[{title}] Filled {env.cum_filled}/{env.total_qty} | IS vs start (bps): {cum_is_bps:.2f}\")\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "255feafd",
      "metadata": {
        "id": "255feafd"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "def _route_and_queue_logged(self, taker_qty: int, passive_qty: int, passive_offset: int):\n",
        "    \"\"\"\n",
        "    Choose best venues, enqueue child orders with latency, and LOG the decision.\n",
        "    NOTE: This is a helper; the active monkey-patch should be _route_and_queue_capped,\n",
        "    which internally delegates to this function.\n",
        "    \"\"\"\n",
        "    if not hasattr(self, \"_pending\"):    self._pending = []\n",
        "    if not hasattr(self, \"_route_log\"):  self._route_log = []\n",
        "\n",
        "    if passive_qty > 0:\n",
        "        best_eff, best_sv, best_px = np.inf, None, None\n",
        "        for sv in self.vl:\n",
        "            bb = sv.venue.book.bids[0].price if sv.venue.book.bids else None\n",
        "            if bb is None: continue\n",
        "            px  = max(1, bb + int(passive_offset) * sv.venue.tick)\n",
        "            eff = px * (1 + sv.venue.maker_bps / 1e4)\n",
        "            if eff < best_eff:\n",
        "                best_eff, best_sv, best_px = eff, sv, px\n",
        "        if best_sv is not None:\n",
        "            lat = best_sv.latency.base_steps + (\n",
        "                np.random.randint(0, best_sv.latency.jitter_steps + 1)\n",
        "                if best_sv.latency.jitter_steps > 0 else 0\n",
        "            )\n",
        "            arrive_t = self.t + lat\n",
        "            self._pending.append((\"BUY\", int(best_px), int(passive_qty), best_sv, \"maker\", arrive_t))\n",
        "            self._route_log.append({\n",
        "                \"t\": self.t, \"role\": \"maker\", \"venue\": best_sv.venue.name,\n",
        "                \"qty\": int(passive_qty), \"px\": int(best_px), \"arrive_t\": arrive_t\n",
        "            })\n",
        "\n",
        "    if taker_qty > 0:\n",
        "        best_eff, best_sv, best_px = np.inf, None, None\n",
        "        for sv in self.vl:\n",
        "            ba = sv.venue.book.asks[0].price if sv.venue.book.asks else None\n",
        "            if ba is None: continue\n",
        "            px  = ba + 3 * sv.venue.tick\n",
        "            eff = px * (1 + sv.venue.taker_bps / 1e4)\n",
        "            if eff < best_eff:\n",
        "                best_eff, best_sv, best_px = eff, sv, px\n",
        "        if best_sv is not None:\n",
        "            lat = best_sv.latency.base_steps + (\n",
        "                np.random.randint(0, best_sv.latency.jitter_steps + 1)\n",
        "                if best_sv.latency.jitter_steps > 0 else 0\n",
        "            )\n",
        "            arrive_t = self.t + lat\n",
        "            self._pending.append((\"BUY\", int(best_px), int(taker_qty), best_sv, \"taker\", arrive_t))\n",
        "            self._route_log.append({\n",
        "                \"t\": self.t, \"role\": \"taker\", \"venue\": best_sv.venue.name,\n",
        "                \"qty\": int(taker_qty), \"px\": int(best_px), \"arrive_t\": arrive_t\n",
        "            })\n",
        "\n",
        "    return {\"maker\": int(passive_qty), \"taker\": int(taker_qty)}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ad58256",
      "metadata": {
        "id": "1ad58256"
      },
      "outputs": [],
      "source": [
        "#Patch: cap counted fills inside\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def _step_capped(self, action):\n",
        "    taker_share    = float(np.clip(action[0], 0.0, 1.0))\n",
        "    passive_offset = int(np.clip(round(action[1]), 0, 2))\n",
        "\n",
        "    step_ref_mid = self._arrival_mid_global()\n",
        "\n",
        "    _ = self._apply_feed_step()\n",
        "\n",
        "    bg_before = [len(sv.venue.book.trades) for sv in self.vl]\n",
        "\n",
        "    before_fills = {sv.venue.name: len(sv.venue.book.trades) for sv in self.vl}\n",
        "    self._process_arrivals()\n",
        "\n",
        "    allowed      = max(0, self.total_qty - self.cum_filled)\n",
        "    step_spent   = 0.0\n",
        "    step_fee     = 0.0\n",
        "    step_filled  = 0\n",
        "\n",
        "    for sv in self.vl:\n",
        "        if step_filled >= allowed:\n",
        "            break\n",
        "        new_trades = sv.venue.book.trades[before_fills[sv.venue.name]:]\n",
        "        for _, price, q in new_trades:\n",
        "            if step_filled >= allowed:\n",
        "                break\n",
        "            bb, ba, m = sv.venue.snapshot()\n",
        "            role_fee_bps = sv.venue.taker_bps if (m is not None and price > m) else sv.venue.maker_bps\n",
        "            q_use = int(min(q, allowed - step_filled))\n",
        "            if q_use <= 0:\n",
        "                continue\n",
        "            step_spent  += price * q_use\n",
        "            step_fee    += (role_fee_bps / 1e4) * price * q_use\n",
        "            step_filled += q_use\n",
        "\n",
        "    bg_vol = 0\n",
        "    for i, sv in enumerate(self.vl):\n",
        "        new = sv.venue.book.trades[bg_before[i]:]\n",
        "        bg_vol += int(sum(q for _, _, q in new))\n",
        "    self.recent_bg_vol.pop(0); self.recent_bg_vol.append(bg_vol)\n",
        "    avg_bg = max(1, int(np.mean(self.recent_bg_vol)))\n",
        "\n",
        "    if self.remaining > 0 and self.t < self.max_steps:\n",
        "        pov_target  = max(1, int(np.ceil(self.target_participation * avg_bg)))\n",
        "        taker_qty   = min(int(np.floor(taker_share * pov_target)), self.remaining)\n",
        "        passive_qty = max(0, min(pov_target - taker_qty, self.remaining - taker_qty))\n",
        "        intent = self._route_and_queue(taker_qty, passive_qty, passive_offset)\n",
        "    else:\n",
        "        intent = {\"maker\": 0, \"taker\": 0}\n",
        "\n",
        "    queue_loss_qty = 0\n",
        "    if intent[\"maker\"] > 0 and step_filled == 0 and step_ref_mid is not None:\n",
        "        now_mid = self._arrival_mid_global()\n",
        "        if now_mid is not None and now_mid > step_ref_mid:\n",
        "            queue_loss_qty = intent[\"maker\"]\n",
        "\n",
        "    self.cum_spent   += step_spent\n",
        "    self.cum_fee     += step_fee\n",
        "    self.cum_filled  += step_filled\n",
        "    self.remaining    = max(0, self.total_qty - self.cum_filled)\n",
        "    self.last_step_fills          = step_filled\n",
        "    self.last_step_queue_loss_qty = queue_loss_qty\n",
        "\n",
        "    reward_is = 0.0\n",
        "    if step_filled > 0 and step_ref_mid:\n",
        "        vwap_step_net = (step_spent + step_fee) / step_filled\n",
        "        reward_is = -(((vwap_step_net - step_ref_mid) / step_ref_mid) * 1e4)\n",
        "\n",
        "    remaining_frac = self.remaining / max(1, self.total_qty)\n",
        "    reward_queue   = - self.queue_loss_penalty_bps * (queue_loss_qty / max(1, intent[\"maker\"] if intent[\"maker\"]>0 else 1))\n",
        "    reward_time    = - self.time_penalty_bps_per_step * remaining_frac\n",
        "    reward_fillb   =   self.fill_bonus_bps_per_share * step_filled\n",
        "    reward_step    = float(np.clip(reward_is + reward_queue + reward_time + reward_fillb,\n",
        "                                   -self.reward_clip_bps, self.reward_clip_bps))\n",
        "\n",
        "    self.t += 1\n",
        "    done = (self.remaining <= 0) or (self.t >= self.max_steps)\n",
        "\n",
        "    info = {\n",
        "        \"filled\": step_filled,\n",
        "        \"queue_loss_qty\": queue_loss_qty,\n",
        "        \"remaining\": self.remaining,\n",
        "        \"vwap_net_cum\": (self.cum_spent + self.cum_fee) / self.cum_filled if self.cum_filled else np.nan,\n",
        "        \"is_bps_cum_vs_start\": (((self.cum_spent + self.cum_fee)/max(1, self.cum_filled) - self.arrival0) / self.arrival0) * 1e4\n",
        "                               if self.cum_filled and self.arrival0 else np.nan,\n",
        "    }\n",
        "    return StepResult(self._state(), reward_step, done, info)\n",
        "\n",
        "ExecEnvLatencyV2Real.step = _step_capped\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "212a04de",
      "metadata": {
        "id": "212a04de"
      },
      "outputs": [],
      "source": [
        "#Patch: cap queued qty\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def _queued_qty(self):\n",
        "    if not hasattr(self, \"_pending\"):\n",
        "        return 0\n",
        "    return int(sum(q for _, _, q, _, _, _ in self._pending))\n",
        "\n",
        "def _route_and_queue_capped(self, taker_qty: int, passive_qty: int, passive_offset: int):\n",
        "    cap = max(0, self.total_qty - (self.cum_filled + self._queued_qty()))\n",
        "    if cap <= 0:\n",
        "        return {\"maker\": 0, \"taker\": 0}\n",
        "\n",
        "    ask_total = int(taker_qty) + int(passive_qty)\n",
        "    if ask_total > cap and ask_total > 0:\n",
        "        scale = cap / ask_total\n",
        "        taker_qty   = int(np.floor(taker_qty   * scale))\n",
        "        passive_qty = int(np.floor(passive_qty * scale))\n",
        "        if taker_qty + passive_qty == 0:\n",
        "            taker_qty = min(1, cap)\n",
        "\n",
        "    return _route_and_queue_logged(self, taker_qty, passive_qty, passive_offset)\n",
        "\n",
        "def _process_arrivals_clip(self):\n",
        "    \"\"\"\n",
        "    Deliver arrivals that have reached their latency, but:\n",
        "      - Clip qty so cumulative fills never exceed total_qty\n",
        "      - Log arrivals for this step so ExecEnvLatencyV2Real.step() can\n",
        "        charge exact maker/taker fees by matching fills to our own orders.\n",
        "\n",
        "    Populates: self._arrived_this_step = {venue: [{\"px\":int,\"qty\":int,\"role\":\"maker\"|\"taker\"}, ...]}\n",
        "    \"\"\"\n",
        "    if not hasattr(self, \"_pending\"):\n",
        "        self._pending = []\n",
        "    carry = []\n",
        "    remaining_cap = max(0, self.total_qty - self.cum_filled)\n",
        "\n",
        "    self._arrived_this_step = {}\n",
        "\n",
        "    for side, px, qty, sv, role, arrive_t in self._pending:\n",
        "        if arrive_t <= self.t and remaining_cap > 0:\n",
        "            q = min(int(qty), remaining_cap)\n",
        "            if q > 0:\n",
        "                sv.venue.book.submit(side, int(px), q)\n",
        "                remaining_cap -= q\n",
        "                name = sv.venue.name\n",
        "                self._arrived_this_step.setdefault(name, []).append(\n",
        "                    {\"px\": int(px), \"qty\": int(q), \"role\": role}\n",
        "                )\n",
        "        else:\n",
        "            carry.append((side, px, qty, sv, role, arrive_t))\n",
        "    self._pending = carry\n",
        "\n",
        "ExecEnvLatencyV2._queued_qty       = _queued_qty\n",
        "ExecEnvLatencyV2._route_and_queue  = _route_and_queue_capped\n",
        "ExecEnvLatencyV2._process_arrivals = _process_arrivals_clip\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a305bdc8",
      "metadata": {
        "id": "a305bdc8"
      },
      "outputs": [],
      "source": [
        "#Patch: recompute arrival\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def run_adaptive_kept_logged(env, title=\"Adaptive v0 + keeper + log\"):\n",
        "    \"\"\"Run adaptive policy with liquidity keeper + routing log and safe arrival0.\"\"\"\n",
        "    state = env.reset()\n",
        "    seed_books_from_feed(env, feed)\n",
        "    env.arrival0 = env._arrival_mid_global()\n",
        "\n",
        "    pol = make_adaptive_policy(env)\n",
        "\n",
        "    metrics, fills = [], []\n",
        "    last_len = {sv.venue.name: len(sv.venue.book.trades) for sv in env.vl}\n",
        "    env._route_log = []\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = pol(state)\n",
        "        result = env.step(action)\n",
        "        rows = _snapshot_metrics(env)\n",
        "        metrics += rows\n",
        "\n",
        "        row = {\"t\": env.t}\n",
        "        for sv in env.vl:\n",
        "            name = sv.venue.name\n",
        "            new_trades = sv.venue.book.trades[last_len[name]:]\n",
        "            row[f\"fill_{name}\"] = int(sum(q for _, _, q in new_trades))\n",
        "            last_len[name] = len(sv.venue.book.trades)\n",
        "        fills.append(row)\n",
        "\n",
        "        if USE_KEEPER or _feed_stalled(env):\n",
        "          keep_books_healthy(env)\n",
        "        state, done = result.state, result.done\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics)\n",
        "    df_fills   = pd.DataFrame(fills).fillna(0)\n",
        "    route_log  = pd.DataFrame(getattr(env, \"_route_log\", []))\n",
        "\n",
        "    if getattr(env, \"cum_filled\", 0) and env.arrival0 is not None:\n",
        "        cum_is_bps = (((env.cum_spent + env.cum_fee)/max(1, env.cum_filled) - env.arrival0) / env.arrival0) * 1e4\n",
        "    else:\n",
        "        cum_is_bps = np.nan\n",
        "\n",
        "    print(f\"[{title}] Filled {env.cum_filled}/{env.total_qty} | IS vs start (bps): {cum_is_bps:.2f}\")\n",
        "    if not route_log.empty:\n",
        "        alloc = (route_log.groupby([\"role\",\"venue\"])[\"qty\"].sum().unstack(fill_value=0))\n",
        "        print(\"\\nIntended allocation (shares) by role × venue:\\n\", alloc)\n",
        "\n",
        "    fill_cols = [c for c in df_fills.columns if c.startswith(\"fill_\")]\n",
        "    if fill_cols:\n",
        "        realized = df_fills[fill_cols].sum().rename(lambda c: c.replace(\"fill_\",\"\"))\n",
        "        print(\"\\nRealized fills by venue (shares):\\n\", realized)\n",
        "\n",
        "    return df_metrics, df_fills, route_log\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b95d8d65",
      "metadata": {
        "id": "b95d8d65"
      },
      "outputs": [],
      "source": [
        "#Impact Head\n",
        "\n",
        "import numpy as np, pandas as pd, torch\n",
        "from torch import nn\n",
        "\n",
        "def _collapse_metrics(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Collapse venue rows; if 'symbol' exists, keep symbols separate so different coins\n",
        "    with the same step index 't' don't get merged.\n",
        "    \"\"\"\n",
        "    keys = [\"t\"]\n",
        "    if \"symbol\" in df.columns:\n",
        "        keys = [\"symbol\", \"t\"]\n",
        "\n",
        "    g = (\n",
        "        df.groupby(keys, as_index=False)\n",
        "          .agg(mid=(\"mid\",\"mean\"),\n",
        "               spread=(\"spread\",\"mean\"),\n",
        "               depth_bid_top3=(\"depth_bid_top3\",\"sum\"),\n",
        "               depth_ask_top3=(\"depth_ask_top3\",\"sum\"))\n",
        "          .sort_values(keys)\n",
        "          .reset_index(drop=True)\n",
        "    )\n",
        "    return g\n",
        "\n",
        "\n",
        "def _build_feats_bps(g: pd.DataFrame, horizon: int = 5):\n",
        "    \"\"\"\n",
        "    Build BPS-scale features and a directional label at +horizon steps.\n",
        "    Returns: Xz, y, cols, mu, sigma, g2  (where g2 is the cleaned/filtered frame)\n",
        "\n",
        "    Notes:\n",
        "      - Includes winsorization to keep outliers (caused by thin books / tiny mids) from exploding.\n",
        "      - spr_bps is clipped to [0, 50] bps which is generous for crypto majors.\n",
        "    \"\"\"\n",
        "    g = g.copy()\n",
        "\n",
        "    g = g.dropna(subset=[\"mid\"]).reset_index(drop=True)\n",
        "\n",
        "    g[\"ret1_bps\"]  = g[\"mid\"].pct_change(1)  * 1e4\n",
        "    g[\"ret3_bps\"]  = g[\"mid\"].pct_change(3)  * 1e4\n",
        "    g[\"ret5_bps\"]  = g[\"mid\"].pct_change(5)  * 1e4\n",
        "    g[\"ret10_bps\"] = g[\"mid\"].pct_change(10) * 1e4\n",
        "    g[\"ret20_bps\"] = g[\"mid\"].pct_change(20) * 1e4\n",
        "\n",
        "    g[\"rv10_bps\"] = g[\"ret1_bps\"].rolling(10).std() * np.sqrt(10)\n",
        "\n",
        "    g[\"imb\"]      = (g[\"depth_bid_top3\"] - g[\"depth_ask_top3\"]) / (\n",
        "                     g[\"depth_bid_top3\"] + g[\"depth_ask_top3\"] + 1e-9)\n",
        "    g[\"spr_bps\"]  = (g[\"spread\"] / (g[\"mid\"] + 1e-9)) * 1e4\n",
        "\n",
        "    ema20 = g[\"mid\"].ewm(span=20, adjust=False).mean()\n",
        "    ema50 = g[\"mid\"].ewm(span=50, adjust=False).mean()\n",
        "    g[\"ema20_diff\"] = (g[\"mid\"] - ema20) / (ema20 + 1e-9) * 1e4\n",
        "    g[\"ema50_diff\"] = (g[\"mid\"] - ema50) / (ema50 + 1e-9) * 1e4\n",
        "\n",
        "    try:\n",
        "        idx_ts = pd.to_datetime(g[\"ts\"], unit=\"ms\", errors=\"coerce\")\n",
        "    except Exception:\n",
        "        idx_ts = None\n",
        "    if idx_ts is not None and not idx_ts.isnull().all():\n",
        "        hod = idx_ts.dt.hour.astype(float)\n",
        "        dow = idx_ts.dt.dayofweek.astype(float)\n",
        "    else:\n",
        "        rng = pd.RangeIndex(len(g))\n",
        "        hod = (rng % 24).astype(float)\n",
        "        dow = (rng % 7).astype(float)\n",
        "    g[\"hod_sin\"] = np.sin(2 * np.pi * hod / 24.0)\n",
        "    g[\"hod_cos\"] = np.cos(2 * np.pi * hod / 24.0)\n",
        "    g[\"dow_sin\"] = np.sin(2 * np.pi * dow / 7.0)\n",
        "    g[\"dow_cos\"] = np.cos(2 * np.pi * dow / 7.0)\n",
        "\n",
        "    g[\"ema20_slope\"] = (ema20.pct_change(1) * 1e4).clip(-500, 500)\n",
        "    g[\"ema50_slope\"] = (ema50.pct_change(1) * 1e4).clip(-500, 500)\n",
        "    g[\"ema20_curve\"] = (ema20.pct_change(2) * 1e4).clip(-800, 800)\n",
        "\n",
        "    g[\"ret1_bps\"]  = g[\"ret1_bps\"].clip(-200, 200)\n",
        "    g[\"ret3_bps\"]  = g[\"ret3_bps\"].clip(-400, 400)\n",
        "    g[\"ret5_bps\"]  = g[\"ret5_bps\"].clip(-600, 600)\n",
        "    g[\"ret10_bps\"] = g[\"ret10_bps\"].clip(-1000, 1000)\n",
        "    g[\"ret20_bps\"] = g[\"ret20_bps\"].clip(-1500, 1500)\n",
        "    g[\"rv10_bps\"]  = g[\"rv10_bps\"].clip(0, 500)\n",
        "    g[\"spr_bps\"]   = g[\"spr_bps\"].clip(0, 50)\n",
        "    g[\"ema20_diff\"] = g[\"ema20_diff\"].clip(-500, 500)\n",
        "    g[\"ema50_diff\"] = g[\"ema50_diff\"].clip(-500, 500)\n",
        "\n",
        "    cols = [\n",
        "        \"ret1_bps\", \"ret3_bps\", \"ret5_bps\",\n",
        "        \"ret10_bps\", \"ret20_bps\",\n",
        "        \"rv10_bps\",\n",
        "        \"imb\", \"spr_bps\",\n",
        "        \"ema20_diff\", \"ema50_diff\",\n",
        "        \"hod_sin\", \"hod_cos\", \"dow_sin\", \"dow_cos\",\n",
        "        \"ema20_slope\", \"ema50_slope\", \"ema20_curve\",\n",
        "    ]\n",
        "\n",
        "    g[\"y\"] = np.sign(g[\"mid\"].shift(-horizon) - g[\"mid\"])\n",
        "\n",
        "    g = g.replace([np.inf, -np.inf], np.nan)\n",
        "    g = g.dropna(subset=cols + [\"y\"]).reset_index(drop=True)\n",
        "\n",
        "    X = g[cols].astype(float).values\n",
        "    y = (g[\"y\"] > 0).astype(int).values\n",
        "\n",
        "    mask = np.isfinite(X).all(axis=1)\n",
        "    if mask.sum() != len(mask):\n",
        "        X = X[mask]; y = y[mask]; g = g.loc[mask].reset_index(drop=True)\n",
        "\n",
        "    mu = X.mean(axis=0)\n",
        "    sigma = X.std(axis=0) + 1e-9\n",
        "    Xz = (X - mu) / sigma\n",
        "\n",
        "    return Xz, y, cols, mu, sigma, g\n",
        "\n",
        "try:\n",
        "    _Logit\n",
        "except NameError:\n",
        "    class _Logit(nn.Module):\n",
        "        def __init__(self, n):\n",
        "            super().__init__()\n",
        "            self.lin = nn.Linear(n, 1)\n",
        "        def forward(self, x):\n",
        "            return self.lin(x).squeeze(-1)\n",
        "\n",
        "def _train_impact(Xz, y, epochs: int = 300, lr: float = 0.05):\n",
        "    \"\"\"Train a logistic head on BPS features.\"\"\"\n",
        "    X_t = torch.tensor(Xz, dtype=torch.float32)\n",
        "    y_t = torch.tensor(y,  dtype=torch.float32)\n",
        "    model = _Logit(X_t.shape[1])\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    for _ in range(int(epochs)):\n",
        "        opt.zero_grad()\n",
        "        loss = loss_fn(model(X_t), y_t)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    return model\n",
        "\n",
        "def fit_and_predict_oos_bps(df_metrics: pd.DataFrame, horizon: int = 5, split: float = 0.7,\n",
        "                            epochs: int = 300, lr: float = 0.05):\n",
        "    \"\"\"\n",
        "    Train on the first `split` slice and return:\n",
        "      ctx  : {'model','cols','mu','sigma','horizon'}\n",
        "      preds: DataFrame with ['t','y_true','p_up'] for the test slice\n",
        "    (Robust to small/empty test slices; returns empty preds if needed.)\n",
        "    \"\"\"\n",
        "    g = _collapse_metrics(df_metrics)\n",
        "    Xz, y, cols, mu, sigma, g2 = _build_feats_bps(g, horizon=horizon)\n",
        "\n",
        "    n = len(y)\n",
        "    if n < 2:\n",
        "        ctx = {\"model\": _Logit(len(cols)), \"cols\": cols, \"mu\": mu, \"sigma\": sigma, \"horizon\": horizon}\n",
        "        ctx[\"model\"].eval()\n",
        "        return ctx, pd.DataFrame(columns=[\"t\",\"y_true\",\"p_up\"])\n",
        "\n",
        "    cut = max(1, min(int(n * float(split)), n - 1))\n",
        "\n",
        "    Xtr, ytr = Xz[:cut], y[:cut]\n",
        "    Xte, yte = Xz[cut:], y[cut:]\n",
        "    t_te = g2.index[cut:].to_numpy()\n",
        "\n",
        "    model = _train_impact(Xtr, ytr, epochs=epochs, lr=lr)\n",
        "    with torch.no_grad():\n",
        "        Xte_t = torch.tensor(Xte, dtype=torch.float32)\n",
        "        logits = model(Xte_t)\n",
        "        pte = torch.sigmoid(logits).cpu().numpy().ravel() if Xte_t.numel() else np.array([], dtype=float)\n",
        "\n",
        "    ctx = {\"model\": model, \"cols\": cols, \"mu\": mu, \"sigma\": sigma, \"horizon\": horizon}\n",
        "    preds = pd.DataFrame({\"t\": t_te, \"y_true\": yte, \"p_up\": pte})\n",
        "    return ctx, preds\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08729e54",
      "metadata": {
        "id": "08729e54"
      },
      "outputs": [],
      "source": [
        "#Back-compat alias-\n",
        "\n",
        "def fit_and_predict_oos(*args, **kwargs):\n",
        "    \"\"\"Alias: older cells call fit_and_predict_oos; redirect to fit_and_predict_oos_bps.\"\"\"\n",
        "    return fit_and_predict_oos_bps(*args, **kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4320bbe8",
      "metadata": {
        "id": "4320bbe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f299bb2b-4a1e-4f86-a343-93836a9ac6f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Export prefix: /content/data/exports/run_20250909-223518\n",
            "Export prefix: /content/data/exports/run_20250909-223526\n",
            "Export prefix: /content/data/exports/run_20250909-223536\n",
            "Export prefix: /content/data/exports/run_20250909-223542\n",
            "Export prefix: /content/data/exports/run_20250909-223545\n",
            "df_metrics_all shape: (519, 9)\n",
            "symbol\n",
            "BTCUSDT    110\n",
            "BNBUSDT    104\n",
            "ETHUSDT    102\n",
            "SOLUSDT    102\n",
            "ADAUSDT    101\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#Multi-symbol replay\n",
        "\n",
        "import os, glob, pandas as pd\n",
        "\n",
        "FEEDS_DIR = \"/content/data/feeds\"\n",
        "\n",
        "symbols = [\"BTCUSDT\",\"ETHUSDT\",\"SOLUSDT\",\"BNBUSDT\",\"ADAUSDT\"]\n",
        "dfs = []\n",
        "\n",
        "def _latest_feed_for(sym):\n",
        "    if \"feed_paths\" in globals() and isinstance(feed_paths, dict) and sym in feed_paths:\n",
        "        p = feed_paths[sym]\n",
        "        if os.path.exists(p): return p\n",
        "    candidates = sorted(glob.glob(os.path.join(FEEDS_DIR, f\"{sym}_trades_*d.csv\")))\n",
        "    return candidates[-1] if candidates else None\n",
        "\n",
        "for sym in symbols:\n",
        "    path = _latest_feed_for(sym)\n",
        "    if not path:\n",
        "        print(f\"[warn] No stitched feed on disk for {sym}. \"\n",
        "              f\"Run the feed builder (the 'Guards/defaults' cell) first.\")\n",
        "        continue\n",
        "\n",
        "    feed = DataFeed(path, step_ms=500, tick_size=1.0, default_venue=\"FAST\")\n",
        "    env_real = ExecEnvLatencyV2Real([fast, slow], feed=feed, tick_size=0.01,\n",
        "                                    total_qty=3000, max_steps=5000, target_participation=4.0)\n",
        "\n",
        "    df_m, df_f, rlog, df_fills_true, prefix = run_adaptive_kept_logged_v3(env_real, parent_side=\"BUY\")\n",
        "\n",
        "    if not df_m.empty:\n",
        "        df_m[\"symbol\"] = sym\n",
        "        dfs.append(df_m)\n",
        "\n",
        "df_metrics_all = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
        "print(\"df_metrics_all shape:\", df_metrics_all.shape)\n",
        "if \"symbol\" in df_metrics_all.columns:\n",
        "    print(df_metrics_all[\"symbol\"].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c71c736",
      "metadata": {
        "id": "4c71c736",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84c30619-abc5-4454-89f6-192c20c466fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using feed: /content/data/feeds/BTCUSDT_trades_3d.csv\n",
            "Export prefix: /content/data/exports/run_20250909-223546\n",
            "Replay complete | Metrics: (9706, 8) | Fills: (9706, 3)\n",
            "OOS training complete | preds head:\n",
            "       t  y_true      p_up\n",
            "0  5311       0  0.524152\n",
            "1  5312       1  0.998377\n",
            "2  5313       0  0.505454\n",
            "3  5314       1  0.998472\n",
            "4  5315       0  0.562116\n"
          ]
        }
      ],
      "source": [
        "#Replay & OOS training\n",
        "\n",
        "import os, glob, pandas as pd\n",
        "\n",
        "FEEDS_DIR = \"/content/data/feeds\"\n",
        "SYMBOL     = \"BTCUSDT\"\n",
        "\n",
        "def latest_feed(sym: str):\n",
        "    if \"feed_paths\" in globals() and isinstance(feed_paths, dict) and sym in feed_paths:\n",
        "        p = feed_paths[sym]\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    cands = sorted(glob.glob(os.path.join(FEEDS_DIR, f\"{sym}_trades_*d.csv\")))\n",
        "    return cands[-1] if cands else None\n",
        "\n",
        "path = latest_feed(SYMBOL)\n",
        "if not path:\n",
        "    raise FileNotFoundError(\n",
        "        f\"No stitched feed on disk for {SYMBOL}. \"\n",
        "        \"Re-run the feed builder cell (the memory-safe 'Guards / defaults' cell).\"\n",
        "    )\n",
        "\n",
        "print(f\"Using feed: {path}\")\n",
        "\n",
        "step_ms = globals().get(\"STEP_MS\", 500)\n",
        "feed = DataFeed(path, step_ms=step_ms, tick_size=1.0, default_venue=\"FAST\")\n",
        "\n",
        "env_real = ExecEnvLatencyV2Real(\n",
        "    [fast, slow],\n",
        "    feed=feed,\n",
        "    tick_size=0.01,\n",
        "    total_qty=300_000,\n",
        "    max_steps=2_000_000,\n",
        "    target_participation=4.0,\n",
        "    queue_loss_penalty_bps=3.0,\n",
        ")\n",
        "\n",
        "df_metrics, df_fills, route_log, df_fills_true, prefix = run_adaptive_kept_logged_v3(\n",
        "    env_real, parent_side=\"BUY\"\n",
        ")\n",
        "\n",
        "print(\"Replay complete | Metrics:\", df_metrics.shape, \"| Fills:\", df_fills.shape)\n",
        "\n",
        "if \"fit_and_predict_oos\" in globals():\n",
        "    ctx_oos, preds_oos = fit_and_predict_oos(df_metrics, horizon=5, split=0.7)\n",
        "    print(\"OOS training complete | preds head:\\n\", preds_oos.head())\n",
        "else:\n",
        "    print(\"fit_and_predict_oos(...) not defined in this session.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "FEEDS_DIR = \"/content/data/feeds\"\n",
        "for sym in [\"BTCUSDT\",\"ETHUSDT\",\"SOLUSDT\",\"BNBUSDT\",\"ADAUSDT\"]:\n",
        "    files = sorted(glob.glob(os.path.join(FEEDS_DIR, f\"{sym}_trades_*d.csv\")))\n",
        "    print(f\"{sym:8} ->\", files[-1] if files else \"(none)\")\n"
      ],
      "metadata": {
        "id": "TYa02pEiJVCE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5395a0e1-20d8-4277-d3ca-707bfdcc9590"
      },
      "id": "TYa02pEiJVCE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BTCUSDT  -> /content/data/feeds/BTCUSDT_trades_3d.csv\n",
            "ETHUSDT  -> /content/data/feeds/ETHUSDT_trades_3d.csv\n",
            "SOLUSDT  -> /content/data/feeds/SOLUSDT_trades_3d.csv\n",
            "BNBUSDT  -> /content/data/feeds/BNBUSDT_trades_3d.csv\n",
            "ADAUSDT  -> /content/data/feeds/ADAUSDT_trades_3d.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "RAW_DIR = \"/content/data/raw\"\n",
        "for sym in [\"BTCUSDT\",\"ETHUSDT\",\"SOLUSDT\",\"BNBUSDT\",\"ADAUSDT\"]:\n",
        "    c = glob.glob(os.path.join(RAW_DIR, f\"{sym}-aggTrades-*.csv\"))\n",
        "    print(f\"{sym:8} raw days:\", len(c))\n"
      ],
      "metadata": {
        "id": "J3kFrDceJf2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3716f9b6-9584-44fa-aa65-a6cbdaa52015"
      },
      "id": "J3kFrDceJf2_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BTCUSDT  raw days: 15\n",
            "ETHUSDT  raw days: 15\n",
            "SOLUSDT  raw days: 15\n",
            "BNBUSDT  raw days: 15\n",
            "ADAUSDT  raw days: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9831b97",
      "metadata": {
        "id": "e9831b97"
      },
      "outputs": [],
      "source": [
        "#Build Features (BPS)\n",
        "\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "def build_features_and_report(\n",
        "    df_metrics: pd.DataFrame,\n",
        "    horizon: int = 5,\n",
        "    split: float = 0.7,\n",
        "    run_walkforward: bool = True,\n",
        "    wf_k: int = 5,\n",
        "    wf_purge: int = 10,\n",
        "    wf_enter: float = 0.52,\n",
        "    wf_exit: float = 0.48,\n",
        "    wf_fee_bps: float = 1,\n",
        "    wf_slippage_bps: float = 1,\n",
        "    wf_hold_max: int = 48,\n",
        "    wf_bar_stride: int = 1,\n",
        "    wf_bt_kwargs: dict | None = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Builds BPS features using _collapse_metrics/_build_feats_bps and prints a diagnostics report,\n",
        "    then (optionally) runs leak-safe walk-forward evaluation (Option A) and prints its summary.\n",
        "\n",
        "    Returns a dict with artifacts:\n",
        "      {'g_raw','g_clean','cols','mu','sigma','Xz','y','stats','corr','cut',\n",
        "       'wf','wf_summary'}  # wf tables present when run_walkforward=True and successful\n",
        "    \"\"\"\n",
        "    print(\"=== Build Features Report (BPS) ===\")\n",
        "    print(f\"horizon={horizon}  split={split}\")\n",
        "    g_raw = _collapse_metrics(df_metrics)\n",
        "    n_raw = len(g_raw)\n",
        "    print(f\"collapsed rows (g_raw): {n_raw}\")\n",
        "\n",
        "    base_cols = [\"mid\",\"spread\",\"depth_bid_top3\",\"depth_ask_top3\"]\n",
        "    if set(base_cols).issubset(g_raw.columns):\n",
        "        base_na = g_raw[base_cols].isna().sum().rename(\"NA_count\")\n",
        "        print(\"\\nNA counts in base columns (pre-features):\")\n",
        "        print(base_na.to_string())\n",
        "\n",
        "    Xz, y, cols, mu, sigma, g_clean = _build_feats_bps(g_raw, horizon=horizon)\n",
        "    n_clean = len(g_clean)\n",
        "    print(f\"\\nrows after feature cleaning (g_clean): {n_clean} (dropped {n_raw - n_clean})\")\n",
        "    print(f\"feature columns: {cols}\")\n",
        "\n",
        "    feat_df = g_clean[cols].reset_index(drop=True)\n",
        "    finite_mask = np.isfinite(feat_df.to_numpy()).all(axis=1)\n",
        "    n_finite = int(finite_mask.sum())\n",
        "    if n_finite != len(feat_df):\n",
        "        print(f\"[warn] non-finite rows still present: {len(feat_df) - n_finite}\")\n",
        "\n",
        "    def q(series: pd.Series, p: float) -> float:\n",
        "        try:\n",
        "            return float(np.nanpercentile(series.to_numpy(dtype=float), p))\n",
        "        except Exception:\n",
        "            return float(\"nan\")\n",
        "\n",
        "    rows_stats = []\n",
        "    for c in cols:\n",
        "        s = feat_df[c].astype(float)\n",
        "        rows_stats.append({\n",
        "            \"feature\": c,\n",
        "            \"min\":  float(np.nanmin(s))  if len(s) else np.nan,\n",
        "            \"p05\":  q(s, 5),\n",
        "            \"p25\":  q(s, 25),\n",
        "            \"p50\":  float(np.nanmedian(s)) if len(s) else np.nan,\n",
        "            \"p75\":  q(s, 75),\n",
        "            \"p95\":  q(s, 95),\n",
        "            \"max\":  float(np.nanmax(s))  if len(s) else np.nan,\n",
        "            \"mean\": float(np.nanmean(s)) if len(s) else np.nan,\n",
        "            \"std\":  float(np.nanstd(s))  if len(s) else np.nan,\n",
        "            \"na\":   int(s.isna().sum()),\n",
        "        })\n",
        "    stats_df = pd.DataFrame(rows_stats).set_index(\"feature\").round(4)\n",
        "\n",
        "    print(\"\\nFeature summary (bps-scaled where applicable):\")\n",
        "    print(stats_df.to_string())\n",
        "\n",
        "    pos = int(y.sum()); neg = int(len(y) - pos)\n",
        "    pos_rate = pos / max(1, len(y))\n",
        "    cut = max(1, min(int(len(y) * float(split)), len(y) - 1))\n",
        "    print(f\"\\nlabel balance (y): +1={pos}  0/-1={neg}  pos_rate={pos_rate:.3f}\")\n",
        "    print(f\"suggested train/test cut index: {cut}  (train={cut}, test={len(y)-cut})\")\n",
        "\n",
        "    try:\n",
        "        corr = feat_df.corr(numeric_only=True).round(3)\n",
        "        print(\"\\nFeature correlation:\")\n",
        "        print(corr.to_string())\n",
        "    except Exception:\n",
        "        corr = None\n",
        "        print(\"\\n[info] correlation not available for these features\")\n",
        "\n",
        "    wf_res = pd.DataFrame(); wf_summary = pd.DataFrame()\n",
        "    if run_walkforward and 'walkforward_bps' in globals():\n",
        "        try:\n",
        "            wf_res, wf_summary = walkforward_bps(\n",
        "                df_metrics,\n",
        "                horizon=horizon,\n",
        "                k=wf_k,\n",
        "                enter=wf_enter,\n",
        "                exit=wf_exit,\n",
        "                fee_bps=wf_fee_bps,\n",
        "                slippage_bps=wf_slippage_bps,\n",
        "                hold_max=wf_hold_max,\n",
        "                purge=wf_purge,\n",
        "            )\n",
        "            print(\"\\nWalk-forward (purged) summary:\")\n",
        "            if isinstance(wf_summary, pd.DataFrame) and not wf_summary.empty:\n",
        "                print(wf_summary.round(4).to_string())\n",
        "            else:\n",
        "                print(\"[wf] no valid folds after guards.\")\n",
        "        except Exception as e:\n",
        "            print(f\"[wf] error: {type(e).__name__}: {e}\")\n",
        "    else:\n",
        "        print(\"\\n[wf] Skipped (set run_walkforward=True and ensure walkforward_bps is defined).\")\n",
        "\n",
        "    out = {\n",
        "        \"g_raw\": g_raw,\n",
        "        \"g_clean\": g_clean,\n",
        "        \"cols\": cols, \"mu\": mu, \"sigma\": sigma,\n",
        "        \"Xz\": Xz, \"y\": y,\n",
        "        \"stats\": stats_df, \"corr\": corr,\n",
        "        \"cut\": cut,\n",
        "        \"wf\": wf_res, \"wf_summary\": wf_summary,\n",
        "    }\n",
        "    print(\"\\n=== End of Build Features Report ===\")\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49e422cb",
      "metadata": {
        "id": "49e422cb"
      },
      "outputs": [],
      "source": [
        "#Benchmarks & Metrics\n",
        "\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "def buy_and_hold(df_bars):\n",
        "    \"\"\"Equity curve for buy&hold on the same bars (close-to-close).\"\"\"\n",
        "    ret = df_bars[\"close\"].pct_change().fillna(0.0)\n",
        "    eq  = (1 + ret).cumprod()\n",
        "    return pd.DataFrame({\"ret\": ret, \"equity\": eq})\n",
        "\n",
        "def perf_metrics(equity, ret, freq_per_year=365*24):  # 1h bars ≈ 8760/year; adjust if needed\n",
        "    eq = np.asarray(equity, float)\n",
        "    r  = np.asarray(ret, float)\n",
        "    # Max drawdown\n",
        "    roll_max = np.maximum.accumulate(eq)\n",
        "    dd = 1.0 - (eq / (roll_max + 1e-12))\n",
        "    max_dd = float(np.nanmax(dd)) if len(dd) else np.nan\n",
        "    # Sharpe (assuming r is simple return per bar)\n",
        "    mu  = float(np.nanmean(r)) if len(r) else np.nan\n",
        "    sig = float(np.nanstd(r))  if len(r) else np.nan\n",
        "    ann_ret    = (eq[-1]**(freq_per_year/max(1,len(r))) - 1.0) if len(r) else np.nan\n",
        "    ann_sharpe = (mu / (sig + 1e-12)) * np.sqrt(freq_per_year) if (sig==sig and sig>0) else np.nan\n",
        "    calmar     = (ann_ret / (max_dd + 1e-12)) if (max_dd==max_dd and max_dd>0) else np.nan\n",
        "    return {\n",
        "        \"AnnReturn\": ann_ret,\n",
        "        \"Sharpe\": ann_sharpe,\n",
        "        \"MaxDD\": max_dd,\n",
        "        \"Calmar\": calmar,\n",
        "    }\n",
        "\n",
        "def turnover(positions):\n",
        "    \"\"\"Fractional turnover per bar for {0,1,-1} positions array.\"\"\"\n",
        "    p = np.asarray(positions, float)\n",
        "    return float(np.nansum(np.abs(np.diff(p)))) / max(1, len(p)-1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79a98d30",
      "metadata": {
        "id": "79a98d30"
      },
      "outputs": [],
      "source": [
        "# --- Signal -> Position -> PnL backtest (calibrated, with slippage, sizing, and risk overlays) ---\n",
        "\n",
        "\n",
        "import numpy as np, pandas as pd, torch\n",
        "\n",
        "def apply_calibration(raw_logit, T=1.0, iso=None):\n",
        "    \"\"\"Calibrate a single logit with temperature and optional isotonic; overflow-safe.\"\"\"\n",
        "    z = float(raw_logit) / float(T)\n",
        "    # clip to avoid exp overflow\n",
        "    z = max(min(z, 30.0), -30.0)\n",
        "    p = 1.0 / (1.0 + np.exp(-z))\n",
        "    if iso is not None:\n",
        "        try:\n",
        "            return float(iso.predict([p])[0])\n",
        "        except Exception:\n",
        "            pass\n",
        "    return float(p)\n",
        "\n",
        "def backtest_bps(\n",
        "    df_bars, ctx, T=1.0, iso=None,\n",
        "    enter=0.60, exit=0.52,\n",
        "    fee_bps=7, slippage_bps=5, hold_max=48,\n",
        "    assumed_spread_bps=8,          # used if df_bars lacks 'spread_bps'\n",
        "    size_cap=1.0, size_k=2.0,      # position sizing params\n",
        "    atr_for_sizing=None            # optional ATR series to down-weight size in high vol (bars)\n",
        "):\n",
        "    \"\"\"\n",
        "    Long/flat strategy driven by calibrated P(up) using BPS features computed from bar data.\n",
        "\n",
        "    Features per bar:\n",
        "      ret1_bps = pct_change(1) * 1e4\n",
        "      ret3_bps = pct_change(3) * 1e4\n",
        "      ret5_bps = pct_change(5) * 1e4\n",
        "      imb      = 0.0   (no depth in bar data; kept for ctx compatibility)\n",
        "      spr_bps  = df_bars['spread_bps'] if present else `assumed_spread_bps`\n",
        "\n",
        "    Transition cost on entries/exits (fraction): (fee_bps + slippage_bps + 0.5*spread_bps) / 1e4\n",
        "    Position sizing: size_k * (P(up) - 0.5), clipped to [-size_cap, size_cap].\n",
        "    \"\"\"\n",
        "    closes = pd.Series(df_bars[\"close\"], dtype=float).reset_index(drop=True)\n",
        "    n = len(closes)\n",
        "    if n < 2:\n",
        "        empty = pd.Series([0.0]*(n or 1))\n",
        "        bench = buy_and_hold(pd.DataFrame({\"close\": closes}))\n",
        "        return {\n",
        "            \"pnl\": empty.rename(\"ret\"),\n",
        "            \"equity\": (1+empty).cumprod().rename(\"equity\"),\n",
        "            \"positions\": empty*0,\n",
        "            \"entries\": [], \"exits\": [], \"logits\": empty*0,\n",
        "            \"bench_equity\": bench[\"equity\"],\n",
        "            \"bench_metrics\": perf_metrics(bench[\"equity\"].values, bench[\"ret\"].values, freq_per_year=8760),\n",
        "            \"metrics\": {\"AnnReturn\": np.nan, \"Sharpe\": np.nan, \"MaxDD\": np.nan, \"Calmar\": np.nan},\n",
        "            \"turnover\": 0.0,\n",
        "        }\n",
        "\n",
        "    # ---------- build BPS features from bars ----------\n",
        "    ret1_bps = closes.pct_change(1) * 1e4\n",
        "    ret3_bps = closes.pct_change(3) * 1e4\n",
        "    ret5_bps = closes.pct_change(5) * 1e4\n",
        "    spr_bps_series = (\n",
        "        pd.Series(df_bars[\"spread_bps\"], dtype=float).reset_index(drop=True)\n",
        "        if \"spread_bps\" in df_bars.columns else pd.Series([assumed_spread_bps]*n, dtype=float)\n",
        "    )\n",
        "    imb_series = pd.Series([0.0]*n, dtype=float)  # no depth with bar data\n",
        "\n",
        "    feats = pd.DataFrame({\n",
        "        \"ret1_bps\": ret1_bps.fillna(0.0).clip(-50, 50),\n",
        "        \"ret3_bps\": ret3_bps.fillna(0.0).clip(-100, 100),\n",
        "        \"ret5_bps\": ret5_bps.fillna(0.0).clip(-150, 150),\n",
        "        \"imb\":      imb_series,\n",
        "        \"spr_bps\":  spr_bps_series.fillna(assumed_spread_bps).clip(0, 50),\n",
        "    })\n",
        "\n",
        "    cols  = ctx[\"cols\"]\n",
        "    mu    = np.asarray(ctx[\"mu\"], float)\n",
        "    sigma = np.asarray(ctx[\"sigma\"], float) + 1e-9\n",
        "\n",
        "    # ---------- model → logits → calibrated probabilities ----------\n",
        "    X  = feats[cols].to_numpy(dtype=float)\n",
        "    Xz = (X - mu) / sigma\n",
        "    with torch.no_grad():\n",
        "        logits = ctx[\"model\"](torch.tensor(Xz, dtype=torch.float32)).cpu().numpy().ravel()\n",
        "\n",
        "    # temperature + optional isotonic (overflow-safe via apply_calibration)\n",
        "    p_up = np.array([apply_calibration(lg, T=T, iso=iso) for lg in logits], dtype=float)\n",
        "\n",
        "    # ---------- trading loop ----------\n",
        "    pos = 0.0\n",
        "    pnl, positions, entries, exits, logits_hist = [], [], [], [], []\n",
        "\n",
        "    def _size_from_p(p, t):\n",
        "        raw = size_k * (p - 0.5)\n",
        "        if atr_for_sizing is not None and t < len(atr_for_sizing):\n",
        "            vol = float(atr_for_sizing.iloc[t])\n",
        "            if vol == vol and vol > 0:\n",
        "                raw = raw / max(vol, 1e-9)\n",
        "        return float(np.clip(raw, -size_cap, size_cap))\n",
        "\n",
        "    for t in range(n):\n",
        "        logits_hist.append(float(logits[t]))\n",
        "        target = _size_from_p(p_up[t], t)\n",
        "\n",
        "        # hysteresis gates\n",
        "        if pos == 0.0 and p_up[t] >= enter:\n",
        "            pos = target; entries.append(t)\n",
        "        elif pos != 0.0 and (p_up[t] <= exit or (entries and t - entries[-1] >= hold_max)):\n",
        "            pos = 0.0; exits.append(t)\n",
        "\n",
        "        positions.append(pos)\n",
        "\n",
        "        if t == 0:\n",
        "            pnl.append(0.0); continue\n",
        "\n",
        "        ret  = (closes.iloc[t] / closes.iloc[t-1]) - 1.0\n",
        "        cost = 0.0\n",
        "        if (t-1) in entries or (t-1) in exits:\n",
        "            spr_here = float(spr_bps_series.iloc[t])\n",
        "            cost = (float(fee_bps) + float(slippage_bps) + 0.5 * spr_here) / 1e4  # bps → fraction\n",
        "\n",
        "        pnl.append(pos * ret - cost)\n",
        "\n",
        "    pnl = pd.Series(pnl, name=\"ret\")\n",
        "    eq  = (1 + pnl).cumprod().rename(\"equity\")\n",
        "\n",
        "    # Benchmarks & metrics\n",
        "    bench = buy_and_hold(pd.DataFrame({\"close\": closes}))\n",
        "    mets  = perf_metrics(eq.values, pnl.values, freq_per_year=8760)\n",
        "\n",
        "    return {\n",
        "        \"pnl\": pnl,\n",
        "        \"equity\": eq,\n",
        "        \"positions\": pd.Series(positions, name=\"pos\"),\n",
        "        \"entries\": entries,\n",
        "        \"exits\": exits,\n",
        "        \"logits\": pd.Series(logits_hist, name=\"logit\"),\n",
        "        \"bench_equity\": bench[\"equity\"],\n",
        "        \"bench_metrics\": perf_metrics(bench[\"equity\"].values, bench[\"ret\"].values, freq_per_year=8760),\n",
        "        \"metrics\": mets,\n",
        "        \"turnover\": turnover(positions),\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58c696cf",
      "metadata": {
        "id": "58c696cf"
      },
      "outputs": [],
      "source": [
        "# --- Risk overlays: kill-switch / max daily loss / cooldown ---\n",
        "\n",
        "def apply_risk_overlays(\n",
        "    pnl_series: pd.Series,\n",
        "    kill_dd=0.25,     # stop if peak-to-trough equity drawdown >= 25%\n",
        "    max_daily=-0.05,  # cap daily loss at -5% (approx; scaled to 24 1h bars)\n",
        "    cooldown_days=1   # pause trading this many days after kill-switch\n",
        "):\n",
        "    pnl = pnl_series.copy().reset_index(drop=True)\n",
        "    eq  = (1 + pnl).cumprod()\n",
        "    peak = eq.copy()\n",
        "    dd = []\n",
        "    paused_until_day = -1\n",
        "    day = 0\n",
        "\n",
        "    for t in range(len(pnl)):\n",
        "        # day boundary every 24 bars (1h bars). Adjust if using a different bar size.\n",
        "        if t % 24 == 0 and t > 0:\n",
        "            day += 1\n",
        "\n",
        "        # cooldown: sit flat but carry equity forward\n",
        "        if day <= paused_until_day:\n",
        "            pnl.iloc[t] = 0.0\n",
        "            eq.iloc[t] = eq.iloc[t-1] if t > 0 else 1.0\n",
        "            peak.iloc[t] = max(peak.iloc[t-1], eq.iloc[t]) if t > 0 else eq.iloc[t]\n",
        "            dd.append(1.0 - eq.iloc[t] / (peak.iloc[t] + 1e-12))\n",
        "            continue\n",
        "\n",
        "        # daily loss cap (rough): limit worst hourly loss to daily cap / 24\n",
        "        if pnl.iloc[t] < (max_daily / 24.0):\n",
        "            pnl.iloc[t] = (max_daily / 24.0)\n",
        "\n",
        "        # update equity and drawdown\n",
        "        eq.iloc[t] = (eq.iloc[t-1] if t > 0 else 1.0) * (1 + pnl.iloc[t])\n",
        "        peak.iloc[t] = max(peak.iloc[t-1], eq.iloc[t]) if t > 0 else eq.iloc[t]\n",
        "        ddt = 1.0 - eq.iloc[t] / (peak.iloc[t] + 1e-12)\n",
        "        dd.append(ddt)\n",
        "\n",
        "        # kill-switch and cooldown trigger\n",
        "        if ddt >= kill_dd:\n",
        "            paused_until_day = day + cooldown_days\n",
        "\n",
        "    return pnl, pd.Series(eq, name=\"equity_guarded\"), pd.Series(dd, name=\"dd\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "948fa94e",
      "metadata": {
        "id": "948fa94e"
      },
      "outputs": [],
      "source": [
        "# --- Example usage ---\n",
        "\n",
        "# df_bars = pd.DataFrame({\"close\": your_close_series, \"spread_bps\": optional_spread_bps})\n",
        "# bt = backtest_bps(\n",
        "#     df_bars, ctx_live,\n",
        "#     T=globals().get(\"TEMP_CAL\", 1.0),\n",
        "#     iso=globals().get(\"CAL_ISO\", None),\n",
        "#     fee_bps=7, slippage_bps=5, assumed_spread_bps=8,\n",
        "#     size_k=2.0, size_cap=1.0\n",
        "# )\n",
        "# pnl_guarded, eq_guarded, dd = apply_risk_overlays(bt[\"pnl\"], kill_dd=0.20, max_daily=-0.03, cooldown_days=2)\n",
        "# print(bt[\"metrics\"], \"Turnover:\", bt[\"turnover\"])\n",
        "# eq_guarded.plot(title=\"Strategy equity (risk overlays)\"); bt[\"bench_equity\"].plot(title=\"Buy&Hold equity\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b0d95ba",
      "metadata": {
        "id": "4b0d95ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2cec258-08ba-49e1-b97b-847fdff16ae6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[wf] No valid folds after purging and guards.\n",
            "df_metrics_all: (519, 9)\n",
            "symbol\n",
            "BTCUSDT    110\n",
            "BNBUSDT    104\n",
            "ETHUSDT    102\n",
            "SOLUSDT    102\n",
            "ADAUSDT    101\n",
            "=== Build Features Report (BPS) ===\n",
            "horizon=60  split=0.7\n",
            "collapsed rows (g_raw): 519\n",
            "\n",
            "NA counts in base columns (pre-features):\n",
            "mid               127\n",
            "spread            127\n",
            "depth_bid_top3      0\n",
            "depth_ask_top3      0\n",
            "\n",
            "rows after feature cleaning (g_clean): 312 (dropped 207)\n",
            "feature columns: ['ret1_bps', 'ret3_bps', 'ret5_bps', 'ret10_bps', 'ret20_bps', 'rv10_bps', 'imb', 'spr_bps', 'ema20_diff', 'ema50_diff', 'hod_sin', 'hod_cos', 'dow_sin', 'dow_cos', 'ema20_slope', 'ema50_slope', 'ema20_curve']\n",
            "\n",
            "Feature summary (bps-scaled where applicable):\n",
            "                min     p05     p25     p50     p75     p95     max    mean     std  na\n",
            "feature                                                                                \n",
            "ret1_bps    -0.4834 -0.1736 -0.0185  0.0018  0.0184  0.1734  0.2029 -0.0005  0.0910   0\n",
            "ret3_bps    -0.6465 -0.1714  0.0000  0.0054  0.0072  0.1579  0.2065 -0.0026  0.0917   0\n",
            "ret5_bps    -0.6461 -0.1759  0.0000  0.0090  0.0108  0.1617  0.2096 -0.0045  0.1064   0\n",
            "ret10_bps   -0.6456 -0.2273 -0.0150  0.0176  0.0253  0.1901  0.2187 -0.0099  0.1361   0\n",
            "ret20_bps   -0.6357 -0.4625 -0.0237  0.0357  0.0506  0.1934  0.2395 -0.0178  0.1739   0\n",
            "rv10_bps     0.0000  0.0066  0.0942  0.1841  0.4023  0.5465  0.6149  0.2454  0.1785   0\n",
            "imb          0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000   0\n",
            "spr_bps      0.0000  0.0000  0.0000  0.0000  0.0009  0.0009  0.0018  0.0003  0.0004   0\n",
            "ema20_diff  -0.4778 -0.1656 -0.0416 -0.0022  0.0427  0.1168  0.1394 -0.0084  0.0876   0\n",
            "ema50_diff  -0.4893 -0.2594 -0.0453  0.0039  0.0483  0.1453  0.1686 -0.0209  0.1195   0\n",
            "hod_sin     -1.0000 -0.9659 -0.7071  0.0000  0.7071  0.9659  1.0000 -0.0000  0.7071   0\n",
            "hod_cos     -1.0000 -0.9659 -0.7071 -0.0000  0.7071  0.9659  1.0000 -0.0000  0.7071   0\n",
            "dow_sin     -0.9749 -0.9749 -0.7818  0.0000  0.7818  0.9749  0.9749  0.0031  0.7075   0\n",
            "dow_cos     -0.9010 -0.9010 -0.9010 -0.2225  0.6235  1.0000  1.0000  0.0065  0.7067   0\n",
            "ema20_slope -0.0503 -0.0174 -0.0044 -0.0002  0.0045  0.0123  0.0147 -0.0009  0.0092   0\n",
            "ema50_slope -0.0200 -0.0106 -0.0019  0.0002  0.0020  0.0059  0.0069 -0.0009  0.0049   0\n",
            "ema20_curve -0.0958 -0.0350 -0.0037  0.0030  0.0055  0.0106  0.0256 -0.0018  0.0160   0\n",
            "\n",
            "label balance (y): +1=196  0/-1=116  pos_rate=0.628\n",
            "suggested train/test cut index: 218  (train=218, test=94)\n",
            "\n",
            "Feature correlation:\n",
            "             ret1_bps  ret3_bps  ret5_bps  ret10_bps  ret20_bps  rv10_bps  imb  spr_bps  ema20_diff  ema50_diff  hod_sin  hod_cos  dow_sin  dow_cos  ema20_slope  ema50_slope  ema20_curve\n",
            "ret1_bps        1.000     0.576     0.437      0.465      0.275    -0.056  NaN    0.550       0.567       0.420   -0.046    0.001    0.018   -0.020        0.567        0.420        0.089\n",
            "ret3_bps        0.576     1.000     0.640      0.522      0.363    -0.158  NaN    0.311       0.654       0.491   -0.115    0.027   -0.011   -0.059        0.654        0.491        0.447\n",
            "ret5_bps        0.437     0.640     1.000      0.636      0.439    -0.217  NaN    0.249       0.746       0.583   -0.142    0.077   -0.033   -0.019        0.746        0.583        0.641\n",
            "ret10_bps       0.465     0.522     0.636      1.000      0.630    -0.265  NaN    0.319       0.846       0.727   -0.078    0.173   -0.004   -0.036        0.846        0.727        0.746\n",
            "ret20_bps       0.275     0.363     0.439      0.630      1.000    -0.040  NaN    0.223       0.803       0.838    0.057    0.063   -0.013   -0.002        0.803        0.838        0.807\n",
            "rv10_bps       -0.056    -0.158    -0.217     -0.265     -0.040     1.000  NaN    0.083      -0.161       0.044   -0.010   -0.036   -0.017    0.031       -0.161        0.044       -0.162\n",
            "imb               NaN       NaN       NaN        NaN        NaN       NaN  NaN      NaN         NaN         NaN      NaN      NaN      NaN      NaN          NaN          NaN          NaN\n",
            "spr_bps         0.550     0.311     0.249      0.319      0.223     0.083  NaN    1.000       0.377       0.307   -0.022   -0.023   -0.046   -0.015        0.377        0.307        0.127\n",
            "ema20_diff      0.567     0.654     0.746      0.846      0.803    -0.161  NaN    0.377       1.000       0.917   -0.073    0.124   -0.007   -0.028        1.000        0.917        0.871\n",
            "ema50_diff      0.420     0.491     0.583      0.727      0.838     0.044  NaN    0.307       0.917       1.000   -0.036    0.111   -0.005   -0.018        0.917        1.000        0.859\n",
            "hod_sin        -0.046    -0.115    -0.142     -0.078      0.057    -0.010  NaN   -0.022      -0.073      -0.036    1.000   -0.000   -0.005   -0.014       -0.073       -0.036       -0.061\n",
            "hod_cos         0.001     0.027     0.077      0.173      0.063    -0.036  NaN   -0.023       0.124       0.111   -0.000    1.000    0.006    0.004        0.124        0.111        0.150\n",
            "dow_sin         0.018    -0.011    -0.033     -0.004     -0.013    -0.017  NaN   -0.046      -0.007      -0.005   -0.005    0.006    1.000   -0.001       -0.007       -0.005       -0.019\n",
            "dow_cos        -0.020    -0.059    -0.019     -0.036     -0.002     0.031  NaN   -0.015      -0.028      -0.018   -0.014    0.004   -0.001    1.000       -0.028       -0.018       -0.021\n",
            "ema20_slope     0.567     0.654     0.746      0.846      0.803    -0.161  NaN    0.377       1.000       0.917   -0.073    0.124   -0.007   -0.028        1.000        0.917        0.871\n",
            "ema50_slope     0.420     0.491     0.583      0.727      0.838     0.044  NaN    0.307       0.917       1.000   -0.036    0.111   -0.005   -0.018        0.917        1.000        0.859\n",
            "ema20_curve     0.089     0.447     0.641      0.746      0.807    -0.162  NaN    0.127       0.871       0.859   -0.061    0.150   -0.019   -0.021        0.871        0.859        1.000\n",
            "[wf] No valid folds after purging and guards.\n",
            "\n",
            "Walk-forward (purged) summary:\n",
            "[wf] no valid folds after guards.\n",
            "\n",
            "=== End of Build Features Report ===\n"
          ]
        }
      ],
      "source": [
        "# --- Walk-forward / Purged CV for BPS head (with bar_stride + bt_kwargs + Trades/Costs) ---\n",
        "\n",
        "\n",
        "import numpy as np, pandas as pd, torch\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "def walkforward_bps(\n",
        "    df_metrics,\n",
        "    horizon=5,\n",
        "    k=5,\n",
        "    enter=0.52,\n",
        "    exit=0.48,\n",
        "    fee_bps=7,\n",
        "    slippage_bps=5,\n",
        "    hold_max=48,\n",
        "    purge=10,\n",
        "    min_train=20,\n",
        "    min_test=5,\n",
        "    bar_stride=5,            # use every Nth bar in test folds (e.g., 5 => ~500 ms if step_ms=100)\n",
        "    bt_kwargs=None           # pass-through to backtest_bps (size_k, size_cap, etc.)\n",
        "):\n",
        "    \"\"\"\n",
        "    Purged chronological K-fold evaluation for the BPS head.\n",
        "\n",
        "    Returns (res_df, summary_df). If data are insufficient, returns empty dataframes\n",
        "    instead of raising.\n",
        "\n",
        "    Args:\n",
        "      df_metrics : per-step metrics from a replay run\n",
        "      horizon    : label lookahead (steps)\n",
        "      k          : desired number of folds (auto-reduced if dataset is small)\n",
        "      enter/exit : hysteresis gates for backtest_bps (no calibration applied here)\n",
        "      fee_bps/slippage_bps/hold_max : cost & holding assumptions\n",
        "      purge      : steps dropped from end of train fold before test (leakage guard)\n",
        "      min_train  : minimum train rows per fold\n",
        "      min_test   : minimum test rows per fold\n",
        "      bar_stride : downsample test bars by this stride (>=1)\n",
        "      bt_kwargs  : dict passed to backtest_bps (e.g., {\"size_k\":4.0,\"size_cap\":1.5})\n",
        "    \"\"\"\n",
        "    bt_kwargs = bt_kwargs or {}\n",
        "\n",
        "    # Build features exactly like training\n",
        "    g = _collapse_metrics(df_metrics)\n",
        "    Xz, y, cols, mu, sigma, g2 = _build_feats_bps(g, horizon=horizon)\n",
        "\n",
        "    # Early exit if not enough clean rows after feature build\n",
        "    if len(y) < (min_train + min_test + max(purge, 0)):\n",
        "        print(\"[wf] Not enough clean rows after feature build.\")\n",
        "        return pd.DataFrame(), pd.DataFrame({\"mean\": [], \"std\": []})\n",
        "\n",
        "    # Cap k to a sensible value given data size\n",
        "    max_folds = max(2, len(y) // (min_train + min_test))\n",
        "    k_eff = int(max(2, min(int(k), max_folds)))\n",
        "    tscv = TimeSeriesSplit(n_splits=k_eff)\n",
        "\n",
        "    stats = []\n",
        "\n",
        "    for train_idx, test_idx in tscv.split(Xz):\n",
        "        # Purge a tail near the test start to reduce leakage\n",
        "        if purge > 0 and len(train_idx) > 0:\n",
        "            test_start = int(test_idx[0])\n",
        "            train_idx  = train_idx[train_idx <= (test_start - purge)]\n",
        "\n",
        "        # Enforce minimum fold sizes\n",
        "        if len(train_idx) < min_train or len(test_idx) < min_test:\n",
        "            continue\n",
        "\n",
        "        Xtr, ytr = Xz[train_idx], y[train_idx]\n",
        "        if not np.isfinite(Xtr).all():\n",
        "            continue  # skip pathological folds\n",
        "\n",
        "        # Fit tiny torch head\n",
        "        model = _train_impact(Xtr, ytr, epochs=250, lr=0.05)\n",
        "        # Compute train moments for z-scoring\n",
        "        ctx = {\n",
        "            \"model\": model,\n",
        "            \"cols\": cols,\n",
        "            \"mu\": Xtr.mean(axis=0),\n",
        "            \"sigma\": Xtr.std(axis=0) + 1e-9,\n",
        "        }\n",
        "\n",
        "        # === Calibrate temperature (T) and isotonic mapping on the training fold ===\n",
        "        try:\n",
        "            # Compute raw logits on training data\n",
        "            logits_tr = model(torch.tensor(Xtr, dtype=torch.float32)).detach().numpy().ravel()\n",
        "            y_tr      = ytr\n",
        "            # Grid search for T that minimizes Brier score\n",
        "            T_grid = np.linspace(0.6, 4.0, 18)\n",
        "            best_T = 1.0\n",
        "            best_brier = np.inf\n",
        "            for T in T_grid:\n",
        "                # logistic probabilities with temperature\n",
        "                p = 1.0 / (1.0 + np.exp(-(logits_tr / T)))\n",
        "                brier = float(np.mean((p - y_tr)**2)) if len(y_tr) else np.inf\n",
        "                if brier < best_brier:\n",
        "                    best_brier, best_T = brier, float(T)\n",
        "            # Fit isotonic regression on calibrated probabilities\n",
        "            p_tr  = 1.0 / (1.0 + np.exp(-(logits_tr / best_T)))\n",
        "            from sklearn.isotonic import IsotonicRegression\n",
        "            iso_model = IsotonicRegression(y_min=0.0, y_max=1.0, out_of_bounds=\"clip\")\n",
        "            iso_model.fit(p_tr, y_tr)\n",
        "        except Exception:\n",
        "            # Fallback: no calibration\n",
        "            best_T = 1.0\n",
        "            iso_model = None\n",
        "\n",
        "        # Build test-bar proxy from mids; allow downsampling by bar_stride\n",
        "        mids = (\n",
        "            g2.loc[test_idx, \"mid\"]\n",
        "              .astype(float)\n",
        "              .replace([np.inf, -np.inf], np.nan)\n",
        "              .dropna()\n",
        "              .reset_index(drop=True)\n",
        "        )\n",
        "        if bar_stride > 1:\n",
        "            mids = mids.iloc[::bar_stride].reset_index(drop=True)\n",
        "        if len(mids) < 2:\n",
        "            continue\n",
        "\n",
        "        df_bars = pd.DataFrame({\"close\": mids})\n",
        "\n",
        "        # ---- Per-fold dwell/hysteresis tuning ----\n",
        "        # We search for the best enter/exit/hold_max gates on the training fold (leakage-free)\n",
        "        enter_fold, exit_fold, hold_fold = enter, exit, hold_max\n",
        "        try:\n",
        "            gate_grid = [\n",
        "                (enter, exit, hold_max),\n",
        "                (min(enter + 0.02, 0.99), min(exit + 0.02, 0.99), max(int(hold_max * 1.5), 1)),\n",
        "                (min(enter + 0.05, 0.99), min(exit + 0.04, 0.99), max(int(hold_max * 2.0), 1)),\n",
        "            ]\n",
        "            best_score = -np.inf\n",
        "            mids_tr = (\n",
        "                g2.loc[train_idx, \"mid\"]\n",
        "                  .astype(float)\n",
        "                  .replace([np.inf, -np.inf], np.nan)\n",
        "                  .dropna()\n",
        "                  .reset_index(drop=True)\n",
        "            )\n",
        "            if bar_stride > 1:\n",
        "                mids_tr = mids_tr.iloc[::bar_stride].reset_index(drop=True)\n",
        "            if len(mids_tr) >= 2:\n",
        "                df_tr_bars = pd.DataFrame({\"close\": mids_tr})\n",
        "                for en_g, ex_g, hm_g in gate_grid:\n",
        "                    try:\n",
        "                        bt_tr = backtest_bps(\n",
        "                            df_tr_bars, ctx, T=best_T, iso=iso_model,\n",
        "                            enter=en_g, exit=ex_g,\n",
        "                            fee_bps=fee_bps, slippage_bps=slippage_bps,\n",
        "                            hold_max=hm_g,\n",
        "                            **bt_kwargs\n",
        "                        )\n",
        "                        score = float(bt_tr[\"metrics\"].get(\"Sharpe\", np.nan))\n",
        "                        if np.isfinite(score) and score > best_score:\n",
        "                            best_score = score\n",
        "                            enter_fold, exit_fold, hold_fold = en_g, ex_g, hm_g\n",
        "                    except Exception:\n",
        "                        continue\n",
        "        except Exception:\n",
        "            enter_fold, exit_fold, hold_fold = enter, exit, hold_max\n",
        "\n",
        "        # Backtest on this test fold using per-fold calibration and tuned gates\n",
        "        try:\n",
        "            bt = backtest_bps(\n",
        "                df_bars, ctx, T=best_T, iso=iso_model,\n",
        "                enter=enter_fold, exit=exit_fold,\n",
        "                fee_bps=fee_bps, slippage_bps=slippage_bps, hold_max=hold_fold,\n",
        "                **bt_kwargs\n",
        "            )\n",
        "\n",
        "            # Count trades as the number of position changes (entries/exits)\n",
        "            pos = bt.get(\"positions\", pd.Series(dtype=float)).reset_index(drop=True)\n",
        "            trades = int((pos.diff().abs() > 0).sum())\n",
        "\n",
        "            stats.append({\n",
        "                \"Sharpe\":        float(bt[\"metrics\"][\"Sharpe\"]),\n",
        "                \"HitRate\":       float((bt[\"pnl\"] > 0).mean()),\n",
        "                \"Turnover\":      float(bt[\"turnover\"]),\n",
        "                \"Trades\":        trades,\n",
        "                \"Fee_bps\":       float(fee_bps),\n",
        "                \"Slippage_bps\":  float(slippage_bps),\n",
        "            })\n",
        "        except Exception:\n",
        "            # Skip a broken fold rather than failing the whole evaluation\n",
        "            continue\n",
        "\n",
        "    if not stats:\n",
        "        print(\"[wf] No valid folds after purging and guards.\")\n",
        "        return pd.DataFrame(), pd.DataFrame({\"mean\": [], \"std\": []})\n",
        "\n",
        "    res = pd.DataFrame(stats)\n",
        "    summary = res.agg([\"mean\", \"std\"]).T\n",
        "    print(\"Walk-forward summary:\\n\", summary)\n",
        "    return res, summary\n",
        "\n",
        "wf_res, wf_sum = walkforward_bps(\n",
        "    df_metrics_all,\n",
        "    horizon=60, k=3, purge=10,\n",
        "    enter=0.52, exit=0.48,\n",
        "    fee_bps=1.0, slippage_bps=1.0, hold_max=180,\n",
        "    bar_stride=5,                          # ~0.5s bars\n",
        "    bt_kwargs={\"size_k\": 4.0, \"size_cap\": 1.5}\n",
        ")\n",
        "\n",
        "print(\"df_metrics_all:\", df_metrics_all.shape)\n",
        "print(df_metrics_all[\"symbol\"].value_counts().to_string())\n",
        "\n",
        "rep = build_features_and_report(\n",
        "    df_metrics_all, horizon=60, split=0.7,\n",
        "    run_walkforward=True, wf_k=3, wf_purge=10,\n",
        "    wf_enter=0.52, wf_exit=0.48,\n",
        "    wf_fee_bps=1.0, wf_slippage_bps=1.0, wf_hold_max=180,\n",
        "    wf_bar_stride=5,\n",
        "    wf_bt_kwargs={\"size_k\": 4.0, \"size_cap\": 1.5}\n",
        ")\n",
        "\n",
        "# --- Temperature calibration (learn T on OOS to minimize Brier) ---\n",
        "import numpy as np, torch\n",
        "\n",
        "def calibrate_temperature(ctx, df_metrics, horizon=5, split=0.7, T_grid=None):\n",
        "    \"\"\"\n",
        "    Rebuild BPS features on df_metrics, split train/test, run model on test,\n",
        "    choose T in T_grid that minimizes Brier score of sigmoid(logit/T).\n",
        "    Returns best_T (float).\n",
        "    \"\"\"\n",
        "    if T_grid is None:\n",
        "        T_grid = np.linspace(0.6, 4.0, 18)  # tune if needed\n",
        "\n",
        "    # Rebuild BPS features exactly like fit_and_predict_oos_bps\n",
        "    g = _collapse_metrics(df_metrics)\n",
        "    Xz, y, cols, mu, sigma, g2 = _build_feats_bps(g, horizon=horizon)\n",
        "    n = len(y); cut = max(20, int(n*float(split)))\n",
        "    if n - cut < 3: cut = max(5, n - 3)\n",
        "\n",
        "    # Run model on test slice to get raw logits\n",
        "    Xte = torch.tensor(Xz[cut:], dtype=torch.float32)\n",
        "    yte = np.asarray(y[cut:], dtype=int)\n",
        "    with torch.no_grad():\n",
        "        logits = ctx[\"model\"](Xte).numpy().ravel()\n",
        "\n",
        "    # Grid search T\n",
        "    def brier(p, y): return float(np.mean((p - y)**2)) if len(y) else np.nan\n",
        "    best_T, best_brier = 1.0, np.inf\n",
        "    for T in T_grid:\n",
        "        p = 1.0 / (1.0 + np.exp(-(logits / T)))\n",
        "        s = brier(p, yte)\n",
        "        if s < best_brier:\n",
        "            best_brier, best_T = s, float(T)\n",
        "    return best_T\n",
        "\n",
        "# Example:\n",
        "# df_metrics, _, _ = run_adaptive_kept_logged(env_real)\n",
        "# T_star = calibrate_temperature(ctx_live, df_metrics, horizon=5, split=0.7)\n",
        "# print(\"Best T:\", T_star)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f0d1f8e",
      "metadata": {
        "id": "8f0d1f8e"
      },
      "outputs": [],
      "source": [
        "# --- Isotonic calibration on OOS ---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d73f0ecd",
      "metadata": {
        "id": "d73f0ecd"
      },
      "outputs": [],
      "source": [
        "!pip -q install scikit-learn\n",
        "\n",
        "import numpy as np, torch\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "def fit_isotonic(ctx, df_metrics, horizon=5, split=0.7):\n",
        "    g = _collapse_metrics(df_metrics)\n",
        "    Xz, y, cols, mu, sigma, g2 = _build_feats_bps(g, horizon=horizon)\n",
        "    n = len(y); cut = max(20, int(n*float(split)))\n",
        "    if n - cut < 3: cut = max(5, n - 3)\n",
        "\n",
        "    Xte = torch.tensor(Xz[cut:], dtype=torch.float32)\n",
        "    yte = np.asarray(y[cut:], dtype=int)\n",
        "    with torch.no_grad():\n",
        "        p_raw = torch.sigmoid(ctx[\"model\"](Xte)).numpy().ravel()\n",
        "\n",
        "    iso = IsotonicRegression(y_min=0.0, y_max=1.0, out_of_bounds=\"clip\")\n",
        "    iso.fit(p_raw, yte)\n",
        "    return iso  # callable: p_cal = iso.predict([p])\n",
        "\n",
        "# Example:\n",
        "# iso = fit_isotonic(ctx_live, df_metrics)\n",
        "# p_cal = iso.predict([0.73])[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e617ad2d",
      "metadata": {
        "id": "e617ad2d"
      },
      "outputs": [],
      "source": [
        "# --- Adaptive v1.1: smooth forecast-to-action mapping (NaN-safe, calib-guarded) ---\n",
        "\n",
        "\n",
        "import numpy as np, pandas as pd, torch\n",
        "\n",
        "def _p_up_from_ctx(ctx, df_metrics):\n",
        "    \"\"\"\n",
        "    Compute P(up) from the most recent rows of df_metrics using the BPS features in ctx.\n",
        "    NaN/inf-safe, and won't call isotonic if the raw probability is NaN.\n",
        "    \"\"\"\n",
        "    g = _collapse_metrics(df_metrics).copy()\n",
        "    if len(g) < 6:\n",
        "        return 0.5\n",
        "\n",
        "    # Build BPS features on the last row\n",
        "    last = g.iloc[[-1]].copy()\n",
        "    last[\"ret1_bps\"] = g[\"mid\"].pct_change(1).iloc[-1] * 1e4\n",
        "    last[\"ret3_bps\"] = g[\"mid\"].pct_change(3).iloc[-1] * 1e4\n",
        "    last[\"ret5_bps\"] = g[\"mid\"].pct_change(5).iloc[-1] * 1e4\n",
        "    last[\"imb\"]      = (g[\"depth_bid_top3\"] - g[\"depth_ask_top3\"]) / (\n",
        "                        g[\"depth_bid_top3\"] + g[\"depth_ask_top3\"] + 1e-9)\n",
        "    last[\"spr_bps\"]  = (g[\"spread\"] / (g[\"mid\"] + 1e-9)) * 1e4\n",
        "\n",
        "    cols, mu, sigma = ctx[\"cols\"], ctx[\"mu\"], ctx[\"sigma\"]\n",
        "\n",
        "    # If last row is dirty, backfill with the most recent clean row; else fallback to 0.5\n",
        "    row = last[cols].replace([np.inf, -np.inf], np.nan)\n",
        "    if row.isna().any(axis=None):\n",
        "        prev = g.iloc[:-1].copy()\n",
        "        prev[\"ret1_bps\"] = prev[\"mid\"].pct_change(1) * 1e4\n",
        "        prev[\"ret3_bps\"] = prev[\"mid\"].pct_change(3) * 1e4\n",
        "        prev[\"ret5_bps\"] = prev[\"mid\"].pct_change(5) * 1e4\n",
        "        prev[\"imb\"]      = (prev[\"depth_bid_top3\"] - prev[\"depth_ask_top3\"]) / (\n",
        "                            prev[\"depth_bid_top3\"] + prev[\"depth_ask_top3\"] + 1e-9)\n",
        "        prev[\"spr_bps\"]  = (prev[\"spread\"] / (prev[\"mid\"] + 1e-9)) * 1e4\n",
        "        prev = prev.replace([np.inf, -np.inf], np.nan).dropna(subset=cols).tail(1)\n",
        "        if prev.empty:\n",
        "            return 0.5\n",
        "        x = prev[cols].to_numpy(dtype=float)\n",
        "    else:\n",
        "        x = row.to_numpy(dtype=float)\n",
        "\n",
        "    # Normalize (NaN -> 0 to be safe)\n",
        "    x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    xz = (x - np.asarray(mu, float)) / (np.asarray(sigma, float) + 1e-9)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logit = ctx[\"model\"](torch.tensor(xz, dtype=torch.float32)).item()\n",
        "\n",
        "    # Temperature calibration\n",
        "    T = float(globals().get(\"TEMP_CAL\", 1.0))\n",
        "    p_raw = 1.0 / (1.0 + np.exp(-(logit / T)))\n",
        "\n",
        "    # If p_raw isn’t finite, don’t call isotonic; return neutral-ish value\n",
        "    if not np.isfinite(p_raw):\n",
        "        return 0.5\n",
        "\n",
        "    # Optional isotonic calibration (guarded)\n",
        "    iso = globals().get(\"CAL_ISO\", None)\n",
        "    try:\n",
        "        return float(iso.predict([p_raw])[0]) if iso is not None else float(p_raw)\n",
        "    except Exception:\n",
        "        # Any sklearn check_array/NaN issues -> just use p_raw\n",
        "        return float(p_raw)\n",
        "\n",
        "def run_adaptive_v11(env, ctx, alpha=0.8, lag_k=0.6, title=\"Adaptive v1.1\"):\n",
        "    \"\"\"\n",
        "    Forecast-aware router:\n",
        "      - P(up) tilts taker_share around a base\n",
        "      - schedule lag increases aggression (lag_k)\n",
        "      - passive_offset tightens when spread <= 1 tick\n",
        "    \"\"\"\n",
        "    state = env.reset()\n",
        "    seed_books_from_feed(env, feed)\n",
        "    env.arrival0 = env._arrival_mid_global()\n",
        "\n",
        "    metrics = []\n",
        "    df_so_far = pd.DataFrame()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Build calibrated probability (NaN-safe)\n",
        "        p_up = 0.5 if df_so_far.empty else _p_up_from_ctx(ctx, df_so_far)\n",
        "\n",
        "        # schedule lag in [0,1]\n",
        "        expected  = env.total_qty * (env.t + 1) / max(1, env.max_steps)\n",
        "        lag       = expected - (env.total_qty - env.remaining)\n",
        "        lag_ratio = float(np.clip(lag / max(1, 0.15 * env.total_qty), 0.0, 1.0))\n",
        "\n",
        "        base        = 0.25 + lag_k * lag_ratio\n",
        "        tilt        = alpha * (0.5 - p_up)    # ↑taker when bearish, ↓ when bullish\n",
        "        taker_share = float(np.clip(base + tilt, 0.0, 1.0))\n",
        "\n",
        "        # average spread\n",
        "        spreads = []\n",
        "        for sv in env.vl:\n",
        "            bb = sv.venue.book.bids[0].price if sv.venue.book.bids else None\n",
        "            ba = sv.venue.book.asks[0].price if sv.venue.book.asks else None\n",
        "            if bb is not None and ba is not None:\n",
        "                spreads.append(ba - bb)\n",
        "        avg_spread = np.mean(spreads) if spreads else 1\n",
        "        passive_offset = 0 if avg_spread <= 1 else 1\n",
        "\n",
        "        action = np.array([taker_share, passive_offset], dtype=float)\n",
        "        result = env.step(action)\n",
        "\n",
        "        rows = _snapshot_metrics(env)\n",
        "        metrics += rows\n",
        "        df_so_far = (pd.concat([df_so_far, pd.DataFrame(rows)], ignore_index=True)\n",
        "                     if not df_so_far.empty else pd.DataFrame(rows))\n",
        "\n",
        "        state, done = result.state, result.done\n",
        "\n",
        "    df = pd.DataFrame(metrics)\n",
        "    cum_is_bps = (\n",
        "        (((env.cum_spent + env.cum_fee) / max(1, env.cum_filled) - env.arrival0) / env.arrival0) * 1e4\n",
        "        if env.cum_filled else np.nan\n",
        "    )\n",
        "    print(f\"[{title}] Filled {env.cum_filled}/{env.total_qty} | IS vs start (bps): {cum_is_bps:.2f} | last P(up)={p_up:.3f}\")\n",
        "    return df\n",
        "\n",
        "# Back-compat alias\n",
        "def run_adaptive_v1(env, ctx, title=\"Adaptive v1\"):\n",
        "    return run_adaptive_v11(env, ctx, title=title)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eee08a08",
      "metadata": {
        "id": "eee08a08"
      },
      "outputs": [],
      "source": [
        "# --- Export + TCA summary (CSV files + quick attribution printouts + manifest) ---\n",
        "\n",
        "\n",
        "import os, time, json, numpy as np, pandas as pd\n",
        "\n",
        "def export_run(metrics_df: pd.DataFrame,\n",
        "               fills_df: pd.DataFrame | None = None,\n",
        "               route_log: pd.DataFrame | None = None,\n",
        "               preds_df: pd.DataFrame | None = None,\n",
        "               env=None,\n",
        "               tag: str = \"run\",\n",
        "               config: dict | None = None,   # NEW: record costs, sizing, risk, etc.\n",
        "               seed: int | None = None       # NEW: record RNG seed if used\n",
        "               ):\n",
        "    \"\"\"\n",
        "    Writes CSVs to /content/data/exports and prints a small TCA-style summary.\n",
        "      metrics_df : per-step metrics (from run_* functions)\n",
        "      fills_df   : per-step realized fills by venue (from *_logged)\n",
        "      route_log  : intended routing decisions (from *_logged)\n",
        "      preds_df   : OOS predictions table (from fit_and_predict_oos), optional\n",
        "      env        : the env you ran (for cumulative IS calc), optional\n",
        "      tag        : label to distinguish runs\n",
        "      config     : dict of run parameters (fee/slip, sizing, overlays, etc.)\n",
        "      seed       : RNG seed for reproducibility\n",
        "    \"\"\"\n",
        "    os.makedirs(\"/content/data/exports\", exist_ok=True)\n",
        "    ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    base = f\"/content/data/exports/{tag}_{ts}\"\n",
        "\n",
        "    # Save files\n",
        "    metrics_path = f\"{base}_metrics.csv\"; metrics_df.to_csv(metrics_path, index=False)\n",
        "    fills_path   = f\"{base}_fills.csv\"     if fills_df   is not None else None\n",
        "    route_path   = f\"{base}_route_log.csv\" if route_log  is not None else None\n",
        "    preds_path   = f\"{base}_preds.csv\"     if preds_df   is not None else None\n",
        "    if fills_df   is not None: fills_df.to_csv(fills_path, index=False)\n",
        "    if route_log  is not None: route_log.to_csv(route_path, index=False)\n",
        "    if preds_df   is not None: preds_df.to_csv(preds_path, index=False)\n",
        "\n",
        "    # Summary numbers\n",
        "    cum_is_bps = np.nan\n",
        "    if env is not None and getattr(env, \"cum_filled\", 0):\n",
        "        cum_is_bps = (((env.cum_spent + env.cum_fee)/max(1, env.cum_filled) - env.arrival0) / env.arrival0) * 1e4\n",
        "\n",
        "    print(f\"Saved: {metrics_path} (+ optional *_fills, *_route_log, *_preds, *_manifest.json)\")\n",
        "    print(f\"IS vs start (bps): {cum_is_bps:.2f}\")\n",
        "\n",
        "    # Avg spread by venue\n",
        "    if \"venue\" in metrics_df.columns and \"spread\" in metrics_df.columns:\n",
        "        avg_spread = (metrics_df.dropna(subset=[\"spread\"]).groupby(\"venue\")[\"spread\"].mean())\n",
        "        if not avg_spread.empty:\n",
        "            print(\"\\nAvg spread by venue:\")\n",
        "            print(avg_spread)\n",
        "\n",
        "    # Realized fills by venue\n",
        "    if fills_df is not None:\n",
        "        fill_cols = [c for c in fills_df.columns if c.startswith(\"fill_\")]\n",
        "        if fill_cols:\n",
        "            realized = fills_df[fill_cols].sum().rename(lambda c: c.replace(\"fill_\",\"\"))\n",
        "            print(\"\\nRealized fills by venue (shares):\")\n",
        "            print(realized)\n",
        "\n",
        "    # Intended allocation by role × venue\n",
        "    if route_log is not None and not route_log.empty:\n",
        "        alloc = (route_log.groupby([\"role\",\"venue\"])[\"qty\"].sum().unstack(fill_value=0))\n",
        "        print(\"\\nIntended allocation (shares) by role × venue:\")\n",
        "        print(alloc)\n",
        "\n",
        "    # -------- NEW: JSON manifest for reproducibility --------\n",
        "    manifest = {\n",
        "        \"tag\": tag,\n",
        "        \"timestamp\": ts,\n",
        "        \"paths\": {\n",
        "            \"metrics\": metrics_path,\n",
        "            \"fills\": fills_path,\n",
        "            \"route_log\": route_path,\n",
        "            \"preds\": preds_path\n",
        "        },\n",
        "        \"env\": {\n",
        "            \"total_qty\": getattr(env, \"total_qty\", None),\n",
        "            \"max_steps\": getattr(env, \"max_steps\", None),\n",
        "            \"tick_size\": getattr(env, \"tick_size\", None),\n",
        "            \"cum_is_bps\": float(cum_is_bps) if cum_is_bps == cum_is_bps else None\n",
        "        },\n",
        "        \"config\": config or {},  # e.g., {\"fee_bps\":7,\"slippage_bps\":5,\"assumed_spread_bps\":8,\"size_k\":2.0,\"size_cap\":1.0,\"risk\":{\"kill_dd\":0.2,\"max_daily\":-0.03,\"cooldown_days\":2}}\n",
        "        \"seed\": seed\n",
        "    }\n",
        "    with open(f\"{base}_manifest.json\", \"w\") as f:\n",
        "        json.dump(manifest, f, indent=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ede9bfc",
      "metadata": {
        "id": "3ede9bfc"
      },
      "outputs": [],
      "source": [
        "# ---- Example (uncomment to use on a fresh run) ----\n",
        "\n",
        "# df_m, df_f, log = run_adaptive_kept_logged(env_real)\n",
        "# ctx_oos, preds_oos = fit_and_predict_oos(df_m, horizon=5, split=0.7)\n",
        "# export_run(\n",
        "#     df_m, fills_df=df_f, route_log=log, preds_df=preds_oos, env=env_real, tag=\"adaptive_v1\",\n",
        "#     config={\"fee_bps\":7,\"slippage_bps\":5,\"assumed_spread_bps\":8,\"size_k\":2.0,\"size_cap\":1.0,\n",
        "#             \"risk\":{\"kill_dd\":0.20,\"max_daily\":-0.03,\"cooldown_days\":2}},\n",
        "#     seed=42\n",
        "# )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2284a2b5",
      "metadata": {
        "id": "2284a2b5"
      },
      "outputs": [],
      "source": [
        "# --- Benchmark suite: compare TWAP/POV/Aggressive vs Adaptive v1 on the same replay ---\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def _policy_vec(name):\n",
        "    n = (name or \"\").lower()\n",
        "    if n in (\"twap\", \"twap_like\"):      return np.array([0.20, 1], dtype=float)\n",
        "    if n in (\"pov\", \"pov_like\"):        return np.array([0.40, 1], dtype=float)\n",
        "    if n in (\"aggressive\", \"aggr\"):     return np.array([0.80, 0], dtype=float)\n",
        "    return np.array([0.40, 1], dtype=float)\n",
        "\n",
        "def _run_fixed(env, name):\n",
        "    \"\"\"Run a fixed policy with proper seeding + arrival0 set, collect per-step metrics.\"\"\"\n",
        "    pol_vec = _policy_vec(name)\n",
        "    s = env.reset()\n",
        "    seed_books_from_feed(env, feed)\n",
        "    env.arrival0 = env._arrival_mid_global()  # ensure IS reference is valid\n",
        "\n",
        "    rows, done = [], False\n",
        "    while not done:\n",
        "        keep_books_healthy(env, target_spread_ticks=2, min_top_qty=30)\n",
        "        out = env.step(pol_vec)\n",
        "        rows += _snapshot_metrics(env)\n",
        "        keep_books_healthy(env, target_spread_ticks=2, min_top_qty=30)\n",
        "        s, done = out.state, out.done\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    is_bps = (((env.cum_spent + env.cum_fee)/max(1, env.cum_filled) - env.arrival0) / env.arrival0) * 1e4 if env.cum_filled else np.nan\n",
        "    steps  = int(df[\"t\"].max() + 1) if \"t\" in df.columns else len(df)\n",
        "    filled = int(df.groupby(\"t\", as_index=False)[\"cum_filled\"].max()[\"cum_filled\"].iloc[-1]) if \"cum_filled\" in df.columns else env.cum_filled\n",
        "    return df, {\"policy\": name, \"steps\": steps, \"filled\": filled, \"total_qty\": env.total_qty, \"IS_bps\": float(is_bps)}\n",
        "\n",
        "def run_benchmarks(env, ctx=None, include_adaptive_v1=True):\n",
        "    \"\"\"Run TWAP/POV/Aggressive (fixed) and optional Adaptive v1 (forecast-aware).\"\"\"\n",
        "    results = []\n",
        "    runs = {}\n",
        "\n",
        "    # TWAP-like\n",
        "    df_twap, s_twap = _run_fixed(env, \"twap\")\n",
        "    results.append(s_twap); runs[\"twap\"] = df_twap\n",
        "\n",
        "    # POV-like\n",
        "    df_pov,  s_pov  = _run_fixed(env, \"pov\")\n",
        "    results.append(s_pov);  runs[\"pov\"]  = df_pov\n",
        "\n",
        "    # Aggressive\n",
        "    df_aggr, s_aggr = _run_fixed(env, \"aggressive\")\n",
        "    results.append(s_aggr); runs[\"aggressive\"] = df_aggr\n",
        "\n",
        "    # Adaptive v1 (forecast-aware) if we have a trained ctx\n",
        "    if include_adaptive_v1 and ctx is not None:\n",
        "        df_v1 = run_adaptive_v1(env, ctx, title=\"Adaptive v1 (bench)\")  # uses your existing function\n",
        "        steps = int(df_v1[\"t\"].max() + 1) if \"t\" in df_v1.columns else len(df_v1)\n",
        "        filled = int(df_v1.groupby(\"t\", as_index=False)[\"cum_filled\"].max()[\"cum_filled\"].iloc[-1]) if \"cum_filled\" in df_v1.columns else env.cum_filled\n",
        "        is_bps = (((env.cum_spent + env.cum_fee)/max(1, env.cum_filled) - env.arrival0) / env.arrival0) * 1e4 if env.cum_filled else np.nan\n",
        "        results.append({\"policy\": \"adaptive_v1\", \"steps\": steps, \"filled\": filled, \"total_qty\": env.total_qty, \"IS_bps\": float(is_bps)})\n",
        "        runs[\"adaptive_v1\"] = df_v1\n",
        "\n",
        "    summary = pd.DataFrame(results).sort_values([\"IS_bps\"], ascending=True).reset_index(drop=True)\n",
        "    print(summary)\n",
        "    return runs, summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43eb7423",
      "metadata": {
        "id": "43eb7423"
      },
      "outputs": [],
      "source": [
        "# ---- Example usage (assumes you've trained ctx_oos with fit_and_predict_oos) ----\n",
        "\n",
        "# runs, table = run_benchmarks(env_real, ctx=ctx_oos, include_adaptive_v1=True)\n",
        "# quick_plots(runs[\"adaptive_v1\"], \"Adaptive v1 (bench)\")   # or plot any other run\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aac5f007",
      "metadata": {
        "id": "aac5f007",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27bf9cc4-9f15-4778-85e7-4ff52dd7cb16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Streamlit launched in background.\n",
            "Open your dashboard at:\n",
            "   http://localhost:8501   (if running locally)\n",
            "   or the 'External URL' Colab shows above.\n"
          ]
        }
      ],
      "source": [
        "# --- Streamlit Dashboard v0 (headless, exits after printing URL) ---\n",
        "\n",
        "!pip -q install streamlit plotly pandas numpy\n",
        "\n",
        "import os, subprocess, time\n",
        "\n",
        "APP_PATH = \"/content/exec_dashboard.py\"\n",
        "EXPORT_DIR = \"/content/data/exports\"\n",
        "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
        "\n",
        "# Write the dashboard app code\n",
        "app_code = r\"\"\"\n",
        "import os, glob, numpy as np, pandas as pd, streamlit as st, plotly.express as px\n",
        "\n",
        "EXPORT_DIR = os.environ.get(\"EXPORT_DIR\", \"/content/data/exports\")\n",
        "\n",
        "def _safe_read_csv(path):\n",
        "    try:\n",
        "        if os.path.exists(path) and os.path.getsize(path) > 0:\n",
        "            return pd.read_csv(path)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "@st.cache_data\n",
        "def list_runs():\n",
        "    paths = sorted(glob.glob(os.path.join(EXPORT_DIR, \"*_metrics.csv\")))\n",
        "    runs = []\n",
        "    for p in paths:\n",
        "        base = os.path.basename(p)\n",
        "        prefix = p[:-len(\"_metrics.csv\")]\n",
        "        label  = base[:-len(\"_metrics.csv\")]\n",
        "        runs.append({\"label\": label, \"prefix\": prefix})\n",
        "    return runs\n",
        "\n",
        "@st.cache_data\n",
        "def load_run(prefix):\n",
        "    read = lambda suf: _safe_read_csv(f\"{prefix}_{suf}.csv\")\n",
        "    return {\"metrics\": read(\"metrics\"), \"fills\": read(\"fills\"),\n",
        "            \"route\": read(\"route_log\"), \"preds\": read(\"preds\")}\n",
        "\n",
        "st.set_page_config(page_title=\"Exec Dashboard v0\", layout=\"wide\")\n",
        "st.title(\"Execution Dashboard v0 (CSV)\")\n",
        "\n",
        "runs = list_runs()\n",
        "if not runs:\n",
        "    st.warning(f\"No exports found in {EXPORT_DIR}. Expect *_metrics.csv, *_fills.csv, *_route_log.csv, *_preds.csv.\")\n",
        "    st.stop()\n",
        "\n",
        "labels = [r[\"label\"] for r in runs]\n",
        "sel = st.selectbox(\"Select a run\", labels, index=len(labels)-1)\n",
        "prefix = runs[labels.index(sel)][\"prefix\"]\n",
        "data = load_run(prefix)\n",
        "\n",
        "m, f, r, p = data[\"metrics\"], data[\"fills\"], data[\"route\"], data[\"preds\"]\n",
        "\n",
        "st.subheader(\"Summary\")\n",
        "col1, col2, col3, col4 = st.columns(4)\n",
        "def _metric(df, k, fmt=\"{:,.4f}\"):\n",
        "    try:\n",
        "        return fmt.format(df[k].iloc[-1])\n",
        "    except Exception:\n",
        "        return \"—\"\n",
        "\n",
        "if m is not None:\n",
        "    col1.metric(\"PnL\", _metric(m, \"pnl\", \"{:,.2f}\"))\n",
        "    col2.metric(\"Sharpe\", _metric(m, \"sharpe\"))\n",
        "    col3.metric(\"Win Rate\", _metric(m, \"win_rate\", \"{:,.1%}\"))\n",
        "    col4.metric(\"Trades\", _metric(m, \"n_trades\", \"{:,.0f}\"))\n",
        "else:\n",
        "    col1.metric(\"PnL\",\"—\"); col2.metric(\"Sharpe\",\"—\"); col3.metric(\"Win Rate\",\"—\"); col4.metric(\"Trades\",\"—\")\n",
        "\n",
        "tabs = st.tabs([\"Equity Curve\",\"Fills\",\"Routing Log\",\"Preds\"])\n",
        "\n",
        "with tabs[0]:\n",
        "    if m is not None and {\"ts\",\"equity\"} <= set(m.columns):\n",
        "        st.plotly_chart(px.line(m, x=\"ts\", y=\"equity\", title=\"Equity Curve\"), use_container_width=True)\n",
        "        st.dataframe(m.tail(200))\n",
        "\n",
        "with tabs[1]:\n",
        "    if f is not None: st.dataframe(f.tail(200))\n",
        "\n",
        "with tabs[2]:\n",
        "    if r is not None: st.dataframe(r.tail(200))\n",
        "\n",
        "with tabs[3]:\n",
        "    if p is not None: st.dataframe(p.tail(200))\n",
        "\"\"\"\n",
        "\n",
        "with open(APP_PATH, \"w\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "# Launch Streamlit in background (no blocking)\n",
        "env = os.environ.copy()\n",
        "env[\"EXPORT_DIR\"] = EXPORT_DIR\n",
        "proc = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", APP_PATH, \"--server.headless=true\", \"--server.port=8501\", \"--browser.gatherUsageStats=false\"],\n",
        "    stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT, env=env\n",
        ")\n",
        "\n",
        "# Print the URL immediately and exit\n",
        "print(\"✅ Streamlit launched in background.\")\n",
        "print(\"Open your dashboard at:\")\n",
        "print(\"   http://localhost:8501   (if running locally)\")\n",
        "print(\"   or the 'External URL' Colab shows above.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1bc3853",
      "metadata": {
        "id": "f1bc3853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "10295926-ed45-444b-e225-053602c7b8a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Streamlit launched in background.\n",
            "Local URL   : http://localhost:8503\n",
            "Colab URL   : https://8503-m-s-1cfeh7mkjko0t-b.us-west1-1.prod.colab.dev\n"
          ]
        }
      ],
      "source": [
        "# --- Exec Dashboard (robust) — writes Streamlit app, launches in background, returns immediately ---\n",
        "\n",
        "!pip -q install streamlit plotly pandas numpy\n",
        "\n",
        "import os, subprocess, time, sys, textwrap\n",
        "\n",
        "APP_PATH   = \"/content/exec_dashboard.py\"\n",
        "EXPORT_DIR = \"/content/data/exports\"\n",
        "PORT       = 8503\n",
        "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
        "\n",
        "app_code = r\"\"\"\n",
        "import os, glob, numpy as np, pandas as pd, streamlit as st, plotly.express as px\n",
        "\n",
        "EXPORT_DIR = os.environ.get(\"EXPORT_DIR\", \"/content/data/exports\")\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def _safe_read_csv(path):\n",
        "    try:\n",
        "        if os.path.exists(path) and os.path.getsize(path) > 0:\n",
        "            return pd.read_csv(path)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "@st.cache_data\n",
        "def list_runs():\n",
        "    paths = sorted(glob.glob(os.path.join(EXPORT_DIR, \"*_metrics.csv\")))\n",
        "    runs = []\n",
        "    for p in paths:\n",
        "        base = os.path.basename(p)\n",
        "        prefix = p[:-len(\"_metrics.csv\")]\n",
        "        label  = base[:-len(\"_metrics.csv\")]\n",
        "        runs.append({\"label\": label, \"prefix\": prefix})\n",
        "    return runs\n",
        "\n",
        "@st.cache_data\n",
        "def load_run(prefix):\n",
        "    read = lambda suf: _safe_read_csv(f\"{prefix}_{suf}.csv\")\n",
        "    return {\n",
        "        \"metrics\": read(\"metrics\"),\n",
        "        \"fills\":   read(\"fills\"),\n",
        "        \"route\":   read(\"route_log\"),\n",
        "        \"preds\":   read(\"preds\"),\n",
        "    }\n",
        "\n",
        "# ---------- UI ----------\n",
        "st.set_page_config(page_title=\"Exec Dashboard\", layout=\"wide\")\n",
        "st.title(\"Execution Dashboard (CSV)\")\n",
        "\n",
        "runs = list_runs()\n",
        "if not runs:\n",
        "    st.warning(f\"No exports found in {EXPORT_DIR}. Expect *_metrics.csv, *_fills.csv, *_route_log.csv, *_preds.csv.\")\n",
        "    st.stop()\n",
        "\n",
        "labels = [r[\"label\"] for r in runs]\n",
        "sel = st.selectbox(\"Select a run\", labels, index=len(labels)-1)\n",
        "prefix = runs[labels.index(sel)][\"prefix\"]\n",
        "data = load_run(prefix)\n",
        "\n",
        "m = data.get(\"metrics\")\n",
        "f = data.get(\"fills\")\n",
        "r = data.get(\"route\")\n",
        "p = data.get(\"preds\")\n",
        "\n",
        "# Ensure DataFrames (or safe empty frames)\n",
        "m = m if isinstance(m, pd.DataFrame) else pd.DataFrame()\n",
        "f = f if isinstance(f, pd.DataFrame) else pd.DataFrame()\n",
        "r = r if isinstance(r, pd.DataFrame) else pd.DataFrame()\n",
        "p = p if isinstance(p, pd.DataFrame) else pd.DataFrame()\n",
        "\n",
        "# ---- Summary row (defensive) ----\n",
        "cols = st.columns(4)\n",
        "\n",
        "if not m.empty:\n",
        "    if \"t\" in m.columns:\n",
        "        try:\n",
        "            steps = int(np.nanmax(m[\"t\"])) + 1\n",
        "        except Exception:\n",
        "            steps = len(m)\n",
        "    else:\n",
        "        steps = len(m)\n",
        "else:\n",
        "    steps = 0\n",
        "\n",
        "prog_cols = [c for c in [\"cum_filled\",\"remaining\"] if c in m.columns]\n",
        "if prog_cols and \"t\" in m.columns:\n",
        "    prg = m.dropna(subset=prog_cols).groupby(\"t\", as_index=False)[prog_cols].max()\n",
        "else:\n",
        "    prg = pd.DataFrame(columns=[\"t\"] + prog_cols)\n",
        "\n",
        "final_cum = int(prg[\"cum_filled\"].iloc[-1]) if \"cum_filled\" in prg.columns and len(prg)>0 else None\n",
        "final_rem = int(prg[\"remaining\"].iloc[-1])  if \"remaining\"  in prg.columns and len(prg)>0 else None\n",
        "\n",
        "avg_spread = None\n",
        "if {\"venue\",\"spread\"} <= set(m.columns):\n",
        "    try:\n",
        "        avg_spread = m.dropna(subset=[\"spread\"]).groupby(\"venue\")[\"spread\"].mean()\n",
        "    except Exception:\n",
        "        avg_spread = None\n",
        "\n",
        "cols[0].metric(\"Steps\", steps)\n",
        "cols[1].metric(\"Cum filled\", final_cum if final_cum is not None else \"—\")\n",
        "cols[2].metric(\"Remaining\", final_rem if final_rem is not None else \"—\")\n",
        "if avg_spread is not None and len(avg_spread) > 0:\n",
        "    cols[3].metric(\"Avg spread (ticks)\", f\"{avg_spread.mean():.2f}\")\n",
        "else:\n",
        "    cols[3].metric(\"Avg spread (ticks)\", \"—\")\n",
        "\n",
        "st.divider()\n",
        "\n",
        "tab1, tab2, tab3, tab4 = st.tabs([\"Mid & Spread\", \"Fill Progress\", \"Attribution\", \"Predictions\"])\n",
        "\n",
        "with tab1:\n",
        "    if {\"t\",\"mid\",\"venue\"} <= set(m.columns):\n",
        "        ms = m.dropna(subset=[\"mid\"])\n",
        "        st.plotly_chart(px.line(ms, x=\"t\", y=\"mid\", color=\"venue\", title=\"Mid by venue\"), use_container_width=True)\n",
        "    else:\n",
        "        st.info(\"Need columns: t, mid, venue.\")\n",
        "    if {\"t\",\"spread\",\"venue\"} <= set(m.columns):\n",
        "        sp = m.dropna(subset=[\"spread\"])\n",
        "        st.plotly_chart(px.line(sp, x=\"t\", y=\"spread\", color=\"venue\", title=\"Spread by venue\"), use_container_width=True)\n",
        "\n",
        "with tab2:\n",
        "    ycols = [c for c in [\"cum_filled\",\"remaining\"] if c in m.columns]\n",
        "    if ycols and \"t\" in m.columns:\n",
        "        pr = m.groupby(\"t\", as_index=False)[ycols].max()\n",
        "        st.plotly_chart(px.line(pr, x=\"t\", y=ycols, title=\"Fill progress\"), use_container_width=True)\n",
        "    else:\n",
        "        st.info(\"Need columns: t and cum_filled/remaining.\")\n",
        "\n",
        "with tab3:\n",
        "    if not f.empty:\n",
        "        fill_cols = [c for c in f.columns if c.startswith(\"fill_\")]\n",
        "        if fill_cols:\n",
        "            tot = f[fill_cols].sum().rename(lambda c: c.replace(\"fill_\",\"\")).reset_index()\n",
        "            tot.columns = [\"venue\",\"shares\"]\n",
        "            st.plotly_chart(px.bar(tot, x=\"venue\", y=\"shares\", title=\"Realized fills by venue\"), use_container_width=True)\n",
        "    if not r.empty and {\"role\",\"venue\",\"qty\"} <= set(r.columns):\n",
        "        alloc = r.groupby([\"role\",\"venue\"])[\"qty\"].sum().reset_index()\n",
        "        st.plotly_chart(px.bar(alloc, x=\"venue\", y=\"qty\", color=\"role\", barmode=\"group\",\n",
        "                               title=\"Intended allocation (role × venue)\"), use_container_width=True)\n",
        "    st.dataframe(r if not r.empty else pd.DataFrame({\"msg\":[\"No route_log for this run.\"]}))\n",
        "\n",
        "with tab4:\n",
        "    if \"p_up\" in p.columns:\n",
        "        p2 = p.reset_index(drop=True).copy()\n",
        "        # Choose an x-axis\n",
        "        xcol = \"ts\" if \"ts\" in p2.columns else ( \"t\" if \"t\" in p2.columns else p2.index )\n",
        "        st.plotly_chart(px.line(p2, x=xcol, y=\"p_up\", title=\"P(up) time series\"), use_container_width=True)\n",
        "        st.plotly_chart(px.histogram(p2, x=\"p_up\", nbins=20, title=\"P(up) histogram\"), use_container_width=True)\n",
        "        if \"y_true\" in p2.columns:\n",
        "            st.write(\"Preview (tail):\")\n",
        "            st.dataframe(p2.tail(20))\n",
        "    else:\n",
        "        st.info(\"No predictions file or missing p_up column.\")\n",
        "\"\"\"\n",
        "\n",
        "# write app\n",
        "with open(APP_PATH, \"w\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "# kill any old server on the same app path (best-effort)\n",
        "os.system('pkill -f \"streamlit run /content/exec_dashboard.py\" >/dev/null 2>&1')\n",
        "\n",
        "# launch in background (non-blocking)\n",
        "env = os.environ.copy()\n",
        "env[\"EXPORT_DIR\"] = EXPORT_DIR\n",
        "subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", APP_PATH, \"--server.headless=true\",\n",
        "     f\"--server.port={PORT}\", \"--browser.gatherUsageStats=false\",\n",
        "     \"--server.enableCORS=false\", \"--server.enableXsrfProtection=false\",\n",
        "     \"--server.fileWatcherType=none\"],\n",
        "    stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT, env=env\n",
        ")\n",
        "\n",
        "# print URL(s) and finish\n",
        "print(\"✅ Streamlit launched in background.\")\n",
        "print(f\"Local URL   : http://localhost:{PORT}\")\n",
        "# Colab proxy URL if available\n",
        "try:\n",
        "    from google.colab.output import eval_js\n",
        "    proxy = eval_js(f\"google.colab.kernel.proxyPort({PORT})\")\n",
        "    print(\"Colab URL   :\", proxy)\n",
        "except Exception:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c46c16a",
      "metadata": {
        "id": "2c46c16a"
      },
      "outputs": [],
      "source": [
        "# --- Benchmark suite: compare TWAP/POV/Aggressive vs Adaptive v1 on the same replay ---\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Guard this duplicate so we only define _policy_vec once in the notebook\n",
        "if \"_policy_vec\" not in globals():\n",
        "    def _policy_vec(name):\n",
        "        n = (name or \"\").lower()\n",
        "        if n in (\"twap\", \"twap_like\"):      return np.array([0.20, 1], dtype=float)\n",
        "        if n in (\"pov\", \"pov_like\"):        return np.array([0.40, 1], dtype=float)\n",
        "        if n in (\"aggressive\", \"aggr\"):     return np.array([0.80, 0], dtype=float)\n",
        "        return np.array([0.40, 1], dtype=float)\n",
        "\n",
        "def _run_fixed(env, name):\n",
        "    \"\"\"Run a fixed tactic with proper seeding + arrival0 set; collect per-step metrics.\"\"\"\n",
        "    pol_vec = _policy_vec(name)\n",
        "    s = env.reset()\n",
        "    seed_books_from_feed(env, feed)\n",
        "    env.arrival0 = env._arrival_mid_global()  # ensure IS reference is valid\n",
        "\n",
        "    rows, done = [], False\n",
        "    while not done:\n",
        "        # Only keep books healthy if explicitly enabled or if the feed stalls\n",
        "        if 'USE_KEEPER' in globals():\n",
        "            if USE_KEEPER or ('_feed_stalled' in globals() and _feed_stalled(env)):\n",
        "                keep_books_healthy(env, target_spread_ticks=2, min_top_qty=30)\n",
        "\n",
        "        out = env.step(pol_vec)\n",
        "        rows += _snapshot_metrics(env)\n",
        "\n",
        "        if 'USE_KEEPER' in globals():\n",
        "            if USE_KEEPER or ('_feed_stalled' in globals() and _feed_stalled(env)):\n",
        "                keep_books_healthy(env, target_spread_ticks=2, min_top_qty=30)\n",
        "\n",
        "        s, done = out.state, out.done\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    is_bps = (\n",
        "        (((env.cum_spent + env.cum_fee) / max(1, env.cum_filled) - env.arrival0) / env.arrival0) * 1e4\n",
        "        if env.cum_filled else np.nan\n",
        "    )\n",
        "    steps  = int(df[\"t\"].max() + 1) if \"t\" in df.columns else len(df)\n",
        "    filled = int(df.groupby(\"t\", as_index=False)[\"cum_filled\"].max()[\"cum_filled\"].iloc[-1]) \\\n",
        "             if \"cum_filled\" in df.columns and len(df) else env.cum_filled\n",
        "    return df, {\"policy\": name, \"steps\": steps, \"filled\": filled,\n",
        "                \"total_qty\": env.total_qty, \"IS_bps\": float(is_bps)}\n",
        "\n",
        "def run_benchmarks(env, ctx=None, include_adaptive_v1=True):\n",
        "    \"\"\"\n",
        "    Run TWAP/POV/Aggressive (fixed) and optional Adaptive v1 (forecast-aware)\n",
        "    on the same replay tape; return (runs_dict, summary_df).\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    runs = {}\n",
        "\n",
        "    # TWAP-like\n",
        "    df_twap, s_twap = _run_fixed(env, \"twap\")\n",
        "    results.append(s_twap); runs[\"twap\"] = df_twap\n",
        "\n",
        "    # POV-like\n",
        "    df_pov,  s_pov  = _run_fixed(env, \"pov\")\n",
        "    results.append(s_pov);  runs[\"pov\"]  = df_pov\n",
        "\n",
        "    # Aggressive\n",
        "    df_aggr, s_aggr = _run_fixed(env, \"aggressive\")\n",
        "    results.append(s_aggr); runs[\"aggressive\"] = df_aggr\n",
        "\n",
        "    # Adaptive v1 (forecast-aware) if we have a trained ctx and want to include it\n",
        "    if include_adaptive_v1 and ctx is not None:\n",
        "        df_v1 = run_adaptive_v1(env, ctx, title=\"Adaptive v1 (bench)\")\n",
        "        steps = int(df_v1[\"t\"].max() + 1) if \"t\" in df_v1.columns else len(df_v1)\n",
        "        filled = int(df_v1.groupby(\"t\", as_index=False)[\"cum_filled\"].max()[\"cum_filled\"].iloc[-1]) \\\n",
        "                 if \"cum_filled\" in df_v1.columns and len(df_v1) else env.cum_filled\n",
        "        is_bps = (\n",
        "            (((env.cum_spent + env.cum_fee) / max(1, env.cum_filled) - env.arrival0) / env.arrival0) * 1e4\n",
        "            if env.cum_filled else np.nan\n",
        "        )\n",
        "        results.append({\"policy\": \"adaptive_v1\", \"steps\": steps, \"filled\": filled,\n",
        "                        \"total_qty\": env.total_qty, \"IS_bps\": float(is_bps)})\n",
        "        runs[\"adaptive_v1\"] = df_v1\n",
        "\n",
        "    summary = pd.DataFrame(results).sort_values([\"IS_bps\"], ascending=True).reset_index(drop=True)\n",
        "    print(summary)\n",
        "    return runs, summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a53cd28",
      "metadata": {
        "id": "5a53cd28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dd63d10-5442-4268-b6aa-e0d02e3f3b48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4120074851.py:9: DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.multiarray.\n",
            "  add_safe_globals([numpy.core.multiarray._reconstruct])\n"
          ]
        }
      ],
      "source": [
        "# ---- Robust save for impact ctx (keep) + NumPy allowlist ----\n",
        "\n",
        "import numpy as np, torch\n",
        "from torch.serialization import add_safe_globals\n",
        "\n",
        "# Allowlist NumPy reconstruct (safe here because you're loading your own file)\n",
        "try:\n",
        "    import numpy\n",
        "    add_safe_globals([numpy.core.multiarray._reconstruct])\n",
        "except Exception:\n",
        "    pass  # fine if not needed\n",
        "\n",
        "CTX_PATH = \"/content/impact_ctx.pt\"\n",
        "\n",
        "def save_impact_ctx(ctx, path=CTX_PATH):\n",
        "    payload = {\n",
        "        \"state_dict\": ctx[\"model\"].state_dict(),\n",
        "        \"cols\": list(ctx[\"cols\"]),\n",
        "        \"mu\":  np.asarray(ctx[\"mu\"], dtype=float).tolist(),   # save as Python lists\n",
        "        \"sigma\": np.asarray(ctx[\"sigma\"], dtype=float).tolist(),\n",
        "        \"horizon\": int(ctx[\"horizon\"]),\n",
        "        \"version\": 1,\n",
        "    }\n",
        "    torch.save(payload, path)\n",
        "\n",
        "# NOTE: load_impact_ctx is defined once later (robust loader with weights_only=False).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38e753ec",
      "metadata": {
        "id": "38e753ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cce14b2-47ec-420e-d750-6468268e3518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Live ctx ready at: /content/impact_ctx.pt\n",
            " - cols: 17\n",
            " - mu: (17,)\n",
            " - sigma: (17,)\n",
            " - horizon: int\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1704631182.py:18: DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.multiarray.\n",
            "  np.core.multiarray._reconstruct,   # older pickles sometimes reference this\n"
          ]
        }
      ],
      "source": [
        "# ---- Final fix: allowlist NumPy types + robust save/load + sanity print ----\n",
        "import os, numpy as np, torch\n",
        "from torch.serialization import add_safe_globals\n",
        "\n",
        "# Where to save the live context\n",
        "try:\n",
        "    CTX_PATH  # keep user's existing path if already set\n",
        "except NameError:\n",
        "    CTX_PATH = \"/content/ctx_live.pt\"\n",
        "\n",
        "# 1) Allowlist NumPy objects PyTorch's safe loader blocks by default\n",
        "try:\n",
        "    add_safe_globals([\n",
        "        np.ndarray,\n",
        "        np.dtype, np.number, np.ufunc, np.generic,\n",
        "        np.bool_, np.int64, np.float64,\n",
        "        np.complexfloating, np.complex64, np.complex128,\n",
        "        np.core.multiarray._reconstruct,   # older pickles sometimes reference this\n",
        "    ])\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# 2) Define robust save/load helpers (these were missing)\n",
        "def save_impact_ctx(ctx_obj, path):\n",
        "    \"\"\"Safe torch save of your runtime context dict/object.\"\"\"\n",
        "    # if it's a plain dict, save as-is; otherwise rely on torch.save pickling\n",
        "    torch.save(ctx_obj, path)\n",
        "\n",
        "def load_impact_ctx(path):\n",
        "    \"\"\"Safe torch load of your runtime context dict/object.\"\"\"\n",
        "    return torch.load(path, map_location=\"cpu\", weights_only=False)\n",
        "\n",
        "# 3) Optionally (re)save a clean checkpoint if we already have ctx_oos in RAM\n",
        "#    If you DON'T have ctx_oos yet, we just skip saving and try to load the last saved one.\n",
        "if \"ctx_oos\" in globals():\n",
        "    # clean out any old file to avoid partial writes\n",
        "    try:\n",
        "        if os.path.exists(CTX_PATH):\n",
        "            os.remove(CTX_PATH)\n",
        "    except Exception:\n",
        "        pass\n",
        "    save_impact_ctx(ctx_oos, CTX_PATH)\n",
        "\n",
        "# 4) Raw read for sanity (optional)\n",
        "if os.path.exists(CTX_PATH):\n",
        "    _raw = torch.load(CTX_PATH, map_location=\"cpu\", weights_only=False)  # may be dict or object\n",
        "else:\n",
        "    raise FileNotFoundError(f\"No context file at {CTX_PATH}. Generate ctx_oos first and rerun this cell.\")\n",
        "\n",
        "# 5) Canonical load using our loader\n",
        "ctx_live = load_impact_ctx(CTX_PATH)\n",
        "\n",
        "# 6) Friendly summary print (works for dict-like contexts)\n",
        "print(\"Live ctx ready at:\", CTX_PATH)\n",
        "if isinstance(ctx_live, dict):\n",
        "    def _shape_of(key):\n",
        "        x = ctx_live.get(key)\n",
        "        try:\n",
        "            return getattr(x, \"shape\", None) or (len(x) if hasattr(x, \"__len__\") else type(x).__name__)\n",
        "        except Exception:\n",
        "            return type(x).__name__\n",
        "    for k in [\"cols\", \"mu\", \"sigma\", \"horizon\"]:\n",
        "        if k in ctx_live:\n",
        "            print(f\" - {k}: {_shape_of(k)}\")\n",
        "        else:\n",
        "            print(f\" - {k}: (missing)\")\n",
        "else:\n",
        "    # fallback: best-effort introspection\n",
        "    print(\" - type:\", type(ctx_live).__name__)\n",
        "    for attr in [\"cols\",\"mu\",\"sigma\",\"horizon\"]:\n",
        "        if hasattr(ctx_live, attr):\n",
        "            v = getattr(ctx_live, attr)\n",
        "            shp = getattr(v, \"shape\", None)\n",
        "            print(f\" - {attr}: {shp if shp is not None else type(v).__name__}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7a205cf",
      "metadata": {
        "id": "d7a205cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d37d4a4-72d4-48a9-d9a9-89c3511c897a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "robust loader active?  True\n"
          ]
        }
      ],
      "source": [
        "# --- Robust context loader (single source of truth) ---\n",
        "\n",
        "import numpy as np, torch, inspect\n",
        "\n",
        "# Fallback tiny head in case _Logit isn't in scope yet\n",
        "try:\n",
        "    _Logit\n",
        "except NameError:\n",
        "    class _Logit(torch.nn.Module):\n",
        "        def __init__(self, n):\n",
        "            super().__init__()\n",
        "            self.lin = torch.nn.Linear(n, 1)\n",
        "        def forward(self, x):\n",
        "            return self.lin(x).squeeze(-1)\n",
        "\n",
        "def load_impact_ctx(path=CTX_PATH):\n",
        "    \"\"\"\n",
        "    Load a saved impact head context written by save_impact_ctx(...).\n",
        "    Robust to PyTorch 2.6 (weights_only=False) and coerces numpy types.\n",
        "    Returns: {'model','cols','mu','sigma','horizon'}\n",
        "    \"\"\"\n",
        "    # PyTorch 2.6+ needs weights_only=False for custom payloads\n",
        "    data = torch.load(path, map_location=\"cpu\", weights_only=False)\n",
        "\n",
        "    # Basic validation & coercion\n",
        "    cols     = list(data.get(\"cols\", []))\n",
        "    state    = data.get(\"state_dict\", {})\n",
        "    mu       = np.asarray(data.get(\"mu\", []), dtype=float)\n",
        "    sigma    = np.asarray(data.get(\"sigma\", []), dtype=float)\n",
        "    horizon  = int(data.get(\"horizon\", 5))\n",
        "\n",
        "    if not cols or not isinstance(state, dict):\n",
        "        raise ValueError(\"Bad ctx payload: missing 'cols' or 'state_dict'\")\n",
        "\n",
        "    # Rebuild the tiny head and load weights\n",
        "    m = _Logit(len(cols))\n",
        "    m.load_state_dict(state)\n",
        "    m.eval()\n",
        "\n",
        "    return {\"model\": m, \"cols\": cols, \"mu\": mu, \"sigma\": sigma, \"horizon\": horizon}\n",
        "\n",
        "print(\"robust loader active? \", \"weights_only=False\" in inspect.getsource(load_impact_ctx))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a2907c4",
      "metadata": {
        "id": "6a2907c4"
      },
      "outputs": [],
      "source": [
        "# --- Live Predictor v0.9 (BPS-aware, per-symbol normalizer, US-friendly, de-saturated, with stop conditions) ---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94ebac96",
      "metadata": {
        "id": "94ebac96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30bd2c8e-91ce-42fe-dbb2-3ac0efe61172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "robust loader active?  True\n",
            "ctx_live ready | cols: ['ret1_bps', 'ret3_bps', 'ret5_bps', 'ret10_bps', 'ret20_bps', 'rv10_bps', 'imb', 'spr_bps', 'ema20_diff', 'ema50_diff', 'hod_sin', 'hod_cos', 'dow_sin', 'dow_cos', 'ema20_slope', 'ema50_slope', 'ema20_curve'] | horizon: 5\n"
          ]
        }
      ],
      "source": [
        "!pip -q install websockets nest_asyncio\n",
        "\n",
        "\n",
        "import asyncio, json, time, collections, inspect\n",
        "import numpy as np, pandas as pd, torch, websockets, nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Ensure robust loader is active and BPS ctx is loaded\n",
        "print(\"robust loader active? \", \"weights_only=False\" in inspect.getsource(load_impact_ctx))\n",
        "try:\n",
        "    ctx_live\n",
        "except NameError:\n",
        "    ctx_live = load_impact_ctx(CTX_PATH)\n",
        "print(\"ctx_live ready | cols:\", ctx_live[\"cols\"], \"| horizon:\", ctx_live[\"horizon\"])\n",
        "\n",
        "# Symbols to stream (Binance.US does NOT list XMR)\n",
        "ALL_SYMBOLS = (\"BTCUSDT\",\"ETHUSDT\",\"SOLUSDT\",\"BNBUSDT\",\"ADAUSDT\",\"XRPUSDT\",\"DOGEUSDT\")\n",
        "\n",
        "# Endpoints to try in order\n",
        "BINANCE_WS_CANDIDATES = [\n",
        "    \"wss://stream.binance.us:9443/stream\",  # US-friendly\n",
        "    \"wss://stream.binance.vision/stream\",   # public mirror\n",
        "]\n",
        "\n",
        "def _alias_for_us(symbols):\n",
        "    \"\"\"\n",
        "    For each XYZUSDT, also try XYZUSD (Binance.US).\n",
        "    Keeps order and de-duplicates.\n",
        "    \"\"\"\n",
        "    out, seen = [], set()\n",
        "    for s in symbols:\n",
        "        for cand in (s, s.replace(\"USDT\",\"USD\")):\n",
        "            if cand not in seen:\n",
        "                out.append(cand); seen.add(cand)\n",
        "    return tuple(out)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed499536",
      "metadata": {
        "id": "ed499536"
      },
      "outputs": [],
      "source": [
        "# -------- Live features (BPS-aware) — updated to match expanded ctx --------\n",
        "class RollingFeatures:\n",
        "    \"\"\"\n",
        "    Collect per-tick/bookTicker snapshots and emit a single-row feature vector\n",
        "    matching ctx['cols'] whenever enough bars are available.\n",
        "\n",
        "    Computes:\n",
        "      ret{1,3,5,10,20}_bps, rv10_bps, spr_bps, imb,\n",
        "      ema20_diff/ema50_diff, ema20_slope/ema50_slope/ema20_curve,\n",
        "      hod_sin/hod_cos, dow_sin/dow_cos\n",
        "    \"\"\"\n",
        "    def __init__(self, maxlen=256):\n",
        "        from collections import deque\n",
        "        self.buf = deque(maxlen=maxlen)\n",
        "\n",
        "    def push(self, mid, spr, bsz, asz, t_ms):\n",
        "        self.buf.append({\"mid\": float(mid), \"spr\": float(spr),\n",
        "                         \"bsz\": float(bsz), \"asz\": float(asz), \"t\": float(t_ms)})\n",
        "\n",
        "    def make_row(self, ctx_cols):\n",
        "        if len(self.buf) < 6:\n",
        "            return None\n",
        "        import pandas as pd, numpy as np\n",
        "        df  = pd.DataFrame(self.buf)\n",
        "        mid = df[\"mid\"].astype(float)\n",
        "        spr = df[\"spr\"].astype(float)\n",
        "        bsz = df[\"bsz\"].astype(float)\n",
        "        asz = df[\"asz\"].astype(float)\n",
        "\n",
        "        feats = {}\n",
        "        def rbps(n): return (mid.pct_change(n).iloc[-1] * 1e4)\n",
        "        for n in (1,3,5,10,20):\n",
        "            feats[f\"ret{n}_bps\"] = float(np.clip(rbps(n), -5000, 5000)) if len(mid) > n else np.nan\n",
        "\n",
        "        if len(mid) > 11:\n",
        "            ret1 = mid.pct_change(1) * 1e4\n",
        "            feats[\"rv10_bps\"] = float(np.clip(ret1.rolling(10).std().iloc[-1] * np.sqrt(10), 0, 500))\n",
        "        else:\n",
        "            feats[\"rv10_bps\"] = np.nan\n",
        "\n",
        "        feats[\"spr_bps\"] = float(np.clip((spr.iloc[-1] / (mid.iloc[-1] + 1e-9)) * 1e4, -50, 50))\n",
        "        denom = (bsz.iloc[-1] + asz.iloc[-1] + 1e-9)\n",
        "        feats[\"imb\"] = float(np.clip((bsz.iloc[-1] - asz.iloc[-1]) / denom, -0.98, 0.98))\n",
        "\n",
        "        ema20 = mid.ewm(span=20, adjust=False).mean()\n",
        "        ema50 = mid.ewm(span=50, adjust=False).mean()\n",
        "        feats[\"ema20_diff\"]  = float(np.clip(((mid.iloc[-1] - ema20.iloc[-1]) / (ema20.iloc[-1] + 1e-9)) * 1e4, -500, 500)) if len(mid) >= 20 else np.nan\n",
        "        feats[\"ema50_diff\"]  = float(np.clip(((mid.iloc[-1] - ema50.iloc[-1]) / (ema50.iloc[-1] + 1e-9)) * 1e4, -500, 500)) if len(mid) >= 50 else np.nan\n",
        "        feats[\"ema20_slope\"] = float(np.clip(ema20.pct_change(1).iloc[-1] * 1e4, -500, 500)) if len(mid) >= 21 else np.nan\n",
        "        feats[\"ema50_slope\"] = float(np.clip(ema50.pct_change(1).iloc[-1] * 1e4, -500, 500)) if len(mid) >= 51 else np.nan\n",
        "        feats[\"ema20_curve\"] = float(np.clip(ema20.pct_change(2).iloc[-1] * 1e4, -800, 800)) if len(mid) >= 22 else np.nan\n",
        "\n",
        "        ts = pd.to_datetime(df[\"t\"], unit=\"ms\", errors=\"coerce\")\n",
        "        hod = getattr(ts, \"hour\", pd.Series([0]*len(ts))).astype(float).iloc[-1]\n",
        "        dow = getattr(ts, \"dayofweek\", pd.Series([0]*len(ts))).astype(float).iloc[-1]\n",
        "        feats[\"hod_sin\"] = float(np.sin(2*np.pi*hod/24.0))\n",
        "        feats[\"hod_cos\"] = float(np.cos(2*np.pi*hod/24.0))\n",
        "        feats[\"dow_sin\"] = float(np.sin(2*np.pi*dow/7.0))\n",
        "        feats[\"dow_cos\"] = float(np.cos(2*np.pi*dow/7.0))\n",
        "\n",
        "        # Build row in exact ctx order; if any requested feature is NaN, keep collecting\n",
        "        row = []\n",
        "        for c in ctx_cols:\n",
        "            if c in feats and feats[c] == feats[c]:\n",
        "                row.append(feats[c])\n",
        "            elif c in (\"spr\",\"spr_bps\") and feats[\"spr_bps\"] == feats[\"spr_bps\"]:\n",
        "                row.append(feats[\"spr_bps\"])\n",
        "            else:\n",
        "                return None\n",
        "        return np.array([row], dtype=float)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d72fdfc",
      "metadata": {
        "id": "0d72fdfc"
      },
      "outputs": [],
      "source": [
        "# -------- Adaptive normalizer + feature guards --------\n",
        "\n",
        "class LiveNormalizer:\n",
        "    def __init__(self, train_mu, train_sigma, alpha=0.3):  # lean more to live stats\n",
        "        self.train_mu  = np.asarray(train_mu, dtype=float)\n",
        "        self.train_sig = np.asarray(train_sigma, dtype=float)\n",
        "        self.alpha     = float(alpha)\n",
        "        self.count     = 0\n",
        "        self.live_mu   = np.zeros_like(self.train_mu)\n",
        "        self.live_m2   = np.zeros_like(self.train_mu)\n",
        "    def update(self, x):\n",
        "        x = x.astype(float).ravel()\n",
        "        self.count += 1\n",
        "        d = x - self.live_mu\n",
        "        self.live_mu += d / self.count\n",
        "        self.live_m2 += d * (x - self.live_mu)\n",
        "    def blended(self):\n",
        "        if self.count >= 2:\n",
        "            var = np.maximum(self.live_m2 / (self.count - 1), 1e-9)\n",
        "            sig = np.sqrt(var)\n",
        "            mu  = self.alpha * self.train_mu + (1 - self.alpha) * self.live_mu\n",
        "            sig = self.alpha * self.train_sig + (1 - self.alpha) * sig\n",
        "        else:\n",
        "            mu, sig = self.train_mu, self.train_sig\n",
        "        return mu, sig\n",
        "\n",
        "def _preprocess_features(x_raw, cols, spr_weight=0.15,\n",
        "                         caps={\"ret1_bps\":50, \"ret3_bps\":100, \"ret5_bps\":150, \"spr_bps\":20, \"imb\":0.98}):\n",
        "    \"\"\"Clip extreme BPS features and down-weight spread influence before z-scoring.\"\"\"\n",
        "    x = x_raw.copy().astype(float)\n",
        "    col2idx = {c:i for i,c in enumerate(cols)}\n",
        "    for c in [\"ret1_bps\",\"ret3_bps\",\"ret5_bps\"]:\n",
        "        if c in col2idx:\n",
        "            i = col2idx[c]; cap = caps[c]; x[0,i] = np.clip(x[0,i], -cap, cap)\n",
        "    if \"spr_bps\" in col2idx:\n",
        "        i = col2idx[\"spr_bps\"]; x[0,i] = np.clip(x[0,i], -caps[\"spr_bps\"], caps[\"spr_bps\"]) * spr_weight\n",
        "    if \"imb\" in col2idx:\n",
        "        i = col2idx[\"imb\"]; cap = caps[\"imb\"]; x[0,i] = np.clip(x[0,i], -cap, cap)\n",
        "    return x\n",
        "\n",
        "async def live_predict(ctx, symbols=ALL_SYMBOLS, interval_ms=700, warn_idle_s=10,\n",
        "                       z_clip=1.2, temperature=4.2, alpha=0.25, spr_weight=0.15,\n",
        "                       stop_after_s=None, max_prints=None):\n",
        "    cols, mu_tr, sig_tr, model = ctx[\"cols\"], ctx[\"mu\"], ctx[\"sigma\"], ctx[\"model\"]\n",
        "\n",
        "    # base symbols (filter XMR on .us)\n",
        "    base_syms = [s.lower() for s in symbols if not s.lower().startswith(\"xmrusdt\")]\n",
        "    if not base_syms:\n",
        "        print(\"[error] No symbols left to stream after filtering.\"); return\n",
        "\n",
        "    # per-symbol state\n",
        "    R      = {s: RollingFeatures() for s in base_syms}\n",
        "    last_p = {s: 0.0 for s in base_syms}\n",
        "    seen   = {s: 0.0 for s in base_syms}\n",
        "    norms  = {s: LiveNormalizer(mu_tr, sig_tr, alpha=alpha) for s in base_syms}\n",
        "\n",
        "    prints_done = 0\n",
        "    start_time  = time.time()\n",
        "\n",
        "    while True:\n",
        "        for base in BINANCE_WS_CANDIDATES:\n",
        "            # expand to USD pairs for binance.us; otherwise use as-is\n",
        "            if \"binance.us\" in base:\n",
        "                syms = [s.lower() for s in _alias_for_us([s.upper() for s in base_syms])]\n",
        "            else:\n",
        "                syms = base_syms[:]\n",
        "\n",
        "            # ensure state for any newly added aliases\n",
        "            for s in syms:\n",
        "                if s not in R:      R[s] = RollingFeatures()\n",
        "                if s not in last_p: last_p[s] = 0.0\n",
        "                if s not in seen:   seen[s] = 0.0\n",
        "                if s not in norms:  norms[s] = LiveNormalizer(mu_tr, sig_tr, alpha=alpha)\n",
        "\n",
        "            streams = \"/\".join([f\"{s}@bookTicker\" for s in syms])\n",
        "            url = f\"{base}?streams={streams}\"\n",
        "            try:\n",
        "                print(\"Connecting:\", url)\n",
        "                async with websockets.connect(url, ping_interval=20) as ws:\n",
        "                    while True:\n",
        "                        # --- stop conditions (time-based) ---\n",
        "                        if stop_after_s is not None and (time.time() - start_time) >= stop_after_s:\n",
        "                            print(f\"[done] Stopping after {stop_after_s}s.\")\n",
        "                            return\n",
        "\n",
        "                        msg = json.loads(await ws.recv())\n",
        "                        d = msg.get(\"data\", {})\n",
        "                        sym = d.get(\"s\", \"\").lower()\n",
        "                        if sym not in R:\n",
        "                            continue\n",
        "\n",
        "                        bb, ba  = float(d[\"b\"]), float(d[\"a\"])\n",
        "                        bsz, asz = float(d[\"B\"]), float(d[\"A\"])\n",
        "                        mid, spr = 0.5*(bb+ba), (ba - bb)\n",
        "                        now_ms   = time.time()*1000\n",
        "                        seen[sym] = now_ms\n",
        "\n",
        "                        rf = R[sym]; rf.push(mid, spr, bsz, asz, now_ms)\n",
        "                        x = rf.make_row(cols)\n",
        "                        if x is None: continue\n",
        "\n",
        "                        # guard & adapt (per symbol)\n",
        "                        xg = _preprocess_features(x, cols, spr_weight=spr_weight)\n",
        "                        n  = norms[sym]\n",
        "                        n.update(xg[0])\n",
        "                        mu_bl, sig_bl = n.blended()\n",
        "                        xz = (xg - mu_bl) / (sig_bl + 1e-9)\n",
        "                        if z_clip is not None:\n",
        "                            xz = np.clip(xz, -z_clip, z_clip)\n",
        "\n",
        "                        # --------- calibrated probability (TEMP_CAL + CAL_ISO) ----------\n",
        "                        with torch.no_grad():\n",
        "                            logit = model(torch.tensor(xz, dtype=torch.float32))\n",
        "                        # prefer globally-set TEMP_CAL/CAL_ISO; fall back to args/defaults\n",
        "                        T   = globals().get(\"TEMP_CAL\", temperature)\n",
        "                        iso = globals().get(\"CAL_ISO\", None)\n",
        "                        p_raw = torch.sigmoid(logit / float(T)).item()\n",
        "                        p_up  = float(iso.predict([p_raw])[0]) if iso is not None else p_raw\n",
        "                        # -----------------------------------------------------------------\n",
        "\n",
        "                        if now_ms - last_p[sym] >= interval_ms:\n",
        "                            last_p[sym] = now_ms\n",
        "                            if any(c.endswith(\"_bps\") for c in cols):\n",
        "                                spr_bps = (spr / (mid + 1e-9)) * 1e4\n",
        "                                print(f\"{sym.upper():>8}  P(up)={p_up:0.3f}  spr_bps={spr_bps:7.2f}  mid={mid:.2f}\")\n",
        "                            else:\n",
        "                                print(f\"{sym.upper():>8}  P(up)={p_up:0.3f}  spr={spr:.6f}  mid={mid:.2f}\")\n",
        "\n",
        "                            prints_done += 1\n",
        "                            # --- stop conditions (count-based) ---\n",
        "                            if max_prints is not None and prints_done >= max_prints:\n",
        "                                print(f\"[done] Stopping after {prints_done} prints.\")\n",
        "                                return\n",
        "\n",
        "                        # idle warning\n",
        "                        for s in syms:\n",
        "                            if seen[s] and (now_ms - seen[s] > warn_idle_s*1000):\n",
        "                                print(f\"[warn] No data for {s.upper()} in {warn_idle_s}s on {base}.\")\n",
        "                                seen[s] = now_ms\n",
        "            except Exception as e:\n",
        "                print(\"[reconnect on]\", base, \"|\", type(e).__name__, str(e))\n",
        "                await asyncio.sleep(1.5); continue\n",
        "        await asyncio.sleep(2.0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Run it (examples) ---\n",
        "\n",
        "import asyncio\n",
        "\n",
        "# Helper: wrap live_predict with warm-up delay (no extra kwargs passed into live_predict)\n",
        "async def run_live_with_warmup(ctx_live,\n",
        "                               symbols=(\"BTCUSDT\",\"ETHUSDT\"),\n",
        "                               alpha=0.25,\n",
        "                               z_clip=1.2,\n",
        "                               temperature=4.2,\n",
        "                               spr_weight=0.15,\n",
        "                               stop_after_s=60,\n",
        "                               max_prints=None,\n",
        "                               warmup_steps=20,     # require ~20 bars; with STEP_MS=60000 that's ~20 minutes\n",
        "                               step_ms=60000):      # keep in sync with your training STEP_MS\n",
        "    \"\"\"\n",
        "    Sleeps until enough bars could be collected, then calls live_predict().\n",
        "    This avoids 'missing feature' logs when STEP_MS is large.\n",
        "    \"\"\"\n",
        "    warmup_secs = int(warmup_steps * (step_ms / 1000))\n",
        "    if warmup_secs > 0:\n",
        "        print(f\"[warmup] waiting for {warmup_steps} steps (~{warmup_secs//60} minutes at {step_ms} ms)...\")\n",
        "        await asyncio.sleep(warmup_secs)\n",
        "\n",
        "    # Now run live_predict normally (no warmup_steps kwarg!)\n",
        "    await live_predict(\n",
        "        ctx_live,\n",
        "        symbols=symbols,\n",
        "        alpha=alpha,\n",
        "        z_clip=z_clip,\n",
        "        temperature=temperature,\n",
        "        spr_weight=spr_weight,\n",
        "        stop_after_s=stop_after_s,\n",
        "        max_prints=max_prints\n",
        "    )\n",
        "    print(\"[done] live_predict finished cleanly.\")\n",
        "\n",
        "\n",
        "# Example 1: stop after ~60 seconds (after warmup finishes)\n",
        "await run_live_with_warmup(\n",
        "    ctx_live,\n",
        "    symbols=(\"BTCUSDT\",\"ETHUSDT\",\"SOLUSDT\",\"BNBUSDT\",\"ADAUSDT\"),\n",
        "    alpha=0.25,\n",
        "    z_clip=1.2,\n",
        "    temperature=4.2,\n",
        "    spr_weight=0.15,\n",
        "    stop_after_s=60,   # auto-stop after 60s of live prints\n",
        "    max_prints=200,\n",
        "    warmup_steps=20,   # ~20 minutes at 60s bars\n",
        "    step_ms=5000\n",
        ")\n",
        "\n",
        "# Example 2: stop after ~200 prints instead of time (uncomment to use)\n",
        "# await run_live_with_warmup(\n",
        "#     ctx_live,\n",
        "#     symbols=(\"BTCUSDT\",\"ETHUSDT\"),\n",
        "#     alpha=0.25,\n",
        "#     z_clip=1.2,\n",
        "#     temperature=4.2,\n",
        "#     spr_weight=0.15,\n",
        "#     stop_after_s=None,\n",
        "#     max_prints=200,\n",
        "#     warmup_steps=20,\n",
        "#     step_ms=60000\n",
        "# )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLWMR9-6e5MV",
        "outputId": "4b5653e6-0b89-4d9d-c646-db705c565b3f"
      },
      "id": "sLWMR9-6e5MV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[warmup] waiting for 20 steps (~1 minutes at 5000 ms)...\n",
            "Connecting: wss://stream.binance.us:9443/stream?streams=btcusdt@bookTicker/btcusd@bookTicker/ethusdt@bookTicker/ethusd@bookTicker/solusdt@bookTicker/solusd@bookTicker/bnbusdt@bookTicker/bnbusd@bookTicker/adausdt@bookTicker/adausd@bookTicker\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.15  mid=111462.73\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.14  mid=111462.68\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.14  mid=111462.69\n",
            " BTCUSDT  P(up)=0.472  spr_bps=  22.14  mid=111462.68\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.13  mid=111462.69\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.13  mid=111462.68\n",
            " BTCUSDT  P(up)=0.472  spr_bps=  22.13  mid=111462.68\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.13  mid=111462.70\n",
            " ETHUSDT  P(up)=0.500  spr_bps=  13.65  mid=4316.09\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.13  mid=111462.69\n",
            " ETHUSDT  P(up)=0.492  spr_bps=  13.53  mid=4316.06\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.12  mid=111462.70\n",
            " ETHUSDT  P(up)=0.485  spr_bps=  13.48  mid=4316.06\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.12  mid=111462.71\n",
            " ETHUSDT  P(up)=0.471  spr_bps=  13.42  mid=4316.06\n",
            " SOLUSDT  P(up)=0.507  spr_bps=  11.06  mid=216.97\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.12  mid=111462.70\n",
            " ETHUSDT  P(up)=0.472  spr_bps=  13.39  mid=4316.05\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.12  mid=111462.68\n",
            " SOLUSDT  P(up)=0.490  spr_bps=  13.36  mid=217.06\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.11  mid=111462.68\n",
            " SOLUSDT  P(up)=0.540  spr_bps=  15.67  mid=217.03\n",
            "[warn] No data for BNBUSD in 10s on wss://stream.binance.us:9443/stream.\n",
            " ETHUSDT  P(up)=0.468  spr_bps=  13.37  mid=4316.05\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.11  mid=111462.69\n",
            " ETHUSDT  P(up)=0.457  spr_bps=  13.23  mid=4316.07\n",
            " ETHUSDT  P(up)=0.452  spr_bps=  13.16  mid=4316.08\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.11  mid=111462.70\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.11  mid=111462.70\n",
            " SOLUSDT  P(up)=0.525  spr_bps=  15.67  mid=217.03\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.11  mid=111462.70\n",
            " ETHUSDT  P(up)=0.450  spr_bps=  13.14  mid=4316.09\n",
            " SOLUSDT  P(up)=0.518  spr_bps=  14.28  mid=217.04\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.11  mid=111462.70\n",
            " SOLUSDT  P(up)=0.522  spr_bps=  13.82  mid=217.05\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.10  mid=111462.68\n",
            " ETHUSDT  P(up)=0.451  spr_bps=  13.04  mid=4316.09\n",
            " BTCUSDT  P(up)=0.472  spr_bps=  22.10  mid=111462.67\n",
            " ETHUSDT  P(up)=0.447  spr_bps=  12.81  mid=4316.14\n",
            " BTCUSDT  P(up)=0.485  spr_bps=  22.10  mid=111462.65\n",
            " SOLUSDT  P(up)=0.724  spr_bps=   9.21  mid=217.10\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.09  mid=111462.66\n",
            " ETHUSDT  P(up)=0.446  spr_bps=  12.77  mid=4316.15\n",
            " SOLUSDT  P(up)=0.762  spr_bps=   8.29  mid=217.11\n",
            " ETHUSDT  P(up)=0.434  spr_bps=  13.00  mid=4316.24\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.09  mid=111462.67\n",
            " SOLUSDT  P(up)=0.718  spr_bps=   8.29  mid=217.12\n",
            " ETHUSDT  P(up)=0.440  spr_bps=  12.93  mid=4316.25\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.09  mid=111462.68\n",
            " SOLUSDT  P(up)=0.751  spr_bps=   0.92  mid=217.20\n",
            " ETHUSDT  P(up)=0.428  spr_bps=  13.23  mid=4316.33\n",
            " BTCUSDT  P(up)=0.485  spr_bps=  22.08  mid=111462.70\n",
            " SOLUSDT  P(up)=0.757  spr_bps=   2.30  mid=217.22\n",
            " BTCUSDT  P(up)=0.471  spr_bps=  22.08  mid=111462.70\n",
            " ETHUSDT  P(up)=0.439  spr_bps=  13.51  mid=4316.40\n",
            " BTCUSDT  P(up)=0.492  spr_bps=  22.07  mid=111462.70\n",
            " SOLUSDT  P(up)=0.725  spr_bps=   4.14  mid=217.25\n",
            " ETHUSDT  P(up)=0.435  spr_bps=  13.44  mid=4316.40\n",
            " ETHUSDT  P(up)=0.435  spr_bps=  13.41  mid=4316.41\n",
            " BTCUSDT  P(up)=0.471  spr_bps=  22.07  mid=111462.73\n",
            " BTCUSDT  P(up)=0.492  spr_bps=  22.06  mid=111462.73\n",
            " ADAUSDT  P(up)=0.430  spr_bps=  15.01  mid=0.87\n",
            " ETHUSDT  P(up)=0.439  spr_bps=  13.39  mid=4316.41\n",
            " SOLUSDT  P(up)=0.726  spr_bps=   4.14  mid=217.25\n",
            "[warn] No data for BNBUSD in 10s on wss://stream.binance.us:9443/stream.\n",
            " BTCUSDT  P(up)=0.485  spr_bps=  22.06  mid=111462.73\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.05  mid=111462.71\n",
            " SOLUSDT  P(up)=0.724  spr_bps=   4.14  mid=217.25\n",
            " ETHUSDT  P(up)=0.442  spr_bps=  13.32  mid=4316.43\n",
            " ADAUSDT  P(up)=0.526  spr_bps=  20.78  mid=0.87\n",
            " BTCUSDT  P(up)=0.471  spr_bps=  22.05  mid=111462.73\n",
            " BTCUSDT  P(up)=0.472  spr_bps=  22.05  mid=111462.71\n",
            " ADAUSDT  P(up)=0.516  spr_bps=  21.93  mid=0.87\n",
            " ETHUSDT  P(up)=0.447  spr_bps=  13.25  mid=4316.44\n",
            " BTCUSDT  P(up)=0.487  spr_bps=  22.05  mid=111462.71\n",
            " ADAUSDT  P(up)=0.510  spr_bps=  16.16  mid=0.87\n",
            " ETHUSDT  P(up)=0.447  spr_bps=  13.21  mid=4316.45\n",
            " ETHUSDT  P(up)=0.452  spr_bps=  13.25  mid=4316.44\n",
            " SOLUSDT  P(up)=0.756  spr_bps=   0.46  mid=217.30\n",
            " BTCUSDT  P(up)=0.493  spr_bps=  22.04  mid=111462.71\n",
            " ADAUSDT  P(up)=0.596  spr_bps=  15.01  mid=0.87\n",
            " ETHUSDT  P(up)=0.451  spr_bps=  13.16  mid=4316.46\n",
            "[warn] No data for ETHUSD in 10s on wss://stream.binance.us:9443/stream.\n",
            " SOLUSDT  P(up)=0.757  spr_bps=   0.46  mid=217.30\n",
            "[done] Stopping after 60s.\n",
            "[done] live_predict finished cleanly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e246837",
      "metadata": {
        "id": "4e246837"
      },
      "outputs": [],
      "source": [
        "# --- Weekly Signals v2 (all-in-one) ---\n",
        "\n",
        "# Persistence (multi-window), 1h EMA trend filter, ATR-top10% vol check,\n",
        "# per-coin ctx support, max-hold timestamp, stronger confidence floor,\n",
        "# robust USDT/USD aliasing for Binance.US. Now uses TEMP_CAL + CAL_ISO and\n",
        "# can attach a quick per-coin PnL summary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95c74e06",
      "metadata": {
        "id": "95c74e06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01f99a74-39a0-4d03-bc9e-d0be973eda39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weekly Signals using TEMP_CAL= (none)  CAL_ISO= (none)\n"
          ]
        }
      ],
      "source": [
        "!pip -q install aiohttp websockets nest_asyncio\n",
        "\n",
        "\n",
        "import asyncio, aiohttp, json, time, os\n",
        "import numpy as np, pandas as pd, torch, websockets, nest_asyncio\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from collections import deque\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Ensure a BPS ctx is loaded (soft guard)\n",
        "try:\n",
        "    ctx_live\n",
        "except NameError:\n",
        "    ctx_live = load_impact_ctx(CTX_PATH)\n",
        "\n",
        "if \"cols\" not in ctx_live or not all(c in ctx_live[\"cols\"] for c in ['ret1_bps','ret3_bps','ret5_bps','imb','spr_bps']):\n",
        "    raise AssertionError(\"Load BPS ctx first (ctx_live).\")\n",
        "\n",
        "print(\"Weekly Signals using TEMP_CAL=\",\n",
        "      globals().get(\"TEMP_CAL\", \"(none)\"),\n",
        "      \" CAL_ISO=\",\n",
        "      \"set\" if globals().get(\"CAL_ISO\", None) is not None else \"(none)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a37efe03",
      "metadata": {
        "id": "a37efe03"
      },
      "outputs": [],
      "source": [
        "#Endpoints\n",
        "\n",
        "REST_CANDIDATES = [\n",
        "    \"https://api.binance.us/api/v3/klines\",\n",
        "    \"https://api.binance.vision/api/v3/klines\",\n",
        "]\n",
        "WS_CANDIDATES = [\n",
        "    \"wss://stream.binance.us:9443/stream\",\n",
        "    \"wss://stream.binance.vision/stream\",\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5dc88a9",
      "metadata": {
        "id": "a5dc88a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c503457d-9def-441d-f99c-2c1fdc9d849a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weekly plan (selected):\n",
            "(no qualified coins this run)\n",
            "Saved selection to: /content/data/exports/weekly_signals_20250909-233606.csv\n",
            "Full diagnostics saved to: /content/data/exports/weekly_signals_diag_20250909-233606.csv\n"
          ]
        }
      ],
      "source": [
        "#Weekly signals\n",
        "import asyncio, json, time, os\n",
        "from collections import deque\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import aiohttp, websockets\n",
        "\n",
        "WS_CANDIDATES   = globals().get(\"WS_CANDIDATES\",\n",
        "    (\"wss://stream.binance.us:9443/stream\", \"wss://stream.binance.vision/stream\"))\n",
        "REST_CANDIDATES = globals().get(\"REST_CANDIDATES\",\n",
        "    (\"https://api.binance.us/api/v3/klines\", \"https://data-api.binance.vision/api/v3/klines\"))\n",
        "\n",
        "\n",
        "def aliases_for_us(sym: str) -> tuple[str, ...]:\n",
        "    \"\"\"From 'BTCUSDT' -> ('BTCUSDT','BTCUSD') in order (deduped).\"\"\"\n",
        "    s = sym.upper(); out, seen = [], set()\n",
        "    for cand in (s, s.replace(\"USDT\",\"USD\")):\n",
        "        if cand not in seen: out.append(cand); seen.add(cand)\n",
        "    return tuple(out)\n",
        "\n",
        "class RollingFeatures:\n",
        "    \"\"\"Build BPS features matching training.\"\"\"\n",
        "    def __init__(self, maxlen=128): self.buf = deque(maxlen=maxlen)\n",
        "    def push(self, mid, spr, bsz, asz, t_ms): self.buf.append({\"mid\":mid,\"spr\":spr,\"bsz\":bsz,\"asz\":asz,\"t\":t_ms})\n",
        "    def make_row(self, cols):\n",
        "        if len(self.buf) < 6: return None\n",
        "        df = pd.DataFrame(self.buf)\n",
        "        ret1_bps = df[\"mid\"].pct_change(1).iloc[-1] * 1e4\n",
        "        ret3_bps = df[\"mid\"].pct_change(3).iloc[-1] * 1e4\n",
        "        ret5_bps = df[\"mid\"].pct_change(5).iloc[-1] * 1e4\n",
        "        spr_bps  = (df[\"spr\"].iloc[-1] / (df[\"mid\"].iloc[-1] + 1e-9)) * 1e4\n",
        "        imb      = (df[\"bsz\"].iloc[-1] - df[\"asz\"].iloc[-1]) / (df[\"bsz\"].iloc[-1] + df[\"asz\"].iloc[-1] + 1e-9)\n",
        "        row = {\"ret1_bps\":ret1_bps,\"ret3_bps\":ret3_bps,\"ret5_bps\":ret5_bps,\"imb\":imb,\"spr_bps\":spr_bps}\n",
        "        return np.array([[row.get(c, 0.0) for c in cols]], dtype=float)\n",
        "\n",
        "def true_range(h,l,pc): return np.maximum(h-l, np.maximum(np.abs(h-pc), np.abs(l-pc)))\n",
        "\n",
        "def atr_wilder(high, low, close, period=14):\n",
        "    h,l,c = map(np.asarray, (high,low,close))\n",
        "    pc = np.roll(c,1); pc[0]=c[0]\n",
        "    tr = true_range(h,l,pc)\n",
        "    atr = np.empty_like(tr); atr[:period] = np.nan\n",
        "    atr[period-1] = np.nanmean(tr[:period])\n",
        "    for i in range(period, len(tr)):\n",
        "        atr[i] = (atr[i-1]*(period-1) + tr[i]) / period\n",
        "    return atr\n",
        "\n",
        "def ema(series, span):\n",
        "    s = pd.Series(series)\n",
        "    return s.ewm(span=span, adjust=False).mean().to_numpy()\n",
        "\n",
        "async def fetch_klines(session, base_url, symbol, interval=\"1h\", limit=1000, end_ms=None):\n",
        "    params = {\"symbol\":symbol, \"interval\":interval, \"limit\":str(limit)}\n",
        "    if end_ms is not None: params[\"endTime\"] = str(int(end_ms))\n",
        "    async with session.get(base_url, params=params, timeout=aiohttp.ClientTimeout(total=20)) as r:\n",
        "        if r.status != 200: raise RuntimeError(f\"{symbol} {base_url} HTTP {r.status}\")\n",
        "        data = await r.json()\n",
        "        df = pd.DataFrame(data, columns=[\n",
        "            \"t_open\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"t_close\",\"qav\",\"num_trades\",\"taker_base\",\"taker_quote\",\"ignore\"\n",
        "        ])\n",
        "        for col in (\"open\",\"high\",\"low\",\"close\"): df[col] = df[col].astype(float)\n",
        "        return df[[\"t_open\",\"open\",\"high\",\"low\",\"close\",\"t_close\"]]\n",
        "\n",
        "async def fetch_klines_multi(session, symbol, interval=\"1h\", total_limit=2000):\n",
        "    \"\"\"Up to ~2000 recent bars (2 pages) from US then vision.\"\"\"\n",
        "    for base in REST_CANDIDATES:\n",
        "        try:\n",
        "            df1 = await fetch_klines(session, base, symbol, interval=interval, limit=min(1000,total_limit))\n",
        "            if len(df1)==0: continue\n",
        "            end_ms = int(df1[\"t_open\"].iloc[0]) - 1\n",
        "            need = max(0, total_limit - len(df1))\n",
        "            df2 = await fetch_klines(session, base, symbol, interval=interval, limit=min(1000,need), end_ms=end_ms) if need>0 else pd.DataFrame(columns=df1.columns)\n",
        "            return pd.concat([df2, df1], ignore_index=True)\n",
        "        except Exception:\n",
        "            continue\n",
        "    raise RuntimeError(f\"klines failed for {symbol}\")\n",
        "\n",
        "\n",
        "async def _sample_once(ctx_for_base, alias_list, seconds=20, interval_ms=300, recv_timeout=5.0):\n",
        "    \"\"\"One short window of live P(up) sampling (aggregated across aliases).\"\"\"\n",
        "    cols, mu, sig, model = ctx_for_base[\"cols\"], ctx_for_base[\"mu\"], ctx_for_base[\"sigma\"], ctx_for_base[\"model\"]\n",
        "    rf = {al: RollingFeatures() for al in alias_list}\n",
        "    last = {al: 0.0 for al in alias_list}\n",
        "    t0 = time.time(); acc = []\n",
        "    for ws_base in WS_CANDIDATES:\n",
        "        try:\n",
        "            url = f\"{ws_base}?streams=\" + \"/\".join([f\"{al}@bookTicker\" for al in alias_list])\n",
        "            async with websockets.connect(url, ping_interval=20) as ws:\n",
        "                while time.time() - t0 < seconds:\n",
        "                    try:\n",
        "                        raw = await asyncio.wait_for(ws.recv(), timeout=recv_timeout)\n",
        "                    except asyncio.TimeoutError:\n",
        "                        continue\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                    try:\n",
        "                        msg = json.loads(raw)\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                    d = msg.get(\"data\", {})\n",
        "                    al = d.get(\"s\", \"\").lower()\n",
        "                    if al not in rf: continue\n",
        "                    bb,ba = float(d[\"b\"]), float(d[\"a\"])\n",
        "                    bsz,asz = float(d[\"B\"]), float(d[\"A\"])\n",
        "                    mid,spr = 0.5*(bb+ba), (ba-bb)\n",
        "                    now_ms = time.time()*1000\n",
        "                    rf[al].push(mid, spr, bsz, asz, now_ms)\n",
        "                    x = rf[al].make_row(cols)\n",
        "                    if x is None: continue\n",
        "                    xz = (x - mu) / (sig + 1e-9); xz = np.clip(xz, -3.0, 3.0)\n",
        "                    with torch.no_grad():\n",
        "                        logit = float(model(torch.tensor(xz, dtype=torch.float32)).item())\n",
        "                    T   = float(globals().get(\"TEMP_CAL\", 1.0))\n",
        "                    iso = globals().get(\"CAL_ISO\", None)\n",
        "                    p_raw = 1.0 / (1.0 + np.exp(-(logit / T)))\n",
        "                    p = float(iso.predict([p_raw])[0]) if iso is not None else float(p_raw)\n",
        "                    if now_ms - last[al] >= interval_ms and p == p:  # not NaN\n",
        "                        last[al] = now_ms\n",
        "                        acc.append(p)\n",
        "                return float(np.nanmean(acc)) if acc else np.nan\n",
        "        except Exception:\n",
        "            continue\n",
        "    return np.nan\n",
        "\n",
        "\n",
        "async def sample_pup_persistent(ctx_default, base_syms, ctx_by_symbol=None, windows=3, window_sec=20, gap_sec=20):\n",
        "    \"\"\"For each base coin: sample P(up) over multiple windows; require ≥2/3 votes (runs symbols concurrently).\"\"\"\n",
        "    base_syms = [b.upper() for b in base_syms]\n",
        "\n",
        "    async def _per_symbol(b):\n",
        "        aliases = [al.lower() for al in aliases_for_us(b+\"USDT\")]\n",
        "        ctx_b = ctx_by_symbol.get(b, ctx_default) if ctx_by_symbol else ctx_default\n",
        "        p_list = []\n",
        "        for w in range(windows):\n",
        "            p = await _sample_once(ctx_b, aliases, seconds=window_sec)\n",
        "            p_list.append(p)\n",
        "            if w < windows-1:\n",
        "                await asyncio.sleep(gap_sec)\n",
        "        votes = sum(1 if (p>=0.5) else -1 for p in p_list if p==p)\n",
        "        p_med = float(np.nanmedian([p for p in p_list if p==p])) if any(p==p for p in p_list) else np.nan\n",
        "        return b, {\"p_list\":p_list, \"p_med\":p_med, \"votes\":votes,\n",
        "                   \"bias_persistent\":\"LONG\" if votes >= 1 else \"SHORT\"}\n",
        "\n",
        "    pairs = await asyncio.gather(*[_per_symbol(b) for b in base_syms])\n",
        "    return {b: info for b, info in pairs}\n",
        "\n",
        "\n",
        "async def fetch_atr_trend_for_symbol(session, b, ema_span=20, ema_alt_span=50, vol_top_pct=90, total_limit=2000, sem=None):\n",
        "    \"\"\"Try USDT/USD aliases; return (alias_used, last, atr, ema20_last, ema50_last, p90, df) or None if fail/skip.\"\"\"\n",
        "    async def _do(alias):\n",
        "        df = await fetch_klines_multi(session, alias, interval=\"1h\", total_limit=total_limit)\n",
        "        closes = df[\"close\"].to_numpy()\n",
        "        highs  = df[\"high\"].to_numpy()\n",
        "        lows   = df[\"low\"].to_numpy()\n",
        "        ema20 = ema(closes, ema_span); ema50 = ema(closes, ema_alt_span)\n",
        "        ema20_last = float(ema20[-1]); ema50_last = float(ema50[-1])\n",
        "        atr_series = atr_wilder(highs, lows, closes, period=14)\n",
        "        atr_vals = atr_series[~np.isnan(atr_series)]\n",
        "        if len(atr_vals) < 20:\n",
        "            return None\n",
        "        atr_today = float(atr_vals[-1]); last = float(closes[-1])\n",
        "        p90 = float(np.nanpercentile(atr_vals, vol_top_pct))\n",
        "        return alias, last, atr_today, ema20_last, ema50_last, p90, df\n",
        "\n",
        "    async with (sem or asyncio.Semaphore(100)):\n",
        "        for alias in aliases_for_us(b+\"USDT\"):\n",
        "            try:\n",
        "                res = await _do(alias)\n",
        "                if res is not None:\n",
        "                    alias_used, last, atr, ema20_last, ema50_last, p90, df = res\n",
        "                    if atr >= p90:\n",
        "                        return {\"reason\":\"atr_extreme\", \"alias_used\":alias_used, \"ATR_1h\":atr, \"ATR_P90\":p90}\n",
        "                    return {\"alias_used\":alias_used, \"last\":last, \"atr\":atr,\n",
        "                            \"ema20_last\":ema20_last, \"ema50_last\":ema50_last, \"p90\":p90, \"df\":df}\n",
        "            except Exception:\n",
        "                continue\n",
        "    return None\n",
        "\n",
        "#Weekly builder\n",
        "\n",
        "async def build_weekly_plan_v2(\n",
        "    ctx_default,\n",
        "    base_symbols=(\"BTCUSDT\",\"ETHUSDT\",\"SOLUSDT\",\"BNBUSDT\",\"ADAUSDT\"),\n",
        "    ctx_by_symbol=None,\n",
        "    windows=3, window_sec=20, gap_sec=20,\n",
        "    min_conf=0.12,\n",
        "    ema_span=20, ema_alt_span=50,\n",
        "    atr_mult=1.5, rr=2.0,\n",
        "    max_hold_days=2,\n",
        "    atr_lookback_days=90,\n",
        "    vol_top_pct=90,\n",
        "    meta_clf=None, meta_thresh=0.60,\n",
        "    rest_concurrency=4\n",
        "):\n",
        "    base_symbols = tuple(map(str.upper, base_symbols))\n",
        "\n",
        "    pers = await sample_pup_persistent(ctx_default, base_symbols, ctx_by_symbol, windows, window_sec, gap_sec)\n",
        "\n",
        "    rows = []\n",
        "    sem = asyncio.Semaphore(rest_concurrency)\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        async def _atr_task(b):\n",
        "            info = pers[b]; p_med = info[\"p_med\"]; votes = info[\"votes\"]\n",
        "            if not (p_med==p_med):\n",
        "                return b, {\"reason\":\"no_pup\"}\n",
        "\n",
        "            conf = abs(p_med - 0.5) * 2.0\n",
        "            if conf < min_conf:\n",
        "                return b, {\"reason\":\"low_conf\", \"p_up\":round(p_med,3), \"confidence\":round(conf,3), \"votes\":votes}\n",
        "\n",
        "            atr_pack = await fetch_atr_trend_for_symbol(session, b, ema_span, ema_alt_span, vol_top_pct, sem=sem)\n",
        "            if atr_pack is None:\n",
        "                return b, {\"reason\":\"atr_fail\", \"p_up\":round(p_med,3), \"confidence\":round(conf,3), \"votes\":votes}\n",
        "\n",
        "            if atr_pack.get(\"reason\") == \"atr_extreme\":\n",
        "                return b, {\"reason\":\"atr_extreme\", \"p_up\":round(p_med,3), \"confidence\":round(conf,3), \"votes\":votes,\n",
        "                           \"alias_used\":atr_pack[\"alias_used\"], \"ATR_1h\":round(atr_pack[\"ATR_1h\"],6),\n",
        "                           \"ATR_P90\":round(atr_pack[\"ATR_P90\"],6)}\n",
        "\n",
        "            return b, {\"reason\":\"ok\", \"p_med\":p_med, \"conf\":conf, \"votes\":votes, **atr_pack}\n",
        "\n",
        "        atr_pairs = await asyncio.gather(*[_atr_task(b) for b in base_symbols])\n",
        "        atr_map = {b: pack for b, pack in atr_pairs}\n",
        "\n",
        "        for b in base_symbols:\n",
        "            pack = atr_map[b]\n",
        "            if pack[\"reason\"] != \"ok\":\n",
        "                row = {\"symbol\":b, **{k:v for k,v in pack.items() if k!=\"reason\"}}\n",
        "                row[\"reason\"] = pack[\"reason\"]\n",
        "                rows.append(row)\n",
        "                continue\n",
        "\n",
        "            alias_used = pack[\"alias_used\"]; last = pack[\"last\"]; atr = pack[\"atr\"]\n",
        "            ema20_last = pack[\"ema20_last\"]; ema50_last = pack[\"ema50_last\"]; p90 = pack[\"p90\"]; df = pack[\"df\"]\n",
        "            p_med = pack[\"p_med\"]; conf = pack[\"conf\"]; votes = pack[\"votes\"]\n",
        "\n",
        "            bias = \"LONG\" if p_med >= 0.5 else \"SHORT\"\n",
        "            trend_ok = ((bias==\"LONG\" and last > ema20_last) or (bias==\"SHORT\" and last < ema20_last))\n",
        "            if not trend_ok:\n",
        "                rows.append({\"symbol\":b, \"p_up\":round(p_med,3), \"confidence\":round(conf,3), \"votes\":votes,\n",
        "                             \"reason\":\"trend_conflict\", \"alias_used\":alias_used,\n",
        "                             \"last\":round(last,6), \"EMA20_1h\":round(ema20_last,6)})\n",
        "                continue\n",
        "\n",
        "            if meta_clf is not None:\n",
        "                feat = np.array([[p_med, conf, (last-ema20_last)/(last+1e-9)*1e4, atr/(p90+1e-9)]], dtype=float)\n",
        "                try:\n",
        "                    meta_p = float(meta_clf.predict_proba(feat)[:,1])\n",
        "                except Exception:\n",
        "                    meta_p = float(meta_clf.predict(feat)) if hasattr(meta_clf, \"predict\") else 0.0\n",
        "                if meta_p < meta_thresh:\n",
        "                    rows.append({\"symbol\":b, \"p_up\":round(p_med,3), \"confidence\":round(conf,3),\n",
        "                                 \"reason\":\"meta_reject\", \"meta_p\":round(meta_p,3),\n",
        "                                 \"alias_used\":alias_used, \"last\":round(last,6), \"EMA20_1h\":round(ema20_last,6),\n",
        "                                 \"votes\":votes})\n",
        "                    continue\n",
        "\n",
        "            added_with_pnl = False\n",
        "            try:\n",
        "                if \"backtest_bps\" in globals():\n",
        "                    bars = pd.DataFrame({\"close\": df[\"close\"].to_numpy()})\n",
        "                    bt = backtest_bps(\n",
        "                        bars, ctx_default,\n",
        "                        T=float(globals().get(\"TEMP_CAL\", 1.0)),\n",
        "                        iso=globals().get(\"CAL_ISO\", None),\n",
        "                        enter=0.60, exit=0.52,\n",
        "                        fee_bps=7, slippage_bps=5, hold_max=48\n",
        "                    )\n",
        "                    rows.append({\n",
        "                        \"symbol\": b, \"alias_used\": alias_used,\n",
        "                        \"p_up\": round(p_med,3), \"confidence\": round(conf,3),\n",
        "                        \"bias\": bias, \"votes\": votes, \"last\": round(last,6),\n",
        "                        \"ATR_1h\": round(atr,6),\n",
        "                        \"SL\": round(last - atr*1.5, 6) if bias==\"LONG\" else round(last + atr*1.5, 6),\n",
        "                        \"TP\": round(last + rr*1.5*atr, 6) if bias==\"LONG\" else round(last - rr*1.5*atr, 6),\n",
        "                        \"RR\": rr, \"ATR_mult\": 1.5,\n",
        "                        \"EMA20_1h\": round(ema20_last,6), \"EMA50_1h\": round(ema50_last,6),\n",
        "                        \"max_hold_days\": 2,\n",
        "                        \"exit_by_utc\": (datetime.now(timezone.utc) + timedelta(days=2)).isoformat(timespec=\"seconds\"),\n",
        "                        \"reason\": \"ok\",\n",
        "                        \"AnnRet\": bt[\"metrics\"][\"AnnReturn\"],\n",
        "                        \"Sharpe\": bt[\"metrics\"][\"Sharpe\"],\n",
        "                        \"MaxDD\": bt[\"metrics\"][\"MaxDD\"],\n",
        "                        \"Calmar\": bt[\"metrics\"][\"Calmar\"]\n",
        "                    })\n",
        "                    added_with_pnl = True\n",
        "            except Exception:\n",
        "                added_with_pnl = False\n",
        "\n",
        "            if not added_with_pnl:\n",
        "                sl = last - atr*1.5 if bias==\"LONG\" else last + atr*1.5\n",
        "                tp = last + rr*1.5*atr if bias==\"LONG\" else last - rr*1.5*atr\n",
        "                exit_by = datetime.now(timezone.utc) + timedelta(days=2)\n",
        "                rows.append({\n",
        "                    \"symbol\": b, \"alias_used\": alias_used,\n",
        "                    \"p_up\": round(p_med,3), \"confidence\": round(conf,3),\n",
        "                    \"bias\": bias, \"votes\": votes, \"last\": round(last,6),\n",
        "                    \"ATR_1h\": round(atr,6), \"SL\": round(sl,6), \"TP\": round(tp,6),\n",
        "                    \"RR\": rr, \"ATR_mult\": 1.5,\n",
        "                    \"EMA20_1h\": round(ema20_last,6), \"EMA50_1h\": round(ema50_last,6),\n",
        "                    \"max_hold_days\": 2, \"exit_by_utc\": exit_by.isoformat(timespec=\"seconds\"),\n",
        "                    \"reason\": \"ok\"\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    got = df[df[\"reason\"]==\"ok\"].copy()\n",
        "    if not got.empty:\n",
        "        got = got.sort_values(\"confidence\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    os.makedirs(\"/content/data/exports\", exist_ok=True)\n",
        "    stamp = datetime.now(timezone.utc).strftime(\"%Y%m%d-%H%M%S\")\n",
        "    path_sel  = f\"/content/data/exports/weekly_signals_{stamp}.csv\"\n",
        "    path_diag = f\"/content/data/exports/weekly_signals_diag_{stamp}.csv\"\n",
        "    got.to_csv(path_sel, index=False)\n",
        "    df.to_csv(path_diag, index=False)\n",
        "    return got, path_sel, df, path_diag\n",
        "\n",
        "BASES = (\"BTCUSDT\",\"ETHUSDT\",\"SOLUSDT\",\"BNBUSDT\",\"ADAUSDT\")\n",
        "ctx_by_symbol = None\n",
        "\n",
        "df_plan, path_sel, df_diag, path_diag = await build_weekly_plan_v2(\n",
        "    ctx_default=ctx_live,\n",
        "    base_symbols=(\"BTCUSDT\",\"ETHUSDT\",\"SOLUSDT\",\"BNBUSDT\",\"ADAUSDT\"),\n",
        "    ctx_by_symbol=None,\n",
        "    windows=4, window_sec=40, gap_sec=15,\n",
        "    min_conf=0.05,\n",
        "    ema_span=50, ema_alt_span=50,\n",
        "    vol_top_pct=97,\n",
        "    atr_mult=1.5, rr=2.0, max_hold_days=2,\n",
        "    meta_clf=None, meta_thresh=0.60,\n",
        "    rest_concurrency=4\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "print(\"Weekly plan (selected):\")\n",
        "print(df_plan if not df_plan.empty else \"(no qualified coins this run)\")\n",
        "print(\"Saved selection to:\", path_sel)\n",
        "print(\"Full diagnostics saved to:\", path_diag)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b281c09",
      "metadata": {
        "id": "3b281c09"
      },
      "outputs": [],
      "source": [
        "#Meta-labeling\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "def triple_barrier_labels(df_1h, atr_mult=1.5, rr=2.0, max_bars=48):\n",
        "    \"\"\"\n",
        "    df_1h: DataFrame with columns ['open','high','low','close']\n",
        "    For each bar i, set upper=close_i + rr*atr_mult*ATR, lower=close_i - atr_mult*ATR,\n",
        "    label 1 if upper touched first within max_bars, else 0 if lower first or timeout.\n",
        "    \"\"\"\n",
        "    closes = df_1h[\"close\"].to_numpy()\n",
        "    highs  = df_1h[\"high\"].to_numpy()\n",
        "    lows   = df_1h[\"low\"].to_numpy()\n",
        "    atr    = atr_wilder(highs, lows, closes, period=14)\n",
        "    labels = np.full(len(closes), np.nan)\n",
        "\n",
        "    for i in range(len(closes)-1):\n",
        "        if np.isnan(atr[i]): continue\n",
        "        up = closes[i] + rr*atr_mult*atr[i]\n",
        "        dn = closes[i] - atr_mult*atr[i]\n",
        "        end = min(len(closes)-1, i+max_bars)\n",
        "        # earliest touch\n",
        "        hit_up = np.where(highs[i+1:end+1] >= up)[0]\n",
        "        hit_dn = np.where(lows[i+1:end+1]  <= dn)[0]\n",
        "        t_up = hit_up[0] if len(hit_up) else np.inf\n",
        "        t_dn = hit_dn[0] if len(hit_dn) else np.inf\n",
        "        labels[i] = 1.0 if t_up < t_dn else 0.0 if t_dn < np.inf else 0.0\n",
        "    return labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "902322cc",
      "metadata": {
        "id": "902322cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68dc53c0-b878-4ef0-9834-384b2bf9b283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weekly plan (selected):\n",
            "  symbol alias_used   p_up  confidence   bias  votes         last      ATR_1h  \\\n",
            "0    BTC    BTCUSDT  0.008       0.983  SHORT     -3  111578.7000  494.900893   \n",
            "1    ADA    ADAUSDT  0.008       0.983  SHORT     -2       0.8629    0.006678   \n",
            "2    XRP    XRPUSDT  0.009       0.981  SHORT     -3       2.9448    0.018740   \n",
            "\n",
            "              SL             TP   RR  ATR_mult       EMA20_1h       EMA50_1h  \\\n",
            "0  112321.051339  110093.997322  2.0       1.5  111743.644300  111730.944855   \n",
            "1       0.872917       0.842867  2.0       1.5       0.867121       0.860171   \n",
            "2       2.972911       2.888579  2.0       1.5       2.965213       2.946090   \n",
            "\n",
            "   max_hold_days                exit_by_utc reason  \n",
            "0            2.0  2025-09-11T23:26:54+00:00     ok  \n",
            "1            2.0  2025-09-11T23:26:54+00:00     ok  \n",
            "2            2.0  2025-09-11T23:26:54+00:00     ok  \n",
            "Saved selection to: /content/data/exports/weekly_signals_20250909-232654.csv\n",
            "Full diagnostics saved to: /content/data/exports/weekly_signals_diag_20250909-232654.csv\n"
          ]
        }
      ],
      "source": [
        "#Run Weekly Signals v2\n",
        "\n",
        "\n",
        "BASES = (\"BTC\",\"ETH\",\"SOL\",\"BNB\",\"ADA\",\"XRP\",\"DOGE\")\n",
        "\n",
        "# If you have per-coin contexts, pass them here (else leave as None):\n",
        "# ctx_by_symbol = {\"ETH\": ctx_eth, \"SOL\": ctx_sol, ...}\n",
        "ctx_by_symbol = None\n",
        "\n",
        "df_plan, path_sel, df_diag, path_diag = await build_weekly_plan_v2(\n",
        "    ctx_default=ctx_live,\n",
        "    base_symbols=BASES,\n",
        "    ctx_by_symbol=ctx_by_symbol,\n",
        "    # persistence: 3 windows of 20s each with 20s gaps (~1.3 min total)\n",
        "    windows=3, window_sec=20, gap_sec=20,\n",
        "    # stronger confidence floor\n",
        "    min_conf=0.12,\n",
        "    # trend filter: EMA20 on 1h (EMA50 also computed and available)\n",
        "    ema_span=20, ema_alt_span=50,\n",
        "    # bracket sizing\n",
        "    atr_mult=1.5, rr=2.0,\n",
        "    # time exit annotation\n",
        "    max_hold_days=2,\n",
        "    # vol sanity: skip if today's ATR in top 10% of last ~90 days\n",
        "    atr_lookback_days=90, vol_top_pct=90\n",
        ")\n",
        "\n",
        "print(\"Weekly plan (selected):\")\n",
        "print(df_plan if not df_plan.empty else \"(no qualified coins this run)\")\n",
        "print(\"Saved selection to:\", path_sel)\n",
        "print(\"Full diagnostics saved to:\", path_diag)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "print(inspect.signature(run_adaptive_kept_logged))"
      ],
      "metadata": {
        "id": "RhYJypQH13RM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a93e9a1c-786d-452a-d83b-692b6aa0c5c4"
      },
      "id": "RhYJypQH13RM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(env, title='Adaptive v0 + keeper + log')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_metrics, df_fills_counts, route_log, df_fills_true, prefix = run_adaptive_kept_logged_v3(\n",
        "    env_real, parent_side=\"BUY\"\n",
        ")"
      ],
      "metadata": {
        "id": "6tCRramlxVkq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1784dbea-c71b-456d-d3f8-5334ba0b2e90"
      },
      "id": "6tCRramlxVkq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Export prefix: /content/data/exports/run_20250909-225856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Backtest Summary\n",
        "import os, glob, math, numpy as np, pandas as pd\n",
        "from datetime import timedelta\n",
        "\n",
        "EXPORT_DIRS = [\"/content/data/exports\", \"/content/data\"]\n",
        "\n",
        "def _latest_metrics_path():\n",
        "    cands = []\n",
        "    for d in EXPORT_DIRS:\n",
        "        cands += glob.glob(os.path.join(d, \"*_metrics.csv\"))\n",
        "    return max(cands, key=os.path.getmtime) if cands else None\n",
        "\n",
        "def _latest_sibling(path, suffix):\n",
        "    if not path: return None\n",
        "    p = path.replace(\"_metrics.csv\", f\"_{suffix}.csv\")\n",
        "    return p if os.path.exists(p) else None\n",
        "\n",
        "def _infer_step_seconds(df):\n",
        "    if \"ts_ms\" in df.columns:\n",
        "        dt = np.diff(df[\"ts_ms\"].astype(\"int64\").values)\n",
        "        return float(np.nanmedian(dt))/1000.0 if len(dt) else None\n",
        "    if \"ts\" in df.columns:\n",
        "        dt = np.diff(pd.to_numeric(df[\"ts\"], errors=\"coerce\").values)\n",
        "        return float(np.nanmedian(dt)) if len(dt) else None\n",
        "    if \"t\" in df.columns and \"STEP_MS\" in globals():\n",
        "        return float(globals()[\"STEP_MS\"])/1000.0\n",
        "    return None\n",
        "\n",
        "def _equity_and_returns(df):\n",
        "    if \"ret\" in df.columns:\n",
        "        r = pd.to_numeric(df[\"ret\"], errors=\"coerce\").fillna(0.0).values\n",
        "        eq = np.cumprod(1.0 + r)\n",
        "        return eq, r\n",
        "    for col in (\"equity\",\"Equity\",\"eq\"):\n",
        "        if col in df.columns:\n",
        "            eq = pd.to_numeric(df[col], errors=\"coerce\").ffill().values\n",
        "            r = np.insert(np.diff(eq)/eq[:-1], 0, 0.0)\n",
        "            return eq, r\n",
        "    if \"pnl\" in df.columns:\n",
        "        pnl = pd.to_numeric(df[\"pnl\"], errors=\"coerce\").fillna(0.0).values\n",
        "        eq = 10_000.0 + pnl\n",
        "        r = np.insert(np.diff(eq)/eq[:-1], 0, 0.0)\n",
        "        return eq, r\n",
        "    return None, None\n",
        "\n",
        "def _sharpe(r, step_seconds, rf_annual=0.0):\n",
        "    if r is None or len(r) < 2: return np.nan\n",
        "    sec_per_year = 365*24*3600\n",
        "    ann_fac = sec_per_year / max(step_seconds, 1.0) if step_seconds else 252.0\n",
        "    ex = r - (rf_annual/ann_fac)\n",
        "    mu, sd = np.nanmean(ex), np.nanstd(ex, ddof=1)\n",
        "    return (mu/sd)*math.sqrt(ann_fac) if (sd and np.isfinite(sd)) else np.nan\n",
        "\n",
        "def _max_drawdown(equity):\n",
        "    if equity is None or len(equity) == 0: return np.nan\n",
        "    peak = -np.inf; max_dd = 0.0\n",
        "    for x in equity:\n",
        "        peak = max(peak, x)\n",
        "        if peak > 0:\n",
        "            max_dd = max(max_dd, (peak - x)/peak)\n",
        "    return max_dd\n",
        "\n",
        "def _cagr_from_equity(eq, step_seconds):\n",
        "    if eq is None or len(eq) < 2 or not np.isfinite(eq[-1]) or not np.isfinite(eq[0]): return np.nan\n",
        "    sec_per_year = 365*24*3600\n",
        "    years = (len(eq)-1) * (step_seconds or (sec_per_year/252.0)) / sec_per_year\n",
        "    return (eq[-1]/max(eq[0],1e-12))**(1/years) - 1.0 if years > 0 else np.nan\n",
        "\n",
        "if \"df_metrics\" in globals() and isinstance(df_metrics, pd.DataFrame) and len(df_metrics) > 0:\n",
        "    dfm, mpath = df_metrics.copy(), None\n",
        "else:\n",
        "    mpath = _latest_metrics_path()\n",
        "    if not mpath:\n",
        "        raise RuntimeError(\"No metrics found. Provide df_metrics or export '*_metrics.csv'.\")\n",
        "    dfm = pd.read_csv(mpath)\n",
        "\n",
        "fills_path = _latest_sibling(mpath, \"fills\")\n",
        "fills = pd.read_csv(fills_path) if fills_path else pd.DataFrame()\n",
        "\n",
        "step_s = _infer_step_seconds(dfm)\n",
        "\n",
        "eq, rets = _equity_and_returns(dfm)\n",
        "\n",
        "if eq is not None:\n",
        "    total_ret = (eq[-1]/eq[0] - 1.0) if len(eq) > 1 else np.nan\n",
        "    sharpe   = _sharpe(rets, step_s)\n",
        "    mdd      = _max_drawdown(eq)\n",
        "    cagr     = _cagr_from_equity(eq, step_s)\n",
        "    calmar   = (cagr/mdd) if (mdd and np.isfinite(cagr) and mdd>0) else np.nan\n",
        "\n",
        "    n_trades = win_rate = avg_trade_pnl = np.nan\n",
        "    if not fills.empty:\n",
        "        pnl_cols = [c for c in fills.columns if \"pnl\" in c.lower()]\n",
        "        if pnl_cols:\n",
        "            trade_pnl = pd.to_numeric(fills[pnl_cols[0]], errors=\"coerce\")\n",
        "            n_trades = int(trade_pnl.shape[0])\n",
        "            win_rate = float((trade_pnl > 0).mean()) if n_trades > 0 else np.nan\n",
        "            avg_trade_pnl = float(trade_pnl.mean())\n",
        "\n",
        "    if \"ts_ms\" in dfm.columns:\n",
        "        dur_sec = (dfm[\"ts_ms\"].iloc[-1] - dfm[\"ts_ms\"].iloc[0]) / 1000.0\n",
        "    elif \"ts\" in dfm.columns:\n",
        "        dur_sec = float(dfm[\"ts\"].iloc[-1] - dfm[\"ts\"].iloc[0])\n",
        "    elif \"t\" in dfm.columns and step_s:\n",
        "        dur_sec = (int(dfm[\"t\"].iloc[-1]) - int(dfm[\"t\"].iloc[0])) * step_s\n",
        "    else:\n",
        "        dur_sec = (len(dfm)-1) * (step_s or (365*24*3600/252.0))\n",
        "\n",
        "    summary = pd.DataFrame({\n",
        "        \"Sharpe\":[round(sharpe,3) if np.isfinite(sharpe) else np.nan],\n",
        "        \"Total Return\":[round(total_ret,4)],\n",
        "        \"CAGR\":[round(cagr,4) if np.isfinite(cagr) else np.nan],\n",
        "        \"Max Drawdown\":[round(mdd,4) if np.isfinite(mdd) else np.nan],\n",
        "        \"Calmar\":[round(calmar,3) if np.isfinite(calmar) else np.nan],\n",
        "        \"Steps\":[len(dfm)],\n",
        "        \"Backtest Length\":[str(timedelta(seconds=float(max(dur_sec,0))))],\n",
        "        \"Trades\":[n_trades if np.isfinite(n_trades) else np.nan],\n",
        "        \"Win Rate\":[round(win_rate,4) if np.isfinite(win_rate) else np.nan],\n",
        "        \"Avg Trade PnL\":[round(avg_trade_pnl,4) if np.isfinite(avg_trade_pnl) else np.nan],\n",
        "        \"Metrics Source\":[mpath if mpath else \"(RAM: df_metrics)\"],\n",
        "    })\n",
        "else:\n",
        "    cum = pd.to_numeric(dfm.get(\"cum_filled\", pd.Series(dtype=float)), errors=\"coerce\")\n",
        "    rem = pd.to_numeric(dfm.get(\"remaining\",  pd.Series(dtype=float)), errors=\"coerce\")\n",
        "    n_fills = int(((cum - cum.shift(1)).clip(lower=0) > 0).sum()) if not cum.empty else np.nan\n",
        "    total_qty = (cum.iloc[-1] + rem.iloc[-1]) if (len(cum)>0 and len(rem)>0) else np.nan\n",
        "    complete  = bool(len(rem)>0 and rem.iloc[-1] == 0)\n",
        "\n",
        "    t_to_done = np.nan\n",
        "    if \"t\" in dfm.columns and step_s and len(rem)>0:\n",
        "        idx_done = int(rem.eq(0).idxmax()) if (rem==0).any() else None\n",
        "        if idx_done is not None:\n",
        "            t_to_done = (int(dfm.loc[idx_done,\"t\"]) - int(dfm.loc[dfm.index[0], \"t\"])) * step_s\n",
        "\n",
        "    spread_mean = float(pd.to_numeric(dfm.get(\"spread\", pd.Series(dtype=float)), errors=\"coerce\").mean()) if \"spread\" in dfm.columns else np.nan\n",
        "    depth_bid   = float(pd.to_numeric(dfm.get(\"depth_bid_top3\", pd.Series(dtype=float)), errors=\"coerce\").mean()) if \"depth_bid_top3\" in dfm.columns else np.nan\n",
        "    depth_ask   = float(pd.to_numeric(dfm.get(\"depth_ask_top3\", pd.Series(dtype=float)), errors=\"coerce\").mean()) if \"depth_ask_top3\" in dfm.columns else np.nan\n",
        "\n",
        "    if \"t\" in dfm.columns and step_s:\n",
        "        dur_sec = (int(dfm[\"t\"].iloc[-1]) - int(dfm[\"t\"].iloc[0])) * step_s\n",
        "    else:\n",
        "        dur_sec = np.nan\n",
        "\n",
        "    summary = pd.DataFrame({\n",
        "        \"Mode\":[\"Execution (no PnL/equity in metrics)\"],\n",
        "        \"Steps\":[len(dfm)],\n",
        "        \"Backtest Length\":[str(timedelta(seconds=float(dur_sec))) if np.isfinite(dur_sec) else \"—\"],\n",
        "        \"Total Qty\":[int(total_qty) if np.isfinite(total_qty) else \"—\"],\n",
        "        \"Completed?\":[complete],\n",
        "        \"Fills (count)\":[n_fills if np.isfinite(n_fills) else \"—\"],\n",
        "        \"Time to Complete (s)\":[round(t_to_done,2) if np.isfinite(t_to_done) else \"—\"],\n",
        "        \"Avg Spread (price)\":[round(spread_mean,6) if np.isfinite(spread_mean) else \"—\"],\n",
        "        \"Mean Depth Bid Top3\":[round(depth_bid,2) if np.isfinite(depth_bid) else \"—\"],\n",
        "        \"Mean Depth Ask Top3\":[round(depth_ask,2) if np.isfinite(depth_ask) else \"—\"],\n",
        "        \"Metrics Source\":[mpath if mpath else \"(RAM: df_metrics)\"],\n",
        "    })\n",
        "\n",
        "from IPython.display import display\n",
        "print(\"▶ Backtest Summary\")\n",
        "display(summary)\n"
      ],
      "metadata": {
        "id": "2Y_3_aEqL8OY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "3c6eac01-32f7-49a3-b95d-becaa122c105"
      },
      "id": "2Y_3_aEqL8OY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▶ Backtest Summary\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                   Mode  Steps Backtest Length  Total Qty  \\\n",
              "0  Execution (no PnL/equity in metrics)   9837         1:21:58     300000   \n",
              "\n",
              "   Completed?  Fills (count)  Time to Complete (s)  Avg Spread (price)  \\\n",
              "0        True           8031                4918.0             0.51086   \n",
              "\n",
              "  Mean Depth Bid Top3 Mean Depth Ask Top3     Metrics Source  \n",
              "0                   —                   —  (RAM: df_metrics)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a63143d4-8551-439d-8d62-11c6eb9b7950\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Mode</th>\n",
              "      <th>Steps</th>\n",
              "      <th>Backtest Length</th>\n",
              "      <th>Total Qty</th>\n",
              "      <th>Completed?</th>\n",
              "      <th>Fills (count)</th>\n",
              "      <th>Time to Complete (s)</th>\n",
              "      <th>Avg Spread (price)</th>\n",
              "      <th>Mean Depth Bid Top3</th>\n",
              "      <th>Mean Depth Ask Top3</th>\n",
              "      <th>Metrics Source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Execution (no PnL/equity in metrics)</td>\n",
              "      <td>9837</td>\n",
              "      <td>1:21:58</td>\n",
              "      <td>300000</td>\n",
              "      <td>True</td>\n",
              "      <td>8031</td>\n",
              "      <td>4918.0</td>\n",
              "      <td>0.51086</td>\n",
              "      <td>—</td>\n",
              "      <td>—</td>\n",
              "      <td>(RAM: df_metrics)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a63143d4-8551-439d-8d62-11c6eb9b7950')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a63143d4-8551-439d-8d62-11c6eb9b7950 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a63143d4-8551-439d-8d62-11c6eb9b7950');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_025faa88-3e3a-4cac-a2f8-3ee602464c60\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('summary')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_025faa88-3e3a-4cac-a2f8-3ee602464c60 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('summary');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "summary",
              "summary": "{\n  \"name\": \"summary\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"Mode\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Execution (no PnL/equity in metrics)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Steps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 9837,\n        \"max\": 9837,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          9837\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Backtest Length\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"1:21:58\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total Qty\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 300000,\n        \"max\": 300000,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          300000\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Completed?\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Fills (count)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 8031,\n        \"max\": 8031,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          8031\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Time to Complete (s)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 4918.0,\n        \"max\": 4918.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4918.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Avg Spread (price)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.51086,\n        \"max\": 0.51086,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.51086\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Mean Depth Bid Top3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\\u2014\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Mean Depth Ask Top3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\\u2014\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Metrics Source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"(RAM: df_metrics)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}